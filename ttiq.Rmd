---
title: "Quantifying the impact of test-trace-isolate-quarantine (TTIQ) strategies on COVID-19 transmission"
author:
  - "Peter Ashcroft\\textsuperscript{1}, Sonja Lehtinen\\textsuperscript{1}, and Sebastian Bonhoeffer\\textsuperscript{1}"
  - "\\textsuperscript{1}*Institute of Integrative Biology, ETH Zurich, Switzerland*"
output:
  pdf_document:
    citation_package: natbib
    fig_caption: true
    latex_engine: pdflatex
header-includes:
  - \usepackage{multirow}
bibliography: 2020-TTIQ.bib
biblio-style: apalike
---

```{r setup, include = F}
knitr::opts_chunk$set(
  echo = FALSE,
  cache = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 4,
  fig.pos = "ht",
  eval.after = "fig.cap",
  out.extra = ""
)

library(tidyverse)
library(reshape2)
library(parallel)
library(cowplot)
library(stats)
library(MASS)
library(RColorBrewer)
library(magick)

#' Some plot functions
plotTheme <- theme_bw() +
  theme(
    plot.background = element_rect(fill = "white", colour = "white"),
    panel.grid = element_blank(),
    panel.grid.major.y = element_line(size = 0.1, colour = "grey90"),
    strip.background = element_blank()
  )
#' Reduce text size
plotTheme$text$size <- 12

palette_OkabeIto <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

lineSize <- 1.2
strokeSize <- 0.8
my_geom_point <- geom_point(shape = 21, colour = "white", size = lineSize, stroke = strokeSize)


dayLabels <- function(x) {
  labs <- paste(x, ifelse(x == 1, "day", "days"))
  names(labs) <- x
  return(labs)
}

#' Parallel loop function
ParLapply <- function(X, FUN, ..., PARALLEL = TRUE, SEED = NULL) {
  if (PARALLEL) {
    if (is.null(SEED)) SEED <- 111
    num.cores <- min(c(length(X), max(detectCores() - 1, 1)))
    cl <- makeCluster(num.cores, type = "FORK")
    clusterSetRNGStream(cl, SEED)
    out <- parLapply(cl, X, FUN, ...)
    stopCluster(cl)
    return(out)
  } else {
    out <- lapply(X, FUN, ...)
    return(out)
  }
}
```

# Abstract
\textbf{Background}
The test-trace-isolate-quarantine (TTIQ) strategy is used to break chains of transmission during a disease outbreak and is one of the key pillars of the non-pharmaceutical interventions to suppress the ongoing SARS-CoV-2 pandemic.
Here we quantify how the probability of detecting and isolating a case, the fraction of contacts identified and quarantined, and the delays that are inherent to these processes impact the reduction of disease transmission by TTIQ.\newline
\textbf{Methods}
We develop an analytical model of disease transmission that is based on empirical distributions of the timing of SARS-CoV-2 transmission.
The isolation of confirmed cases and quarantine of their contacts is implemented by truncating their respective infectious periods.
Using this model we quantify how the parameters describing the coverage of the TTIQ intervention and the inherent delays impact the level of disease transmission.
We provide an online application to assess the efficacy of TTIQ as a function of these parameters.\newline
\textbf{Findings}
Increasing the coverage of testing and isolating index cases has the largest effect on transmission reduction, followed by reducing the delay between symptom onset and index case isolation.
The impacts of these two changes are substantially greater than the effect of increasing the fraction of contacts which are traced and subsequently quarantined or reducing the delay to quarantine.
We find that, on average, increasing testing and isolation coverage and reducing the delay to isolation have four-fold and three-fold greater impacts, respectively, on transmission reduction compared to increasing contact tracing coverage.
Increasing the duration of lookback in which contacts are identifiable has limited impact on TTIQ efficacy.\newline
\textbf{Interpretation}
To be a successful intervention strategy, TTIQ requires intensive testing.
The majority of transmission is prevented by isolating symptomatic individuals, and doing so in a short amount of time.
Despite the lesser impact, adding contact tracing and quarantine to testing and isolation increases the parameter space in which an epidemic is controllable, and is necessary to control epidemics with a high reproductive number.
Our results show how TTIQ can be improved and optimised.\newline
\textbf{Funding}
This work was supported by the Swiss National Science Foundation.\newline
\textbf{Keywords}
SARS-CoV-2; COVID-19; Contact tracing; Quarantine; Isolation; TTIQ; Intervention.


# Introduction
Individuals who are confirmed as infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) are isolated from the population to prevent further transmission.
Individuals who have been in recent close contact with an infected individual have an increased risk of being infected themselves.
By identifying the potentially-infected contacts through contact tracing and eventually quarantining them, transmission chains can be broken.
Thus contact tracing is an essential public health tool for controlling epidemics \citep{WHO:contactTracing}.
The strategy of testing to identify infected cases, isolating them to prevent further transmission, and tracing \& quarantining their recent close contacts is known as test-trace-isolate-quarantine (TTIQ) \citep{salathe:SwissMed.Wkly.:2020}.
This strategy is a fundamental non-pharmaceutical intervention which is used globally to control the ongoing SARS-CoV-2 pandemic \citep{kucharski:TheLancetInfectiousDiseases:2020}.

Testing typically occurs once an individual develops symptoms of coronavirus disease 2019 (COVID-19).
As presymptomatic transmission makes up approximately 40\% of total onward transmission from eventually-symptomatic infecteds \citep{he:NatMed:2020,ashcroft:SMW:2020,ferretti:medRxiv:2020}, it would be possible for the number of secondary infections to be more than halved if infected individuals are isolated from the community at the time of symptom onset. 
However, as testing follows from symptoms, the testing \& isolating strategy without subsequent contact tracing \& quarantine is unlikely to capture persistently-asymptomatic infections which make up around 20\% of all infecteds \citep{buitrago-garcia:PLOSMedicine:2020}, and thus isolating 100\% of infecteds at symptom onset would not be possible.

Contact tracing \& quarantine have the potential to be effective interventions against the spread of COVID-19 because of the high frequency of presymptomatic and asymptomatic transmission from recently-infected individuals \citep{moghadas:PNAS:2020}.
Potentially-infected contacts can be identified and quarantined before they would be isolated as a result of developing symptoms and/or receiving a positive test result, such that their onward transmission is reduced.
This is exemplified during super-spreader events \citep{riou:Eurosurveillance:2020,endo:WellcomeOpenRes:2020a,adam:Nat.Med.:2020} where large numbers of potentially-infected contacts can be quarantined to prevent widespread community transmission.
Tracing \& quarantine does not depend on symptom development, hence this strategy is capable of reducing onward transmission even from asymptomatically-infected individuals.

TTIQ strategies are not perfect: each stage in the process is subject to delays and uncertainties and it would be impossible to prevent all onward transmission through TTIQ alone \citep{ferretti:Science:2020,kucharski:TheLancetInfectiousDiseases:2020,kretzschmar:TheLancetPublicHealth:2020,quilty:TheLancetPublicHealth:2021,ashcroft:eLife:2021}.
Furthermore, in the presence of widespread community transmission the contact tracers may be overwhelmed by the volume of cases.
In this scenario it is important to optimise the resources (i.e. the person hours of the contact tracers) to minimise onward transmission.

In a previous study of TTIQ efficacy, \citet{ferretti:Science:2020} used an approach based on the empirically-observed timing of transmission events -- but with substantial approximations around the TTIQ process -- to get to an analytically tractable prediction of the impact of TTIQ on SARS-CoV-2 transmission.
They concluded that widespread digital contact tracing (with minimal delay between index case identification and quarantine of secondary contacts) would be necessary to reduce the effective reproduction number below one and to bring an outbreak under control.
\citet{kucharski:TheLancetInfectiousDiseases:2020} simulated the impact of intervention strategies using an agent-based model, where each individual in a population can be infected by and subsequently infect another individual in their own contact network.
While the TTIQ process is more accurately implemented than in \citet{ferretti:Science:2020}, they did not use empirical distributions to describe the timing of transmission, which is crucial for quantifying the impact of isolation and quarantine.
\citet{kretzschmar:TheLancetPublicHealth:2020} opted to simulate a discrete-time branching process model of transmission and TTIQ.
While they explicitly accounted for the timing of infection events and accurately described the TTIQ process, they predominantly focussed on assessing the role of digital contact tracing based on mobile applications.
They also conclude that minimising delays is the key to successful TTIQ intervention.
Finally, \citet{grantz:medRxiv:2020} employed a discrete-time Markov chain model to simulate how infecteds in the community can be detected and isolated and have their contacts quarantined.
Through simplifications regarding the timing of infection events, they could compute the effective reproductive number in the presence of TTIQ interventions and evaluate how changes to TTIQ accuracy and timing impacts this reproductive number.
They concluded that effective TTIQ interventions need to be strong in the ``test'' component, as case detection underlies all other TTIQ components.

What is missing in the literature is a systematic approach to assess the impact of each step in a TTIQ intervention, which captures what we know about the timing of disease transmission but makes minimal assumptions about the remaining population dynamics.
In this paper we build on our previous work in which we have quantified the impact of quarantine duration and highlighted the optimal use of test-and-release strategies \citep{ashcroft:eLife:2021}.
With this mathematical framework, which uses the empirically-observed distributions of transmission timing from \citet{ferretti:medRxiv:2020} to determine when infections occur, we calculate how the probability of identifying and isolating a case, the fraction of contacts identified and quarantined, and the delays that are inherent to these processes impact disease transmission under TTIQ interventions.


# Methods
## Transmission model
Our transmission model is based on a branching process that starts with a single individual who is infected with SARS-CoV-2.
This individual could be persistently asymptomatic (which make up a fraction $a$ of infections), otherwise they are classed as symptomatic ($1-a$).
To be clear, presymptomatic individuals who will go on to develop symptoms are included in the symptomatic fraction.
\citet{buitrago-garcia:PLOSMedicine:2020} have estimated $a \sim 20\%$ based on a meta-analysis of 79 studies.

The timing of onward infections in the model is determined by empirically-observed distributions of transmission dynamics from \citet{ferretti:medRxiv:2020}.
These distributions are:
the generation time distribution (describing the time interval between the infection of an index case and secondary case);
the infectivity profile (describing the time interval between the onset of symptoms in the index case and infection of the secondary case);
and the incubation period distribution (describing the time between the infection of an individual and the onset of their symptoms).
These distributions (shown in Figure \ref{fig:distributions}) are based on large sets of transmission pairs and minimal assumptions about the relationship between infectiousness and symptoms, which would otherwise push the variance of the resulting generation time distribution towards its upper or lower extremes \citep{lehtinen:Interface:2021}.
In the model we assume that these timing distributions are the same between asymptomatic and symptomatic cases.
The fraction of transmission that occurs before symptom onset in symptomatically-infected individuals is defined by the cumulative infectivity profile up to the time of symptom onset.

Using this branching process model we calculate the number of infected individuals in the second generation (secondary infections) and in the third generation (tertiary infections) after the introduction of the first infected individual.
We also keep track of the time at which the transmission events occur.
In our analysis we do not simulate the branching process explicitly, but instead use a statistical description of the dynamics.

The expected number of secondary infections per infected depends on the transmissibility of the virus (e.g. as captured by the basic reproductive number $R_0$), as well as the current level of interventions in place to mitigate the spread of the virus.
As we are interested in quantifying the effects of TTIQ strategies, we introduce the parameter $R$ which represents the effective reproductive number of the virus in the presence of interventions such as mask-wearing, social distancing, school closures etc., but in the absence of isolation and quarantine.
In the absence of TTIQ interventions, we would expect $R$ infections in the second generation, and $R^2$ infections in the third generation.

Furthermore, this $R$-value represents the average effective reproductive number across asymptomatic and symptomatic infections, i.e. $R = aR_a+(1-a)R_s$, where $R_a$ and $R_s$ are the expected number of individuals who are directly infected by an asymptomatic or symptomatic individual in the absence of TTIQ, respectively.
We would expect that $R_a \le R_s$ \citep{buitrago-garcia:PLOSMedicine:2020}.
We can further define the parameter $\alpha = aR_a/R$ as the fraction of all transmission that originates from asymptomatically-infected individuals in the absence of TTIQ.
This fraction has the property that if asymptomatic and symptomatic individuals are equally transmissive ($R_a = R_s$), then $\alpha$ is just the fraction of asymptomatic individuals ($\alpha = a$).
If asymptomatic individuals are less infectious ($R_a < R_s$), then $\alpha < a$.
Therefore the fraction of transmission from asymptomatic individuals in the absence of TTIQ satisfies $0 \le \alpha \le a$.

```{r schematic, fig.cap = caption}
caption <- "Quantifying the impact of TTIQ interventions using a mathematical model.
A) Under testing \\& isolation, index cases are identified and isolated from the population after a delay $\\Delta_1$ after they develop symptoms (at time $t_{S_1}$).
This curtails their duration of infectiousness and reduces the number of secondary infections.
This isolation occurs in a fraction $f$ of symptomatic individuals.
B) Under additional contact tracing \\& quarantine, the contacts of an index case can be identified and quarantined after an additional delay $\\Delta_2$.
This reduces the onward transmission from these secondary contacts.
Only contacts that occur during the contact tracing window can be identified.
This window extends from $\\tau$ days before the index case developed symptoms (i.e. $t_{S_1}-\\tau$) to the time at which the index case was isolated (i.e. $t_{S_1}+\\Delta_1$).
A fraction $g$ of the contacts who were infected within the contact tracing window are quarantined.
The remaining individuals are not quarantined, but could be isolated if they are later detected as an index case.
The distributions shown here are schematic representations of those shown in Fig. \\ref{fig:distributions}."

ggdraw() + draw_image(image_read_pdf("TTIQ-schematic.pdf", density = 600), scale = 1.0)
```

## Testing \& isolating
Individuals who develop symptoms of COVID-19 can be tested and subsequently isolated from the population.
Testing \& isolating acts to reduce the number of secondary infections per index case by shortening the duration in which the index case can infect susceptible individuals, i.e. isolation truncates the distribution of infection times (Fig. \ref{fig:schematic}A).
We assume that isolation happens after a delay of $\Delta_1$ days after symptom onset, and that only a fraction $f$ of symptomatic individuals are isolated.
This incomplete coverage can be attributed to symptom misdiagnosis, a failure or unwillingness to get tested if symptomatic, a false-negative test result, or non-adherence to the isolation protocol.
For those individuals who are isolated, we assume that they cannot infect further for the remaining duration of their infectious period.
In the model, asymptomatic individuals are not tested and they do not isolate.

The infected individuals who are not isolated will infect $R_a$ or $R_s$ secondary contacts, depending on whether they are classified as asymptomatic or symptomatic. 
Isolated symptomatic cases will infect $P(\Delta_1) \times R_s$ secondary contacts, where $0 \le P(\Delta_1) \le 1$ is the cumulative fraction of the infection time distribution that lies before the isolation time $\Delta_1$ (see Fig. \ref{fig:schematic}A).
Averaging across these scenarios gives the expected number of secondary infections per infected case.
We can repeat this analysis for each of the secondary infections to calculate the number tertiary infections under testing \& isolation.


## Contact tracing \& quarantine
When an index case is identified by testing, they can be interviewed by contact tracers to determine whom they have potentially infected, with the aim of quarantining these exposed individuals.
The contact tracers focus on a specific time window of infection to identify the contacts with highest risk of being infected.
In our model this window extends to $\tau$ days before symptom onset in the index case.
It is unlikely that every contact within this time window is memorable or traceable, so we assume that only a fraction $g$ of the secondary contacts within this window are eventually quarantined.
Quarantine begins after a delay of $\Delta_2$ days after the isolation of the index case, representing the time required for the contact to be identified by contact tracers and to enter quarantine.
For those who are quarantined, we assume that they cannot infect further for the remaining duration of their infectious period.
Importantly, quarantine occurs independently of whether these secondary infections are asymptomatic or will eventually develop symptoms.
Quarantine shortens the duration in which the identified secondary contacts can transmit further to tertiary contacts (Fig. \ref{fig:schematic}B).
For each quarantined secondary contact, we calculate the number of onward infections by computing the cumulative fraction of the infection time distribution before quarantine begins and multiplying by $R_a$ or $R_s$, depending on whether the secondary contact is classified as asymptomatic or symptomatic.

If a secondary contact is not quarantined, then they could be detected after symptom onset as a new index case, and subsequently isolated.
The number of onward infections that result from these individuals is typically higher than for those who are quarantined, as they have to wait until symptom onset before they can be isolated, or they may not develop symptoms at all in which case they are not isolated.
Finally, some secondary contacts are not quarantined or isolated, and will infect $R_a$ or $R_s$ tertiary contacts.
The average number of infections caused by the quarantined, isolated, and non-isolated secondary contacts is then the number of tertiary infections per index case.
See Appendix for the full calculation.

## TTIQ parameters
The efficacy of the TTIQ interventions depends on how quickly and accurately they are implemented.
To this end, we have introduced five parameters to describe the TTIQ process (Table \ref{tab:TTIQ-params}).
We systematically explore this TTIQ parameter space, first for the testing \& isolation intervention in the absence of contact tracing (Fig. \ref{fig:schematic}A), and then with additional tracing \& quarantine (Fig. \ref{fig:schematic}B).

\begin{table}[h]
\begin{tabular}{l p{8.8cm} l}
Parameter & Description & Range \\ \hline
$f$ & Probability that a symptomatic individual is isolated from the population & $0\% \le f \le 100\%$ \\ \hline
$\Delta_1$ & Time delay between symptom onset and isolation & $\Delta_1 \ge 0$ days \\ \hline
$\tau$ & Duration prior to symptom onset in which contacts are identifiable & $\tau \ge 0$ days \\ \hline
$g$ & Fraction of identifiable contacts that are successfully traced and quarantined per isolated index case & $0\% \le g \le 100\%$ \\ \hline
$\Delta_2$ & Time delay between isolation of the index case and the start of quarantine for the secondary contacts & $\Delta_2 \ge 0$ days \\ \hline
\end{tabular}
\caption{Parameter definitions for the TTIQ interventions. The delay and lookback parameters $\Delta_1$, $\Delta_2$, and $\tau$ are illustrated in Fig. \ref{fig:schematic}.}
\label{tab:TTIQ-params}
\end{table}


## Effective reproduction number
The effectiveness of the TTIQ intervention can be quantified by calculating the effective reproduction number in the presence of the interventions, $R_{\rm TTIQ}$, which describes the expected number of secondary infections per infected individual.
If $R_{\rm TTIQ} > 1$ then the epidemic is growing, while a value of less than one means the epidemic is being suppressed.
For our branching process model, we define the reproductive number as $R_{\rm TTIQ} = n_3/n_2$, where $n_2$ and $n_3$ are the expected number of tertiary and secondary infections per index case, respectively.
In other words, we define the reproductive number as the average number of infecteds in the third generation per infected in the second generation.
It is necessary to work with the third generation (as opposed to just the first and second generations) as this is where the impact of contact tracing and quarantine is first observed.
Under strategies of testing \& isolation alone (i.e. no contact tracing), we use the notation $R_{\rm TI}$ for clarity.

## Computing uncertainties
As shown in Fig. \ref{fig:distributions}B and C, there is significant uncertainty in the variance of the inferred generation time distribution and infectivity profile.
We propagate this uncertainty into our calculation of $R_{\rm TTIQ}$.
Briefly, we sample parameter combinations that make up the 95\% confidence interval of the generation time distribution and infectivity profile, and then compute $R_{\rm TTIQ}$ for each parameter set.
The maximum and minimum of these values then describe the confidence interval for the level of transmission
Complete details are provided in the Appendix.

## Interactive app
To complement the results in this manuscript, and to allow readers to investigate different TTIQ parameter settings, we have developed an online interactive application.
This can be found at \url{https://ibz-shiny.ethz.ch/covidDashboard/ttiq}.

## Role of the funding source
The funders of the study had no role in study design, data collection, data analysis, data interpretation, writing of the manuscript, or the decision to submit for publication.
All authors had full access to all the data in the study and were responsible for the decision to submit the manuscript for publication.

```{r incubation-distribution-definition-Ferretti, include = F}
# #' Code from Ferretti et al.
# inc<-function(x){
#   (
#     dlnorm(x,meanlog = 1.621, sdlog = 0.418)+ # Lauer
#     dlnorm(x,meanlog = 1.425, sdlog = 0.669)+ # Li
#     dlnorm(x,meanlog = 1.57, sdlog = 0.65)+ # Bi
#     dlnorm(x,meanlog = 1.53, sdlog = 0.464)+ # Jiang???
#     dlnorm(x,meanlog = 1.611, sdlog = 0.472)+ # Linton
#     dlnorm(x,meanlog = 1.54, sdlog = 0.47)+ # Zhang
#     dlnorm(x,meanlog = 1.857, sdlog = 0.547) # Ma
# )/7
# }
# incCum<-function(x){
#   (
#     plnorm(x,meanlog = 1.621, sdlog = 0.418)+
#       plnorm(x,meanlog = 1.425, sdlog = 0.669)+
#       plnorm(x,meanlog = 1.57, sdlog = 0.65)+
#       plnorm(x,meanlog = 1.53, sdlog = 0.464)+
#       plnorm(x,meanlog = 1.611, sdlog = 0.472)+
#       plnorm(x,meanlog = 1.54, sdlog = 0.47)+
#       plnorm(x,meanlog = 1.857, sdlog = 0.547)
#   )/7
# }

# Incubation period distribution is averaged across 7 reported distributions:
incParams <- rbind(
  data.frame(study = "Bi", n = 183, meanlog = 1.570, sdlog = 0.650), # Reported
  data.frame(study = "Lauer", n = 181, meanlog = 1.621, sdlog = 0.418), # Reported
  data.frame(study = "Li", n = 10, meanlog = 1.434, sdlog = 0.661), # From source code of He et al.
  data.frame(study = "Linton", n = 158, meanlog = 1.611, sdlog = 0.472), # with(list(mu = 5.6, sigma = 2.8), c(mean = log(mu^2 / sqrt(mu^2 + sigma^2)), sd = sqrt(log(1 + sigma^2 / mu^2))))
  data.frame(study = "Ma", n = 587, meanlog = 1.857, sdlog = 0.547), # with(list(mu = 7.44, sigma = 4.39), c(mean = log(mu^2 / sqrt(mu^2 + sigma^2)), sd = sqrt(log(1 + sigma^2 / mu^2))))
  data.frame(study = "Zhang", n = 49, meanlog = 1.540, sdlog = 0.470), # Reported
  data.frame(study = "Jiang", n = 2015, meanlog = 1.530, sdlog = 0.464) # Unknown...
)
incParams$study <- factor(incParams$study)
#' Mean and SD of this incubation time
incParams$mean <- mean(exp(incParams$meanlog + (incParams$sdlog^2)/2))
#sqrt(mean((exp(incParams$sdlog^2)-1) * exp(2*incParams$meanlog + incParams$sdlog^2))) # SD

#' PDF and CDF of the incubation period
getIncubationPeriod <- function(times, params, CDF = F) {
  y <- sapply(levels(params$study), function(study) {
    if (CDF) { # Cumulative density function
      return(plnorm(q = times,
                    meanlog = params[params$study == study, "meanlog"],
                    sdlog = params[params$study == study, "sdlog"]))
    } else { # Probability density function
      return(dlnorm(x = times,
                    meanlog = params[params$study == study, "meanlog"],
                    sdlog = params[params$study == study, "sdlog"]))
    }
  })
  
  #' Average over the studies
  if (length(times) == 1) return(mean(y))
  else return(apply(y,1,mean))
}
```

```{r incubation-periods-plot, fig.width = 8, eval = F}
times <- seq(0,20,0.1)

#' Define the incubation distribution from meta-distribution
incDist <- data.frame(
  t = times,
  pdf = getIncubationPeriod(times = times, params = incParams),
  CDF = getIncubationPeriod(times = times, params = incParams, CDF = TRUE),
  study = "combined"
)

#' Calculate incubations distributions for each study individually
incDistSep <- lapply(levels(incParams$study), function(study) {
  params <- incParams[incParams$study == study, ]
  params$study <- factor(study, levels = study)
  
  data.frame(
    t = times,
    pdf = getIncubationPeriod(times = times, params = params),
    CDF = getIncubationPeriod(times = times, params = params, CDF = TRUE),
    study = study
  )
})

df <- rbind(incDist, do.call(rbind,incDistSep))
df$study <- factor(df$study, levels = c("combined", levels(incParams$study)))
colours <- c("black", brewer.pal(length(levels(incParams$study)),"Set1"))

pdfPlot <- ggplot(df, aes(x = t, y = pdf, colour = study)) +
  geom_line(aes(size = ifelse(study == "combined", 1.5, 1.2))) +
  scale_colour_manual(values = colours) +
  guides(size = F) +
  coord_cartesian(xlim = c(0,20), ylim = c(0,0.23), expand = F) +
  labs(x = "incubation period (days)", y = "probability density") +
  ggtitle("incubation period distribution") +
  plotTheme +
  theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5),
        legend.position = c(0.7,0.8))

cdfPlot <- ggplot(df, aes(x = t, y = CDF, colour = study)) +
  geom_line(aes(size = ifelse(study == "combined", 1.5, 1.2))) +
  scale_colour_manual(values = colours) +
  guides(size = F) +
  coord_cartesian(xlim = c(0,20), ylim = c(0,1), expand = F) +
  labs(x = "incubation period (days)", y = "cumulative probability") +
  ggtitle("cumulative incubation period distribution") +
  plotTheme +
  theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5),
        legend.position = "none")

plot_grid(pdfPlot, cdfPlot, align = "hv", axis = "tb", nrow = 1)
```

```{r serial-interval-data, include = F, eval = F}
#' Data from Ferretti (40), Xia (32), Zhang (35), He (66)
info_data <- read.csv("data/Ferretti-TableTransmissionPairs_addendum.tsv", sep = "\t", stringsAsFactors = F)
location <- info_data$Country
info_data[,"Exponential.growth"] <- 0 + (info_data[,"Exponential.growth"] == "YES")
info_data <- cbind(
  apply(info_data[,c("T1L","T1R","s1","T2L","T2R","s2","Tr","Exponential.growth")], 2, as.numeric),
  info_data[,"Source.manuscript", drop = F]
)
info_data <- as.data.frame(info_data)

#' Exposure intervals for non-tested cases in Cheng
dnont <- read.csv("data/Ferretti-Taiwan_transmission_pair_contact_exposure_v0514.csv")
dnont <- dnont[is.na(dnont$test),]
dnont <- data.frame(
  left_exposure = as.numeric(difftime(dnont[,"time2exposure_start"], dnont[,"onset_source"], units = "d")),
  right_exposure = as.numeric(difftime(dnont[,"time2exposure_end"], dnont[,"onset_source"], units = "d"))
)
dnont <- dnont[dnont[,"left_exposure"] <= dnont[,"right_exposure"],]
dnont <- dnont[dnont$left_exposure > -100,]

#' Data of symptomatic cases from Cheng (18)
dt <- read.csv("data/Ferretti-Taiwan_transmission_pair_v0514.csv")
dt <- dt[!is.na(dt$onset),]
#
dtrans <- data.frame(
  left_exposure = as.numeric(difftime(dt[,"time2exposure_start"], dt[,"onset_source"], units = "d")) - 0.5,
  right_exposure = as.numeric(difftime(dt[,"time2exposure_end"], dt[,"onset_source"],units = "d")) + 0.5,
  case.noncase = "case",
  serial_interval = as.numeric(difftime(dt[,"onset"], dt[,"onset_source"], units = "d"))
)
dtrans$left_exposure[dtrans$left_exposure < -100] <- NA

# Joining Ferretti (40), Xia (32), Zhang (35), He (66), and Cheng (18) datasets
info_data_taiwan <- data.frame(
  T1L = NA,
  T1R = NA,
  s1 = 0,
  T2L = dtrans$left_exposure + 0.5,
  T2R = dtrans$right_exposure - 0.5,
  s2 = dtrans$serial_interval,
  Tr = 100,
  Exponential.growth = 0,
  Source.manuscript = "Cheng"
)
serial_interval_data <- rbind(info_data, info_data_taiwan)
save(serial_interval_data, file = "data/Ferretti-serial_interval_data.RData")
```

```{r serial-interval-data-plot, eval = F}
load("data/Ferretti-serial_interval_data.RData")
data <- serial_interval_data
#' Compute serial interval from symptom onsets
data$SI <- data$s2 - data$s1
data$Source.manuscript <- factor(data$Source.manuscript,
                                 levels = c("Zhang", "He", "Ferretti", "Xia", "Cheng"))
minSI <- min(data$SI) - 1.5
maxSI <- max(data$SI) + 1.5
ggplot() +
  geom_histogram(data = data,
                 aes(x = SI, fill = Source.manuscript),
                 breaks = seq(from = minSI, to = maxSI)) +
  scale_fill_brewer(palette = "Set1") +
  coord_cartesian(expand = F) +  # no gaps between bars and axes
  labs(fill = "Dataset:",
       x = "Serial interval (days)",
       y = "Count") +
  plotTheme +
  theme(legend.position = c(0.8, 0.8)) 
```

```{r fit-infectivity-profile, include = F, eval = F}
load("data/Ferretti-serial_interval_data.RData")

#' Maximum duration of infection (days)
M <- 30
mintW <- 2 * M + 2
#' Growth rate of the epidemic (doubling time of 5 days)
r <- log(2)/5

#' Discrete incubation time distribution (integrated probability per day)
Inc <- getIncubationPeriod(seq(0,2*M) + 0.5, params = incParams, CDF = T) -
    getIncubationPeriod(pmax(seq(0,2*M) - 0.5, 0), params = incParams, CDF = T)

#' Student t-distribution for infectivity profile
infectivityProfile <- function(x, pars, CDF = F) {
  if (CDF) {
    return(pt((x - pars["shift"])/exp(pars["log_scale"]), df = exp(pars["log_df"])))
  } else {
    return(dt((x - pars["shift"])/exp(pars["log_scale"]), df = exp(pars["log_df"])) / exp(pars["log_scale"]))
  }
}

#' Limits for the exposure time integrals (this is evaluated per transmission pair)
getLimits <- function(pair_data) {
  v <- list()
  v$T1L <- max(pair_data$T1L, pair_data$s1 - 2*M, na.rm = T)
  v$T1R <- min(pair_data$T1R, pair_data$s1, pair_data$s2, pair_data$T2R, na.rm = T)
  v$T2L <- max(pair_data$T2L, pair_data$s2 - 2*M, v$T1L, na.rm = T)
  v$T2R <- min(pair_data$T2R, pair_data$s2, na.rm = T)
  return(v)
}

#' Perform the double integral (or sum) per transmission pair
#' to get probability of observing serial interval
getProbSI <- function(pair_data, W, limits) {
  sum(
    sapply(seq(limits$T1L, limits$T1R), function(t1) {
      exp(r * pair_data$Exponential.growth * t1) *
        Inc[pair_data$s1 - t1 + 1] * 
        sum(
          sapply(seq(max(t1, limits$T2L), limits$T2R), function(t2) {
            W[t2 - pair_data$s1 + mintW] * Inc[pair_data$s2 - t2 + 1]
          })
        )
    })
  )
}

#' log-likelihood of a single transmission pair (1 row of data)
getLLH <- function(pair_data, W) {
  limits <- getLimits(pair_data)
  log(getProbSI(pair_data, W, limits))
}

#' Evaluate the new profile as a function of the optimisation parameters,
#' and then compute the negative log-likelihood for each transmission pair
getTotalLLH <- function(pars, data, w) {
  x <- seq(0, (4*M + mintW)) - (mintW - 1)
  W <- w(x + 0.5, pars, CDF = T) - w(x - 0.5, pars, CDF = T)
  return(sum(apply(data, 1, function(data_row) getLLH(pair_data = as.list(data_row), W))))
}

#' Set up and run optimiser
data <- subset(
  serial_interval_data,
  select = -c(Source.manuscript),
  subset = (serial_interval_data$Source.manuscript %in% c("Ferretti", "Xia", "He", "Cheng"))
)

optim_out <- optim(par = c(shift = 0, log_scale = 0, log_df = 0),
                    fn = getTotalLLH, data = data, w = infectivityProfile,
                    control = list(fnscale = -1))
optim_out <- optim(par = optim_out$par,
                    fn = getTotalLLH, data = data, w = infectivityProfile,
                    control = list(fnscale = -1))

#' Confidence interval
times <- seq(-10, 10, 0.1)
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
#' Threshold likelihood value for 95%
minLLH <- optim_out$value - qchisq(0.95, df = length(optim_out$par))/2
#' Parameters of the best fitting model
initpars <- optim_out$par
pars <- initpars
df_param <- data.frame(shift = pars["shift"], scale = exp(pars["log_scale"]),
                       df = exp(pars["log_df"]), llh = optim_out$value)
#' Create a random vector on a sphere in the 3D parameter space
set.seed(42)
dir <- qnorm(runif(length(optim_out$par)))
dir <- dir/sqrt(sum(dir^2))
i <- 0
while (i < 1000) {
  #' Perturb parameters along the vector (Monte Carlo)
  pars <- pars + dir*runif(1)*0.2
  llh <- getTotalLLH(pars, data, infectivityProfile)
  if(llh > minLLH) {
    #' Which points make up the confidence interval
    infProf <- infectivityProfile(times, pars)
    lower <- pmin(infProf, lower, na.rm = T)
    upper <- pmax(infProf, upper, na.rm = T)
    df_param <- rbind(df_param, data.frame(shift = pars["shift"], scale = exp(pars["log_scale"]), df = exp(pars["log_df"]), llh = llh))
    i <- i + 1
  } else {
    #' Change the direction of perturbation
    dir <- qnorm(runif(length(optim_out$par)))
    dir <- dir/sqrt(sum(dir^2))
    pars <- initpars
  }
}
#' Save parameters and likelihood value
infParamsLLH <- df_param
save(infParamsLLH, file = "data/infParamsLLH.RData")
```

```{r plot-infectivity-profile, eval = F}
df_plot <- data_frame(
  days = times,
  infectivityProfile = infectivityProfile(times, optim_out$par),
  incubation = getIncubationPeriod(times, params = incParams),
  lower = lower,
  upper = upper
)

# new_df <- lapply(seq_len(nrow(df_param)), function(i) {
#   data.frame(
#     id = factor(i),
#     llh = df_param[[i,"llh"]],
#     days = times,
#     infProf = infectivityProfile(times, c(shift = df_param[[i,"shift"]], log_scale = log(df_param[[i,"scale"]]), log_df = log(df_param[[i,"df"]]))))
# })
# new_df <- do.call(rbind, new_df)

ggplot(data = df_plot, aes(x = days, y = infectivityProfile)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), color = "transparent", fill = "grey") +
  geom_line() +
  geom_vline(xintercept = 0, linetype = "dotted") +
  #geom_line(data = new_df, aes(x = days, y = infProf, colour = llh, group = id)) +
  #scale_colour_viridis_c() +
  labs(x = "TOST (days)", y = "Probability density (per day)") +
  coord_cartesian(xlim = c(-7.5, 10), expand = F) +
  plotTheme
```

```{r infectivity-profile-definition, include = F}
#' Parameters
infParams <- data.frame(shift = -0.0776, scale = 1.857, df = 3.345)
infParams$mean <- infParams$shift/infParams$scale
#' Function
getInfectivityProfile <- function(times, params, CDF = FALSE) {
  if (CDF) return(pt(q = (times - params$shift)/params$scale, df = params$df))
  else return(dt(x = (times - params$shift)/params$scale, df = params$df)/params$scale)
}
```

```{r fit-generation-time, include = F, eval = F}
load("data/Ferretti-serial_interval_data.RData")

#' Maximum duration of infection (days)
M <- 30
#' Growth rate of the epidemic (doubling time of 5 days)
r <- log(2)/5

#' Discrete incubation time distribution (integrated probability per day)
Inc <- getIncubationPeriod(seq(0,2*M) + 0.5, params = incParams, CDF = T) -
    getIncubationPeriod(pmax(seq(0,2*M) - 0.5, 0), params = incParams, CDF = T)

#' Weibull distribution for generation time
generationTime <- function(x, pars, CDF = F) {
  if (CDF) {
    return(pweibull(x, shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"])))
  } else {
    return(dweibull(x, shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"])))
  }
}

#' Limits for the exposure time integrals (this is evaluated per transmission pair)
getLimits <- function(pair_data) {
  v <- list()
  v$T1L <- max(pair_data$T1L, pair_data$s1 - 2*M, na.rm = T)
  v$T1R <- min(pair_data$T1R, pair_data$s1, pair_data$s2, pair_data$T2R, na.rm = T)
  v$T2L <- max(pair_data$T2L, pair_data$s2 - 2*M, v$T1L, na.rm = T)
  v$T2R <- min(pair_data$T2R, pair_data$s2, na.rm = T)
  return(v)
}

#' Perform the double integral (or sum) per transmission pair
#' to get probability of observing serial interval
getProbSI <- function(pair_data, W, limits) {
  sum(
    sapply(seq(limits$T1L, limits$T1R), function(t1) {
      exp(r * pair_data$Exponential.growth * t1) *
        Inc[pair_data$s1 - t1 + 1] * 
        sum(
          sapply(seq(max(t1, limits$T2L), limits$T2R), function(t2) {
            W[t2 - t1 + 1] * Inc[pair_data$s2 - t2 + 1]
          })
        )
    })
  )
}

#' log-likelihood of a single transmission pair (1 row of data)
getLLH <- function(pair_data, W) {
  limits <- getLimits(pair_data)
  log(getProbSI(pair_data, W, limits))
}

#' Evaluate the new profile as a function of the optimisation parameters,
#' and then compute the negative log-likelihood for each transmission pair
getTotalLLH <- function(pars, data, w) {
  x <- seq(0, 4*M)
  W <- w(x + 0.5, pars, CDF = T) - w(pmax(x - 0.5,0), pars, CDF = T)
  return(sum(apply(data, 1, function(data_row) getLLH(pair_data = as.list(data_row), W))))
}

#' Set up and run optimiser
data <- subset(
  serial_interval_data,
  select = -c(Source.manuscript),
  subset = (serial_interval_data$Source.manuscript %in% c("Ferretti", "Xia", "He", "Cheng"))
)

optim_out <- optim(par = c(log_shape = 0, log_scale = 0),
                    fn = getTotalLLH, data = data, w = generationTime,
                    control = list(fnscale = -1))
optim_out <- optim(par = optim_out$par,
                    fn = getTotalLLH, data = data, w = generationTime,
                    control = list(fnscale = -1))

#' Confidence interval
times <- seq(0, 20, 0.1)
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
#' Threshold likelihood value for 95%
minLLH <- optim_out$value - qchisq(0.95, df = length(optim_out$par))/2
#' Parameters of the best fitting model
initpars <- optim_out$par
pars <- initpars
df_param <- data.frame(shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"]),
                       llh = optim_out$value)
#' Create a random vector on a sphere in the 3D parameter space
set.seed(422)
dir <- qnorm(runif(length(optim_out$par)))
dir <- dir/sqrt(sum(dir^2))
i <- 0
while (i < 1000) {
  #' Perturb parameters along the vector (Monte Carlo)
  pars <- pars + dir*runif(1)*0.2
  llh <- getTotalLLH(pars, data, generationTime)
  if(llh > minLLH) {
    #' Which points make up the confidence interval
    genTime <- generationTime(times, pars)
    lower <- pmin(genTime, lower, na.rm = T)
    upper <- pmax(genTime, upper, na.rm = T)
    df_param <- rbind(df_param, data.frame(shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"]), llh = llh))
    i <- i + 1
  } else {
    #' Change the direction of perturbation
    dir <- qnorm(runif(length(optim_out$par)))
    dir <- dir/sqrt(sum(dir^2))
    pars <- initpars
  }
}
#' Save parameters and likelihood value
genParamsLLH <- df_param
save(genParamsLLH, file = "data/genParamsLLH.RData")
```

```{r plot-generation-time, eval = F}
df_plot <- data_frame(
  days = times,
  generationTime = generationTime(times, optim_out$par),
  incubation = getIncubationPeriod(times, params = incParams),
  lower = lower,
  upper = upper
)

# new_df <- lapply(seq_len(nrow(df_param)), function(i) {
#   data.frame(
#     id = factor(i),
#     llh = df_param[[i,"llh"]],
#     days = times,
#     genTime = generationTime(times, c(log_shape = log(df_param[[i,"shape"]]), log_scale = log(df_param[[i,"scale"]]))))
# })
# new_df <- do.call(rbind, new_df)

ggplot(data = df_plot, aes(x = days, y = generationTime)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), color = "transparent", fill = "grey") +
  geom_line() +
  geom_vline(xintercept = 0, linetype = "dotted") +
  #geom_line(data = new_df, aes(x = days, y = genTime, colour = llh, group = id)) +
  #scale_colour_viridis_c() +
  labs(x = "generation time (days)", y = "Probability density (per day)") +
  coord_cartesian(xlim = c(0,15), expand = F) +
  plotTheme
```

```{r generation-time-distribution-definition, include = F}
#' Parameters
#genParams <- data.frame(shape = 3.2862, scale = 6.1244)
genParams <- data.frame(shape = 3.277, scale = 6.127)
genParams$mean <- genParams$scale * gamma(1 + 1/genParams$shape)
#' Function
getGenDist <- function(times, params, CDF = FALSE) {
  if (CDF) return(pweibull(q = times, shape = params$shape, scale = params$scale))
  else dweibull(x = times, shape = params$shape, scale = params$scale)
}
```

```{r save-distributions, include = F}
stepSize <- 1e-2
times <- seq(-25, 25, stepSize)

i0 <- which(times == 0)
iMax <- length(times)

#' Define the incubation distribution, which is independent of the generation time parameters
incDist <- data.frame(
  t = times,
  pdf = getIncubationPeriod(times = times, params = incParams),
  CDF = getIncubationPeriod(times = times, params = incParams, CDF = TRUE)
)

#' Infectivity profile
infProfMLE <- data.frame(
  t = times,
  pdf = getInfectivityProfile(times = times, params = infParams),
  CDF = getInfectivityProfile(times = times, params = infParams, CDF = T)
)
#' Now compute the confidence interval
load("data/infParamsLLH.RData")
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
for (id in seq_len(nrow(infParamsLLH))) {
  ip <- getInfectivityProfile(times = times, params = infParamsLLH[id,])
  lower <- pmin(ip, lower, na.rm = T)
  upper <- pmax(ip, upper, na.rm = T)
}
infProfMLE$lower <- lower
infProfMLE$upper <- upper

#' Generation time distribution
genDistMLE <- data.frame(
  t = times,
  pdf = getGenDist(times = times, params = genParams),
  CDF = getGenDist(times = times, params = genParams, CDF = T)
)
#' Now compute the confidence interval
load("data/genParamsLLH.RData")
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
for (id in seq_len(nrow(genParamsLLH))) {
  gt <- getGenDist(times = times, params = genParamsLLH[id,])
  lower <- pmin(gt, lower, na.rm = T)
  upper <- pmax(gt, upper, na.rm = T)
}
genDistMLE$lower <- lower
genDistMLE$upper <- upper

#' Integral of g(t')Q(t'+t) from 0 to \infty with 0 \le t < \infty
integralMLE <- data.frame(
  t = times,
  J = sapply(seq_along(times), function(i) {
    g <- incDist[seq(i0, iMax), "pdf"]
    Qt <- genDistMLE[seq(i, iMax - i0 + i), "CDF"]
    Qt[is.na(Qt)] <- 1
    sum(g * Qt) * stepSize
  }, USE.NAMES = F)
)

#' Save results
save(times, i0, iMax, stepSize, incDist, infProfMLE, genDistMLE, integralMLE, file = "data/savedDistributions.RData")
```

```{r save-distribution-samples, include = F, eval = F}
#' Load the MLE distributions
load("data/savedDistributions.RData")

#' Infectivity profile
load("data/infParamsLLH.RData")
infProfLLH <- ParLapply(seq_len(nrow(infParamsLLH)), function(id) {
  data.frame(
    t = times,
    pdf = getInfectivityProfile(times = times, params = infParamsLLH[id,]),
    CDF = getInfectivityProfile(times = times, params = infParamsLLH[id,], CDF = T)
  )
})

#' Generation time distribution
load("data/genParamsLLH.RData")
genDistLLH <- ParLapply(seq_len(nrow(infParamsLLH)), function(id) {
  data.frame(
    t = times,
    pdf = getGenDist(times = times, params = genParamsLLH[id,]),
    CDF = getGenDist(times = times, params = genParamsLLH[id,], CDF = T)
  )
})
#' Integral of g(t')Q(t'+t) from 0 to \infty with 0 \le t < \infty
integralLLH <- ParLapply(seq_len(nrow(infParamsLLH)), function(id) {
  data.frame(
    t = times,
    J = sapply(seq_along(times), function(i) {
      g <- incDist[seq(i0, iMax), "pdf"]
      Qt <- genDistLLH[[id]][seq(i, iMax - i0 + i), "CDF"]
      Qt[is.na(Qt)] <- 1
      sum(g * Qt) * stepSize
    }, USE.NAMES = F)
  )
})
save(infProfLLH, genDistLLH, integralLLH, file = "data/savedDistributionsLLH.RData")

#' Create a zip file with the data files:
#' - data/savedDistributions.RData
#' - data/savedDistributionsLLH.RData
#' - data/infParamsLLH.RData
#' - data/genParamsLLH.RData
# tar(tarfile = "archivedDistributions.tar.gz", files = c("data/savedDistributions.RData", "data/savedDistributionsLLH.RData",
# "data/infParamsLLH.RData", "data/genParamsLLH.RData"), compression = "gzip", tar = "tar")
```

```{r compute-secondary-cases, include = F}
#' These are fast to compute, so I won't parallelise anything here
getSeconaryCases <- function(paramList) {
  #' Load pre-computed distributions
  load("data/savedDistributions.RData")
  #' We want to vectorise everything in terms of Delta1
  indexDelta1 <- sapply(paramList$Delta1, function(Delta1) which.min(abs(times - Delta1)), USE.NAMES = F)
  #' Loop over Re values
  out.df <- lapply(paramList$R, function(R) {
    #' Loop over f values
    lapply(paramList$f, function(f) {
      #' Loop over alpha values
      lapply(paramList$alpha, function(alpha) {
        #' Compute cases
        secondaryCases <- R * ((1-alpha)*f * infProfMLE[indexDelta1, "CDF"] + (1 - (1-alpha)*f))
        data.frame(
          alpha = factor(alpha, levels = paramList$alpha),
          f = factor(f, levels = paramList$f),
          Delta1 = factor(paramList$Delta1, levels = paramList$Delta1),
          R = factor(R, levels = paramList$R),
          secondaryCases = secondaryCases
        )
      }) %>% bind_rows()
    }) %>% bind_rows()
  }) %>% bind_rows()
  return(out.df)
}

getSeconaryCasesCI <- function(paramList) {
  #' Load pre-computed distributions
  load("data/savedDistributions.RData")
  load("data/savedDistributionsLLH.RData")
  #' We want to vectorise everything in terms of Delta1
  indexDelta1 <- sapply(paramList$Delta1, function(Delta1) which.min(abs(times - Delta1)), USE.NAMES = F)
  #' Loop over Re values
  out.df <- lapply(paramList$R, function(R) {
    #' Loop over f values
    lapply(paramList$f, function(f) {
      #' Loop over alpha values
      lapply(paramList$alpha, function(alpha) {
        #' Compute cases
        secondaryCases <- sapply(seq_along(infProfLLH), function(id) {
          R * ((1-alpha)*f * infProfLLH[[id]][indexDelta1, "CDF"] + (1 - (1-alpha)*f))
        })
        #' Extract lower and upper bounds on number of secondary cases
        lower <- apply(secondaryCases, 1, min)
        upper <- apply(secondaryCases, 1, max)
        data.frame(
          alpha = factor(alpha, levels = paramList$alpha),
          f = factor(f, levels = paramList$f),
          Delta1 = factor(paramList$Delta1, levels = paramList$Delta1),
          R = factor(R, levels = paramList$R),
          lower = lower,
          upper = upper
        )
      }) %>% bind_rows()
    }) %>% bind_rows()
  }) %>% bind_rows()
  return(out.df)
}
```

```{r compute-summary, include = F}
#' Compute the threshold between controlled and uncontrolled epidemics (R == 1)
#' This only applies to testing and isolating (i.e. functions of alpha, f, and Delta1)
getSummary <- function(data, value) {
  #' Check data
  requiredNames <- c("alpha","f","R","Delta1",value)
  if (!all(requiredNames %in% names(data))) {
    stop(paste0("Missing data columns: data must contain [", paste0(requiredNames, collapse = ", "), "]"))
  }
  
  #' Compute summary for each a, f, Re combination
  out.df <- by(data, INDICES = list(data$alpha, data$f, data$R), function(df) {
    df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1]
    Tc <- -Inf
    if (all(df[[value]] < 1)) Tc <- Inf
    else if (any(df[[value]] < 1) & any(df[[value]] > 1)) Tc <- approx(x = df[[value]], y = df$Delta1, xout = 1)$y
    data.frame(
      alpha = factor(unique(df$alpha), levels = levels(data$alpha)),
      f = factor(unique(df$f), levels = levels(data$f)),
      R = factor(unique(df$R), levels = levels(data$R)),
      Delta1 = Tc
    )
  })
  #' Combine and return
  do.call(rbind, out.df)
}
```

```{r compute-tertiary-cases, include = F}
getTertiaryCases <- function(paramList, times, stepSize, infProf.df, genDist.df, integral.df) {
  #' Compute functions and integrals for each (Delta_1, tau, Delta_2) combination
  functions <- lapply(paramList$Delta1, function(Delta1) {
    index_Delta1 <- which.min(abs(times - Delta1))
    P_Delta1 <- infProf.df[[index_Delta1, "CDF"]]
    J_Delta1 <- integral.df[[index_Delta1, "J"]]
    
    lapply(paramList$tau, function(tau) {
      index_minusTau <- which.min(abs(times + tau))
      P_minusTau <- infProf.df[[index_minusTau, "CDF"]]
      P_diff <- P_Delta1 - P_minusTau
      
      lapply(paramList$Delta2, function(Delta2) {
        integral <- 0
        if(any(paramList$g > 0)) {
          p <- infProf.df[seq(index_minusTau,index_Delta1),"pdf"]
          index_lower <- which.min(abs(times - (Delta1 + Delta2 + tau)))
          index_upper <- which.min(abs(times - Delta2))
          Q <- genDist.df[seq(index_lower,index_upper),"CDF"]
          integral <- sum(p * Q) * stepSize
        }
        data.frame(
          Delta1 = factor(Delta1, levels = paramList$Delta1),
          tau = factor(tau, levels = paramList$tau),
          Delta2 = factor(Delta2, levels = paramList$Delta2),
          P_Delta1 = P_Delta1,
          P_minusTau = P_minusTau,
          P_diff = P_diff,
          J_Delta1 = J_Delta1,
          integral = integral
        )
      }) %>%bind_rows()
    }) %>%bind_rows()
  }) %>%bind_rows()
  
  #' Finally loop over the values of (alpha,f,g,R) and sum up all the cases per parameter set
  paramDF <- expand.grid(R = paramList$R, alpha = paramList$alpha, f = paramList$f, g = paramList$g, KEEP.OUT.ATTRS = F)
  out.df <- lapply(seq_len(nrow(paramDF)), function(i) {
    R <- paramDF[[i,"R"]]
    alpha <- paramDF[[i,"alpha"]]
    f <- paramDF[[i,"f"]]
    g <- paramDF[[i,"g"]]
    
    secondaryCases <- R * ((1-alpha)*f * functions$P_Delta1 + (1 - (1-alpha)*f))
    
    F_f_Delta1 <- (1-alpha)*f * functions$J_Delta1 + (1 - (1-alpha)*f)
    
    tertiaryCases_detected <- R^2 * (1-alpha)*f*g * functions$integral
    tertiaryCases_undetected <- R^2 * (1-alpha)*f*(1-g) * functions$P_diff * F_f_Delta1
    tertiaryCases_outside <- R^2 * (1-alpha)*f * functions$P_minusTau * F_f_Delta1
    tertiaryCases_indexUndetected <- R^2 * (1 - (1-alpha)*f) * F_f_Delta1
    
    tertiaryCases <- tertiaryCases_detected + tertiaryCases_undetected +
        tertiaryCases_outside + tertiaryCases_indexUndetected
    
    
    # Now tertiary cases caused by asymptomatic secondary cases only
    tertiaryCases_asymptomatic <- alpha * tertiaryCases_detected +
      (alpha / F_f_Delta1) * (tertiaryCases_undetected + tertiaryCases_outside + tertiaryCases_indexUndetected)
    
    data.frame(
      R = factor(R, levels = paramList$R),
      alpha = factor(alpha, levels = paramList$alpha),
      f = factor(f, levels = paramList$f),
      g = factor(g, levels = paramList$g),
      Delta1 = functions$Delta1,
      tau = functions$tau,
      Delta2 = functions$Delta2,
      secondaryCases = secondaryCases,
      tertiaryCases = tertiaryCases,
      ter.per.sec = ifelse(secondaryCases > 0, tertiaryCases/secondaryCases, 0),
      tertiaryCases_asymptomatic = tertiaryCases_asymptomatic,
      frac_asymptomatic = ifelse(tertiaryCases > 0, tertiaryCases_asymptomatic/tertiaryCases, 0)
    )
  }) %>% bind_rows()
  return(out.df)
}
```

```{r compute-tertiary-cases-no-tracing, include = F}
#' To enable parallelisation, we pass everything as arguments
getTertiaryCasesNoTracing <- function(paramList, times, infProf.df, integral.df) {
  #' We want to vectorise everything in terms of Delta1
  indexDelta1 <- sapply(paramList$Delta1, function(Delta1) which.min(abs(times - Delta1)), USE.NAMES = F)
  #' Loop over Re values
  out.df <- lapply(paramList$R, function(R) {
    #' Loop over f values
    lapply(paramList$f, function(f) {
      #' Loop over alpha values
      lapply(paramList$alpha, function(alpha) {
        #' Compute cases
        secondaryCases <- R * ((1-alpha)*f * infProf.df[indexDelta1, "CDF"] + (1 - (1-alpha)*f))
        tertiaryCases <- R^2 * ((1-alpha)*f * infProf.df[indexDelta1, "CDF"] + (1 - (1-alpha)*f)) * ((1-alpha)*f * integral.df[indexDelta1, "J"] + (1 - (1-alpha)*f))
        
        data.frame(
          alpha = factor(alpha, levels = paramList$alpha),
          f = factor(f, levels = paramList$f),
          Delta1 = factor(paramList$Delta1, levels = paramList$Delta1),
          R = factor(R, levels = paramList$R),
          secondaryCases = secondaryCases,
          tertiaryCases = tertiaryCases,
          ter.per.sec = ifelse(secondaryCases > 0, tertiaryCases/secondaryCases, 0)
        )
      }) %>% bind_rows()
    }) %>% bind_rows()
  }) %>% bind_rows()
  return(out.df)
}
```

# Results
## Reducing transmission by testing \& isolating
Based on our transmission model, testing \& isolating alone (i.e. without additional contact tracing \& quarantine) is capable of suppressing epidemic growth ($R_{\rm TI}<1$) with a baseline $R$-value in the absence of TTIQ of up to 1.76 [95\% confidence interval (CI): 1.57,1.98], assuming that asymptomatic individuals contribute $\alpha=20\%$ of infections (Fig. \ref{fig:max-asymptomatic}).
To achieve this level of suppression, each symptomatic individual ($f=100\%$) would have to isolate immediately at symptom onset ($\Delta_1=0$ days), representing the upper limit of testing \& isolation performance.
We again note that the baseline $R$ parameter depends on the current suppression measures against SARS-CoV-2 transmission (social distancing, mask wearing, home office, etc., but not TTIQ interventions), as well as seasonality and levels of immunity/vaccination.
Importantly, this predicted upper limit of $R=1.76$ is below the estimated $R_0$ of SARS-CoV-2 ($R_0 \approx 2.5$; \citet{riou:Eurosurveillance:2020}).
Hence, testing \& isolating alone as a control strategy would not have been sufficient to prevent epidemic growth, even before the emergence of more transmissible variants.
One would have to reduce the number of susceptible individuals in the population by at least 30\% (e.g. through vaccination) for testing \& isolating alone to be a viable strategy.

```{r tertiaryCases-noTracing-line, include = F}
#' Tertiary cases as a function of f and Delta1
paramList <- list(
  alpha = 0.2,
  f = seq(0, 1, 0.05),
  Delta1 = seq(0,4,1),
  R = seq(1.1, 1.5, 0.1)
)
load("data/savedDistributions.RData")
tertiaryCasesMLE <- getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                              infProf.df = infProfMLE,
                                              integral.df = integralMLE)

R.plotVals <- c("1.1", "1.3", "1.5")
Delta1.plotVals <- c("0", "2", "4")

#' Prepare and plot
df <- tertiaryCasesMLE
df$f <- as.numeric(levels(df$f))[df$f]
tertiaryCasesPlot <- ggplot(df[df$R %in% R.plotVals & df$Delta1 %in% Delta1.plotVals,], aes(x = f, y = ter.per.sec, colour = Delta1, group = Delta1)) +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  geom_line(size = lineSize) +
  facet_grid(~R) +
  scale_color_manual(values = colorRampPalette(brewer.pal(9, "Oranges"))(12)[c(5,8,10)],
    name = expression("delay" ~ (Delta[1])),
                      aesthetics = c("colour","fill"),
                      labels = dayLabels(levels(df$Delta1))) +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0.45,1.55), expand = F) +
  labs(x = expression("fraction of symptomatic individuals isolated" ~ (f)), y = expression(R[TI])) +
  plotTheme + theme(legend.position = "right", panel.spacing = unit(1.5, "lines"), strip.background = element_blank(), strip.text = element_blank())

#' Do we add CI to the plot?
addCI <- T
if (addCI) {
  out.filename <- "data/tertiaryCases-noTracing-lines-CI.RData"
  if (file.exists(out.filename)) {
    #' Load the data, format, and add layer to plot
    load(out.filename)
    tertiaryCasesCI <- out.df
    tertiaryCasesCI$f <- as.numeric(levels(tertiaryCasesCI$f))[tertiaryCasesCI$f]
    df <- merge(df,tertiaryCasesCI)
    tertiaryCasesPlot$layers <- c(
      geom_ribbon(data = df[df$R %in% R.plotVals & df$Delta1 %in% Delta1.plotVals,], aes(ymin = lower, ymax = upper, fill = Delta1), alpha = 0.4, colour = "transparent"),
      tertiaryCasesPlot$layers)
  } else {
    print(paste("File", out.filename, "is missing."))
    print("Pass the following info file to TTIQ-CIscript.R to generate confidence intervals, and then try again.")
    #' Create file "info-for-script_xx.RData" here
    #' First we create the function which accepts id and idGD and returns a
    #' dataframe for the number of cases for the given parameter set
    func <- function(id, idGD) {
      getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                infProf.df = infProfLLH[[id]], integral.df = integralLLH[[idGD]])
    }
    #' Which output do we want to construct the CI for?
    output <- "ter.per.sec"
    #' Now add the function(s), parameter list, and the output filename
    info.filename <- "info-for-script_1.RData"
    save(func, getTertiaryCasesNoTracing, output, paramList, out.filename, file = info.filename)
    print(paste("Info file is", info.filename))
    print(paste("The output file", out.filename, "can be generated by executing:"))
    #' R --vanilla --slave < TTIQ-CIscript.R --args info-for-script_xx.RData run
    print(paste("R --vanilla --slave < TTIQ-CIscript.R --args", info.filename, "run"))
    addCI <- FALSE
  }
}

# #' Read values
# dat <- df[df$R == "1.1" & df$Delta1 == "0",]
# sapply(c("ter.per.sec","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$R == "1.1" & df$Delta1 == "2",]
# sapply(c("ter.per.sec","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$R == "1.1" & df$Delta1 == "4",]
# sapply(c("ter.per.sec","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$R == "1.5" & df$Delta1 == "0",]
# sapply(c("ter.per.sec","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$R == "1.5" & df$Delta1 == "2",]
# sapply(c("ter.per.sec","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
```

```{r tertiaryCases-noTracing-density, include = F}
#' Critical region where epidemic is controlled
paramList <- list(
  alpha = 0.2,
  f = seq(0, 1, 0.05),
  Delta1 = seq(0,6,0.2),
  R = seq(1.1, 1.5, 0.1)
)
load("data/savedDistributions.RData")
tertiaryCasesMLE <- getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                              infProf.df = infProfMLE, integral.df = integralMLE)

R.plotVals <- c("1.1", "1.3", "1.5")

#' Prepare and plot
df <- tertiaryCasesMLE
df$f <- as.numeric(levels(df$f))[df$f]
df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1]
df$R <- as.character(df$R)

cases.min <- 0.5
cases.max <- 1.5

tertiarySummaryPlot <- ggplot(df[df$R %in% R.plotVals,], aes(x = f, y = Delta1, group = R)) +
  geom_raster(aes(fill = ter.per.sec), interpolate = T) +
  geom_contour(aes(z = ter.per.sec), colour = "black", size = lineSize, breaks = 1) +
  scale_fill_gradientn(colours = c("seagreen","white","white","white","salmon"),
                       name = expression(R[TI]),
                       values = scales::rescale(c(cases.min,1-.Machine$double.eps,1,1+.Machine$double.eps,cases.max)),
                       limits = c(cases.min,cases.max)) +
  facet_grid(~R, labeller = label_bquote(cols = R~"="~.(R))) +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0, 4), expand = F) +
  labs(x = expression("fraction of symptomatic individuals isolated" ~ (f)),
       y = expression(atop("delay to isolation" ~ (Delta[1]),"(days after symptoms)"))) +
  plotTheme +
  theme(legend.position = "right", panel.spacing = unit(1.5, "lines"))

#' Do we add CI to the plot?
addCI <- T
if (addCI) {
  out.filename <- "data/tertiaryCases-noTracing-density-CI.RData"
  if (file.exists(out.filename)) {
    #' Load the data, format, and add layer to plot
    load(out.filename)
    tertiaryCasesCI <- out.df
    tertiaryCasesCI$f <- as.numeric(levels(tertiaryCasesCI$f))[tertiaryCasesCI$f]
    df <- merge(df,tertiaryCasesCI)
    tertiarySummaryPlot <- tertiarySummaryPlot +
      geom_contour(data = df[df$R %in% R.plotVals,], aes(z = lower), colour = "black", linetype = "dashed", breaks = 1) +
      geom_contour(data = df[df$R %in% R.plotVals,], aes(z = upper), colour = "black", linetype = "dashed", breaks = 1)
  } else {
    print(paste("File", out.filename, "is missing."))
    print("Pass the following info file to TTIQ-CIscript.R to generate confidence intervals, and then try again.")
    #' Create file "info-for-script_xx.RData" here
    #' First we create the function which accepts id and idGD and returns a
    #' dataframe for the number of cases for the given parameter set
    func <- function(id, idGD) {
      getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                infProf.df = infProfLLH[[id]], integral.df = integralLLH[[idGD]])
    }
    #' Which output do we want to construct the CI for?
    output <- "ter.per.sec"
    #' Now add the function(s), parameter list, and the output filename
    info.filename <- "info-for-script_2.RData"
    save(func, getTertiaryCasesNoTracing, output, paramList, out.filename, file = info.filename)
    print(paste("Info file is", info.filename))
    print(paste("The output file", out.filename, "can be generated by executing:"))
    #' R --vanilla --slave < TTIQ-CIscript.R --args info-for-script_xx.RData run
    print(paste("R --vanilla --slave < TTIQ-CIscript.R --args", info.filename, "run"))
    addCI <- FALSE
  }
}
```

```{r tertiaryCases-noTracing, dependson = c(-1,-2,-3), fig.height = 6, fig.width = 10, fig.cap = caption}
caption <- "The reproductive number $R_{\\rm TI}$ under testing \\& isolation only.
A) The impact of testing \\& isolation on $R_{\\rm TI}$ as a function of the fraction of symptomatic individuals that are isolated ($f$; x-axis) and delay to isolation after symptom onset ($\\Delta_1$; y-axis) for different baseline $R$ values (columns).
The black line represents the criticial reproductive number $R_{\\rm TI} = 1$.
Above this line (red zone) we have on average more than one secondary infection per infected and the epidemic is growing.
Below this line we have less than one secondary infection per infected and the epidemic is suppressed.
Dashed lines are the 95\\% confidence interval for this threshold, representing the uncertainty in the inferred generation time distribution and infectivity profile.
B) Lines correspond to slices of panel A at a fixed delay to isolation $\\Delta_1=$ 0, 2, or 4 days after symptom onset (colour).
Shaded regions are 95\\% confidence intervals for the reproductive number, representing the uncertainty in the inferred generation time distribution and infectivity profile.
Horizontal grey line is the threshold for epidemic control ($R_{\\rm TI} = 1$).
We fix the fraction of transmission that is attributed to asymptomatic infections to $\\alpha=20\\%$."

#' Combine plots
plots <- list(tertiaryCasesPlot, tertiarySummaryPlot)
legends <- list(get_legend(plots[[1]]), get_legend(plots[[2]]))
plots <- lapply(plots, function(plot) plot + theme(legend.position = "none"))

plot_grid(
  plot_grid(plotlist = list(plots[[2]],plots[[1]]), ncol = 1, axis = "lr", align = "hv", rel_heights = c(1,1), labels = "AUTO"),
  plot_grid(plotlist = list(legends[[2]],legends[[1]]), ncol = 1, rel_heights = c(1,1)),
  rel_widths = c(1,0.1), ncol = 2
)
```

The region of $(f,\Delta_1)$ parameter space in which $R_{\rm TI}$ is less than one, i.e. the region in which an epidemic can be controlled by testing \& isolating, is shrinking for higher $R$ epidemics (Fig. \ref{fig:tertiaryCases-noTracing}A).
Higher testing \& isolation coverage ($f$) or shortened delays between symptom onset and isolation ($\Delta_1$) are required to control SARS-CoV-2 outbreaks as $R$ increases.
By increasing the fraction of symptomatic individuals that are isolated ($f$), there can be a greater delay to isolation without any increase in $R_{\rm TI}$, but with diminishing returns.

A SARS-CoV-2 outbreak with $R=1.1$ (in the absence of TTIQ) can be controlled by isolating as few as 21\% [95\% confidence interval (CI): 18\%,25\%] of symptomatic individuals at the time of symptom onset ($\Delta_1=0$ days) (Fig. \ref{fig:tertiaryCases-noTracing}B).
If the symptomatic individuals wait $\Delta_1=2$ days after symptom onset before isolating (i.e. they wait for a test result), then 39\% [CI: 30\%,54\%] of symptomatic infecteds would have to be isolated for the epidemic to be controlled.
Isolating after $\Delta_1=4$ days would be insufficient to control the epidemic even if all symptomatic individuals were isolated [CI: 63\%,n.a.].
For faster-spreading SARS-CoV-2 outbreaks ($R=1.5$ in the absence of TTIQ), we would require 77\% [CI: 67\%,91\%] of symptomatic infecteds to be isolated immediately after they develop symptoms ($\Delta_1=0$ days) to control the epidemic.
With a delay $\Delta_1 \ge 2$ days, testing & isolating would be insufficient to control the epidemic even if 100\% of symptomatic infecteds are isolated.

We have predominantly focussed on $\alpha=20\%$ of infections being attributable to persistently-asymptomatic individuals in the absence of TTIQ.
As this fraction increases, we observe a linear increase in $R_{\rm TI}$, i.e. increasing the fraction of transmission that is attributable to asymptomatic infections leads to reduced efficacy of testing \& isolating, as fewer cases are identified by testing only symptomatics (Fig. \ref{fig:asymptomatic-impact}A).
It should be noted that under testing \& isolation measures, a larger fraction of onward transmission is attributable to asymptomatic infections when compared to the scenario of no TTIQ (Fig. \ref{fig:asymptomatic-impact}B).


## Reducing transmission by additional contact tracing \& quarantine
With additional contact tracing \& quarantine, the theoretical upper limit of TTIQ efficacy is greatly increased compared to testing \& isolation alone.
Under perfect conditions with all contacts quarantined immediately after symptom onset in the index case, TTIQ can suppress epidemics with a baseline $R$-value in the absence of TTIQ of up to 4.24 [95\% CI: 3.10,5.83] (Fig. \ref{fig:max-asymptomatic} for $\alpha = 20\%$).
However, it is unlikely that such a high level of suppression could be achieved in practice due to delays and inaccuracies in the contact tracing process.

Under ideal TTIQ conditions, additional tracing \& quarantine can more than double the effectiveness of the intervention compared to testing \& isolation alone (Fig. \ref{fig:max-asymptomatic}).
However, under more realistic expectations of inaccuracies and delays in the TTIQ processes, the majority of transmission is prevented by testing \& isolation if less than $g=60\%$ of contacts are quarantined (Fig. \ref{fig:quarantine-vs-isolation}).

We can visualise the additional benefit that contact tracing \& quarantine brings to testing \& isolation in our model by gradually increasing the fraction of contacts of index cases that are isolated, $g$.
For $g=0$, no contacts are traced \& quarantined, and hence we return to the testing \& isolation strategy (Fig. \ref{fig:tertiaryCases-noTracing}).
By increasing $g$, we expand the parameter space in which $R_{\rm TTIQ}<1$ (Fig. \ref{fig:tertiaryCases-CT}), i.e. contact tracing allows an epidemic to be controlled for lower fractions of index cases found ($f$) and/or longer delays to isolating the index case after they develop symptoms ($\Delta_1$).
Furthermore, for a given set of testing \& isolation parameters $f$ and $\Delta_1$, we can control higher $R$-value epidemics with contact tracing \& quarantine that would be otherwise uncontrollable.

```{r tertiaryCases-CT, fig.height = 3, fig.width = 10, fig.cap = caption}
caption <- "The impact of tracing \\& quarantine on the reproductive number $R_{\\rm TTIQ}$ as a function of the fraction of symptomatic individuals that are isolated ($f$; x-axis) and delay to isolation after symptom onset ($\\Delta_1$; y-axis), for different contact tracing \\& quarantine success probabilities $g$ (colour) across different baseline $R$ values (columns).
We fix $\\Delta_2 = 2$ days and $\\tau = 2$ days.
The contours separate the regions where the epidemic is growing ($R_{\\rm TTIQ}>1$; top-left) and the epidemic is suppressed ($R_{\\rm TTIQ}<1$; bottom-right).
The contours for $g=0$ are equivalent to the contours in Fig. \\ref{fig:tertiaryCases-noTracing}.
We fix the fraction of transmission that is attributed to asymptomatic infections to $\\alpha=20\\%$.
We do not show confidence intervals for clarity of presentation."

paramList <- list(
  alpha = 0.2,
  f = seq(0,1,0.05),
  g = seq(0,1,0.2),
  Delta1 = seq(0,4,0.2),
  Delta2 = 2,
  tau = 2,
  R = c(1.1,1.3,1.5)
)
load("data/savedDistributions.RData")
tertiaryCasesMLE <-
  getTertiaryCases(paramList = paramList, times = times, stepSize = stepSize, infProf.df = infProfMLE,
                   genDist.df = genDistMLE, integral.df = integralMLE)

#' Prepare and plot
df <- tertiaryCasesMLE
df$f <- as.numeric(levels(df$f))[df$f]
df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1]
df$R <- as.character(df$R)

ggplot(df, aes(x = f, y = Delta1, group = g)) +
  geom_contour(aes(colour = g, z = ter.per.sec), size = lineSize, breaks = 1) +
  facet_grid(~R, labeller = label_bquote(cols = R~"="~.(R))) +
  scale_colour_viridis_d(end = 0.9, labels = scales::percent(as.numeric(levels(df$g))),
                         name = "fraction of\ncontacts\nquarantined (g)") +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0, 4), expand = F) +
  labs(x = expression("fraction of symptomatic individuals isolated" ~ (f)),
       y = expression(atop("delay to isolation" ~ (Delta[1]),"(days after symptoms)"))) +
  plotTheme +
  theme(legend.position = "right", panel.spacing = unit(1.5, "lines"), panel.grid.major.y = element_blank()) +
  annotate("text", label = expression(R[TTIQ]~"> 1"), size = theme_get()$text[["size"]]/4,
           x = 0.01, y = 3.78,
           hjust = 0, vjust = 0) +
  annotate("text", label = expression(R[TTIQ]~"< 1"), size = theme_get()$text[["size"]]/4,
           x = 0.819, y = 0.05,
           hjust = 0, vjust = 0)
```

To obtain a systematic understanding of the impact that each parameter of the TTIQ process has on the effective reproductive number $R_{\rm TTIQ}$, we can individually vary each of the five TTIQ parameters.
To this end, we calculate $R_{\rm TTIQ}$ for focal parameter sets of $(f,g,\Delta_1,\Delta_2,\tau)$.
We then perturb each single parameter, keeping the remaining four parameters fixed, and compute the new value of $R_{\rm TTIQ}$ (Fig. \ref{fig:tertiaryCases-single}).

```{r tertiaryCases-single, fig.height = 8, fig.width = 10, fig.cap = caption}
caption <- "The response of the reproductive number $R_{\\rm TTIQ}$ to single TTIQ parameter pertubations.
We set the baseline $R=1.5$ throughout, which is the intensity of the epidemic in the absence of any TTIQ intervention.
We consider four focal TTIQ parameter combinations, with $f \\in \\{30\\%,70\\%\\}$, $\\Delta_1 \\in \\{0,2\\}$ days, $g = 50\\%$, $\\Delta_2=1$ day, and $\\tau=2$ days.
$R_{\\rm TTIQ}$ for the focal parameter sets are shown as thin black lines.
With $f=0$ (no TTIQ) we expect $R_{\\rm TTIQ} = R$ (upper grey line).
We then vary each TTIQ parameter individually, keeping the remaining four parameters fixed at the focal values.
The upper panel shows the probability parameters $f$ and $g$, while the lower panel shows the parameters which carry units of time (days).
The critical threshold for controlling an epidemic is $R_{\\rm TTIQ} = 1$ (lower grey line).
We fix the fraction of transmission that is attributed to asymptomatic infections to $\\alpha=20\\%$."

#' Function to perturb a single parameter and compute tertiary cases
perturbParam <- function(paramList, param, values, addCI = F, index = NULL) {
  load("data/savedDistributions.RData")
  #' Perturb values
  paramList[[param]] <- values
  tertiaryCases <- getTertiaryCases(paramList = paramList, times = times,
                                    stepSize = stepSize, infProf.df = infProfMLE,
                                    genDist.df = genDistMLE, integral.df = integralMLE)
  df <- data.frame(
    param = factor(param, levels = c("f", "g", "Delta1", "Delta2", "tau")),
    value = values,
    ter.per.sec = tertiaryCases$ter.per.sec
  )
  
  #' Now add confidence intervals
  if (addCI) {
    out.filename <- paste0("data/tertiaryCases-single-CI_", param, "_", index, ".RData")
    if (file.exists(out.filename)) {
      #' Load files and add lower and upper to df
      load(out.filename)
      df$lower <- out.df$lower
      df$upper <- out.df$upper
    } else {
      #' Need to create file
      print(paste("File", out.filename, "is missing."))
      print("Pass the following info file to TTIQ-CIscript.R to generate confidence intervals, and then try again.")
      #' Create file "info-for-script_xx.RData" here
      #' First we create the function which accepts id and idGD and returns a
      #' dataframe for the number of cases for the given parameter set
      func <- function(id, idGD) {
        getTertiaryCases(paramList = paramList, times = times,
                         stepSize = stepSize, infProf.df = infProfLLH[[id]],
                         genDist.df = genDistLLH[[idGD]], integral.df = integralLLH[[idGD]])
      }
      #' Which output do we want to construct the CI for?
      output <- "ter.per.sec"
      #' Now add the function(s), parameter list, and the output filename
      info.filename <- paste0("info-for-script_", param, "_", index,".RData")
      save(func, getTertiaryCases, output, paramList, out.filename, file = info.filename)
      print(paste("Info file is", info.filename))
      print(paste("The output file", out.filename, "can be generated by executing:"))
      #' R --vanilla --slave < TTIQ-CIscript.R --args info-for-script_xx.RData run
      print(paste("R --vanilla --slave < TTIQ-CIscript.R --args", info.filename, "run"))
      addCI <- FALSE
    }
  }
  if (!addCI) {
    df$lower <- 1
    df$upper <- 1
  }
  return(df)
}

#' Function to compute all data
getData <- function(focalParams, addCI = F, index = NULL) {
  #' Number of cases under focal parameters
  load("data/savedDistributions.RData")
  focal <- getTertiaryCases(paramList = focalParams, times = times,
                            stepSize = stepSize, infProf.df = infProfMLE,
                            genDist.df = genDistMLE, integral.df = integralMLE)
  #' Perturb parameters and compute cases
  df <- rbind(
    perturbParam(paramList = focalParams, param = "f", values = seq(0,1,0.05), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "g", values = seq(0,1,0.05), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "Delta1", values = seq(0,5,0.25), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "Delta2", values = seq(0,5,0.25), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "tau", values = seq(0,5,0.25), addCI = addCI, index = index)
  )
  df$index <- factor(index)
  focal$index <- factor(index)
  return(list(focal = focal, perturbed = df))
}


#' Now compute data
focalParams.list <- list(
  list(f = 0.3, g = 0.5, Delta1 = 2, Delta2 = 1, tau = 2, R = 1.5, alpha = 0.2),
  list(f = 0.3, g = 0.5, Delta1 = 0, Delta2 = 1, tau = 2, R = 1.5, alpha = 0.2),
  list(f = 0.7, g = 0.5, Delta1 = 2, Delta2 = 1, tau = 2, R = 1.5, alpha = 0.2),
  list(f = 0.7, g = 0.5, Delta1 = 0, Delta2 = 1, tau = 2, R = 1.5, alpha = 0.2)
)
allData <- lapply(seq_along(focalParams.list), function(index) {
  getData(focalParams = focalParams.list[[index]], addCI = T, index = index)
})
focalCases <- lapply(allData, function(data) data$focal) %>% bind_rows()
focalCases$R <- as.numeric(levels(focalCases$R))[focalCases$R]
perturbedCases <- lapply(allData, function(data) data$perturbed) %>% bind_rows()

#' Plot
yLim <- c(0.45, 1.55)
plot_labeller <- function(variable, value) {
  labs <- lapply(value, function(index) {
    index <- as.numeric(levels(value))[index]
    lett <- LETTERS[index]
    #f <- focalParams.list[[index]]$f
    f <- scales::percent(focalParams.list[[index]]$f)
    Delta1 <- focalParams.list[[index]]$Delta1
    bquote(bold(.(lett))~~"f ="~.(f)*","~Delta[1]~"="~.(Delta1)~"days")
  })
  return(labs)
}
myColours <- c(f = palette_OkabeIto[1], g = palette_OkabeIto[4],
               Delta1 = palette_OkabeIto[2], Delta2 = palette_OkabeIto[5],
               tau = palette_OkabeIto[3])

probPlot <- ggplot(perturbedCases[perturbedCases$param %in% c("f","g"),],
                   aes(x = value, y = ter.per.sec, colour = param)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = param),
              alpha = 0.4, colour = "transparent") +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = R),
             colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = ter.per.sec),
             colour = "black") +
  geom_line(size = lineSize) + 
  facet_grid(~index, labeller = plot_labeller) +
  scale_colour_manual(values = myColours,
                      aesthetics = c("colour","fill"), name = NULL) +
  coord_cartesian(xlim = c(0,1), ylim = yLim, expand = F) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "probability", y = expression(R[TTIQ])) +
  plotTheme + theme(plot.margin = unit(c(1.1,0.2,0,0), "lines"),
                    legend.position = "right",#c(0.95,0.7),
                    legend.text = element_text(size = 14),
                    legend.key.size = unit(1.5,"line"),
                    strip.text = element_text(hjust = 0, size = 13),
                    panel.spacing = unit(1.5,"lines"))

myLabs <- expression(Delta[1],Delta[2],tau)
names(myLabs) <- c("Delta1","Delta2","tau")
timePlot <- ggplot(perturbedCases[perturbedCases$param %in% c("Delta1","Delta2","tau"),],
                   aes(x = value, y = ter.per.sec, colour = param)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = param),
              alpha = 0.4, colour = "transparent") +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = R),
             colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = ter.per.sec),
             colour = "black") +
  geom_line(size = lineSize) +
  facet_grid(~index) +
  scale_colour_manual(values = myColours,
                      aesthetics = c("colour","fill"), name = NULL, labels = myLabs) +
  coord_cartesian(xlim = c(0,5), ylim = yLim, expand = F) +
  labs(x = "time (days)", y = expression(R[TTIQ])) +
  plotTheme + theme(plot.margin = unit(c(1,0.2,0,0), "lines"),
                    legend.position = "right",#c(0.82,0.7),
                    legend.text = element_text(size = 14, hjust = 0.5),
                    legend.key.size = unit(1.5,"line"),
                    strip.background = element_blank(),
                    strip.text = element_blank(),
                    panel.spacing = unit(1.5,"lines"))

#' Extract legend and then plot together
probPlot.legend <- get_legend(probPlot)
timePlot.legend <- get_legend(timePlot)
probPlot <- probPlot + theme(legend.position = "none")
timePlot <- timePlot + theme(legend.position = "none")

plot_grid(
  plot_grid(probPlot, timePlot, nrow = 2),
  plot_grid(probPlot.legend, timePlot.legend, nrow = 2),
  nrow = 1, rel_widths = c(1,0.08)
)
```

Modifying the fraction of symptomatic index cases that are identified and isolated ($f$) has the largest effect of all parameter changes.
By identifying more index cases (increasing $f$), we not only prevent the onward transmission to secondary contacts through isolation, but we also allow infected contacts to be traced and quarantined.

Increasing the fraction of secondary contacts that are quarantined ($g$) has a smaller benefit than increasing $f$.
If only 30\% of symptomatic index cases are identified, then increasing $g$ results in a small reduction of $R_{\rm TTIQ}$ and for $R=1.5$ the epidemic cannot be controlled even if all secondary contacts ($g=100\%$) of known index cases are quarantined (Figs. \ref{fig:tertiaryCases-single}A \& B).
However, if a large fraction of symptomatic index cases are identified ($f=70\%$), then increasing $g$ can control an epidemic that would be out of control in the absence of contact tracing (Figs. \ref{fig:tertiaryCases-single}C \& D).

After increasing $f$, the next most effective control strategy is to reduce the delay between symptom onset and isolation of the index case ($\Delta_1$).
Reducing the time taken to quarantine secondary contacts ($\Delta_2$) has a lesser effect on $R_{\rm TTIQ}$.
Finally, looking back further while contact tracing (increasing $\tau$) allows more secondary contacts to be traced and quarantined.
However, this does not translate into a substantial reduction in $R_{\rm TTIQ}$ as the extra contacts which are traced have already been infectious for a long time, and will thus have less remaining infectivity potential to be prevented by quarantine.
Hence increasing $\tau$ comes with diminishing returns.

To check the robustness of these effects across all parameter combinations (not just perturbing a single parameter), we randomly sampled parameter combinations $(f,g,\Delta_1,\Delta_2,\tau)$ and used linear discriminant analysis (LDA) to capture the impact that each parameter has on $R_{\rm TTIQ}$ (Fig. \ref{fig:LDA-1D-plot}).
We find that $f$ is the dominant parameter to determine the reproductive number, followed by $\Delta_1$, $g$, $\Delta_2$, and finally $\tau$ has the smallest impact.
Furthermore, by looking at the distribution of the randomly-sampled TTIQ parameters across different $R_{\rm TTIQ}$ values (Fig. \ref{fig:LDA-1D-dist}), we observe that low $f$ values are strongly associated with low TTIQ effectiveness (although a high $f$ value is not necessarily associated with high effectiveness).

The output of the LDA analysis is dependent on the range of parameter values from which we sample (Fig. \ref{fig:LDA-vary-range-plot}).
While $f$ and $g$ are naturally bounded from 0\% to 100\%, the time-valued parameters $\Delta_1$, $\Delta_2$, and $\tau$ have no natural upper limit.
Without empirical data to inform these prior distributions, we focus on durations from zero to five days as shown in Fig. \ref{fig:tertiaryCases-single}.

```{r LDA-1D-calc, message = F, include = F}
#' Parameter ranges
f.range <- list(min = 0, max = 1)
g.range <- list(min = 0, max = 1)
Delta1.range <- list(min = 0, max = 5)
Delta2.range <- list(min = 0, max = 5)
tau.range <- list(min = 0, max = 5)
R.range <- 1.5
alpha.range <- 0.2

#' Random uniform sampling
set.seed(42)
n <- 10000
f.unif <- runif(n = n, min = f.range$min, max = f.range$max)
g.unif <- runif(n = n, min = g.range$min, max = g.range$max)
Delta1.unif <- runif(n = n, min = Delta1.range$min, max = Delta1.range$max)
Delta2.unif <- runif(n = n, min = Delta2.range$min, max = Delta2.range$max)
tau.unif <- runif(n = n, min = tau.range$min, max = tau.range$max)

paramList <- lapply(seq_len(n), function(i) {
  list(
    f = f.unif[i],
    g = g.unif[i],
    Delta1 = Delta1.unif[i],
    Delta2 = Delta2.unif[i],
    tau = tau.unif[i],
    R = R.range,
    alpha = alpha.range
  )
})

#' Compute tertiary cases
load("data/savedDistributions.RData")
tertiaryCasesLDA <- ParLapply(seq_len(n), function(i) {
  getTertiaryCases(paramList = paramList[[i]], times = times, stepSize = stepSize,
                   infProf.df = infProfMLE, genDist.df = genDistMLE,
                   integral.df = integralMLE)
}) %>% bind_rows()

df <- tertiaryCasesLDA
#' Enumerate and normalise param values
df$f <- as.numeric(levels(df$f))[df$f] / (f.range$max - f.range$min)
df$g <- as.numeric(levels(df$g))[df$g] / (g.range$max - g.range$min)
df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1] / (Delta1.range$max - Delta1.range$min)
df$Delta2 <- as.numeric(levels(df$Delta2))[df$Delta2] / (Delta2.range$max - Delta2.range$min)
df$tau <- as.numeric(levels(df$tau))[df$tau] / (tau.range$max - tau.range$min)

names(df)[names(df) == "ter.per.sec"] <- "output"
df <- df[,c("output","f","g","Delta1","Delta2","tau")]


#' Discretise cases into bins
bin <- 0.1
output.min <- 0.0
output.max <- 1.5
df$class <- cut(df$output, seq(output.min, output.max, bin), right = F) #,labels = paste(seq(output.min, output.max-bin, bin), "< n[3] <", seq(output.min+bin, output.max, bin)))
#' Perform the LDA analysis based on the discrete Re class
lda <- lda(formula = class ~ f + g + Delta1 + Delta2 + tau, data = df)
#' Project the data onto the LDA space
plda <- predict(object = lda, newdata = df)
plot.df <- data.frame(class = df[,"class"], output = df[,"output"], lda = plda$x) # Output will be the category, lda.LD1 will be the new x-axis

ff <- seq(output.min, output.max-bin, bin) + bin/2
plot.df$discrete <- ff[as.numeric(plot.df$class)]

#' LDA proportion of variance per dimension
prop.lda <- lda$svd^2 / sum(lda$svd^2)

#' Add the parameter arrows and labels
lda.vectors <- as.data.frame(lda$scaling)
lda.vectors <- cbind(lda.vectors, data.frame(params = rownames(lda$scaling), stringsAsFactors = FALSE))
#' Reorder parameters
lda.vectors$params <- factor(lda.vectors$params, levels = c("tau","Delta2","g","Delta1","f"))
#' Parameter name as expression
expressions <- c(f = "f", g = "g", tau = "tau", Delta2 = "Delta[2]", Delta1 = "Delta[1]")
lda.vectors$param.expressions <- expressions[as.character(levels(lda.vectors$params))[lda.vectors$params]]

# Move to Appendix
# Save all the data that we need to plot the parameter sample distributions
save(df, expressions, ff, output.min, output.max, bin, file = "data/LDA-parameter-distributions.RData")
```

```{r LDA-1D-plot, dependson = c(-1), message = F, fig.height = 5, fig.cap = caption}
caption <- "Linear discriminant analysis (LDA) of the impact of TTIQ strategies on the reproductive number $R_{\\rm TTIQ}$.
We fix the baseline $R=1.5$ and $\\alpha=20\\%$, and then we randomly uniformly sample 10,000 parameter combinations from $f \\in [0\\%,100\\%]$, $g \\in [0\\%,1\\%]$, $\\Delta_1 \\in [0,5]$ days, $\\Delta_2 \\in [0,5]$ days, and $\\tau \\in [0,5]$ days.
The reproductive number is calculated for each TTIQ parameter combination, and the output ($R_{\\rm TTIQ}$) is categorised into bins of width $0.1$ (colour).
We then use LDA to construct a linear combination (LD1) of the five (normalised) TTIQ parameters which maximally separates the output categories.
We then predict the LD1 values for each parameter combination, and construct a histogram of these values for each category.
The lower panel shows the components of the primary linear discriminant vector (LD1).
By multiplying the (normalised) TTIQ parameters by the corresponding vector component, we arrive at the LD1 prediction which corresponds to the predicted reproductive number under that TTIQ strategy.
Longer arrows (larger magnitude components) correspond to a parameter having a larger effect on the reproductive number.
The distributions of parameters per categorised reproductive number is shown in Fig. \\ref{fig:LDA-1D-dist}."

#' Plot the LDA histograms
plot <- ggplot(plot.df, aes(x = lda.LD1, fill = discrete, group = fct_rev(class))) +
  stat_density(aes(y = ..count../nrow(plot.df)), position = "identity", alpha = 0.8, colour = "black") +
  scale_fill_gradientn(colours = c("seagreen","white","white", "white","salmon"), name = expression(R[TTIQ]),
                       values = scales::rescale(c(output.min,1-.Machine$double.eps,1,1+.Machine$double.eps,output.max)),
                       limits = c(output.min,output.max),
                       guide = guide_colorbar(direction = "horizontal")) +
  plotTheme + theme(legend.position = c(0.2,0.85)) +
  coord_cartesian(xlim = c(-1,1)*9, ylim = c(0,0.1), expand = F) +
  scale_x_continuous(breaks = seq(-10,10,2))
#' Add proportion to label
plot <- plot + labs(x = paste("LD1 (", scales::percent(prop.lda[1], accuracy = 0.1), ")", sep = ""), y = "frequency")

#' Plot the parameter arrows and labels
param.plot <- ggplot(lda.vectors, aes(x = 0, xend = LD1, y = params, yend = params)) +
  geom_vline(xintercept = 0) +
  geom_segment(lineend = "butt", size = lineSize, arrow = arrow(length = unit(0.2, "cm"))) +
  #geom_text(aes(label = params, x = ifelse(LD1 > 0, -1, 1))) +
  geom_text(aes(x = LD1 + sign(LD1)*0.6, label = format(round(LD1, digits = 2), nsmall = 2))) +
  geom_text(aes(label = param.expressions, x = sign(-LD1)*0.4), parse = T, size = 5) +
  coord_cartesian(xlim = c(-1,1)*9, ylim = c(0.5,5.5), expand = F) +
  labs(caption = expression("parameter impact on" ~ R[TTIQ])) +
  theme_void() +
  theme(plot.caption = element_text(hjust = 0.5, size = plotTheme$text$size),
        plot.margin = unit(c(1,0,1,0), units = "lines"))

#' Combine
plot_grid(plot, param.plot, nrow = 2, rel_heights = c(1,0.5), align = "v", axis = "lr")#, labels = "AUTO")
```

Finally, we comment on the role of asymptomatic transmission across the TTIQ intervention.
Although quarantine of a traced contact occurs independently of whether that contact will be symptomatic or asymptomatic, the probability that the contact is identified in the first place depends on whether the infector is asymptomatic or not.
Hence, TTIQ will decrease in effectiveness as the fraction of transmission that is attributable to asymptomatic individuals ($\alpha$) increases (Fig. \ref{fig:asymptomatic-impact}A).


# Discussion
By combining empirically well-supported estimates of the infection timing of SARS-CoV-2 with a simple model of transmission dynamics, we have calculated the impact of test-trace-isolate-quarantine (TTIQ) interventions against the spread of COVID-19.
Under idealised conditions, testing \& isolation plus contact tracing \& quarantine can prevent substantially more transmission that testing \& isolation only.
However, the effects of delays and inaccuracies in the TTIQ processes are compounded for contact tracing \& quarantine, which ultimately relies on index case identification to be effective.
If we ignore this compounding effect, then we would potentially be overestimating the impact that contact tracing can have on transmission reduction.
Based on our systematic analysis, we find that the greatest improvement to the TTIQ process would come from increased identification and isolation of symptomatic index cases and reduction of delay between symptom onset and isolation.
These parameters contribute to the direct reduction of onward infection from an index case, and optimising them allows more contacts to be traced earlier.

Increasing the duration of the contact tracing window by looking back further in time has limited return under our model of forward contact tracing (identifying who was infected by the index case).
However, if we were interested in identifying the source of infection (backwards contact tracing), then increasing the duration of the contact tracing window could lead to the identification of transmission clusters.

When comparing to the findings of \citet{ferretti:Science:2020}, we find that contact tracing has less impact on epidemic suppression, and that the speed of contact tracing is of secondary importance to the speed of isolating index cases.
This difference can be attributed to \citet{ferretti:Science:2020}'s approach to model contact tracing and isolation as independent events (i.e. tracing an index cases' contacts says nothing about whether the index case has been isolated), which leads to an overestimation of contact tracing's impact \citep{fraser:PNAS:2004}.

In \citet{kretzschmar:TheLancetPublicHealth:2020} -- this time with contact tracing dependent on testing \& isolation -- they concluded that reducing the delay to isolation after symptom onset has the greatest impact on TTIQ effectiveness.
This conclusion was made without systematic analysis of all parameters, and we now find that changing testing \& isolation coverage has a greater effect on transmission reduction.

Our approach and results are crucially dependent on the distribution of infection times (generation time and infectivity profile) and although we have used well-supported estimates, there are inherent limitations to deriving these distributions based on transmission pairs.
These transmission pairs are representative of symptomatic cases, but the infectiousness profiles for persistently-asymptomatic infections are as-yet unknown \citep{ferretti:medRxiv:2020}.
We have assumed that asymptomatically-infected individuals have the same infection timing distributions as symptomatic individuals, but any differences between the shapes of these profiles will lead to different results in terms of transmission reduction.
The uncertainty in the inferred infection timing distributions is carried through our analysis and is captured by the confidence intervals shown in the figures and reported in the text.
Furthermore, we do account for potential differences in the overall transmissibility between asymptomatic and symptomatic individuals.
It is possible that the 20\% of infections that are asymptomatic are responsible for less than 20\% of transmission in the absence of any TTIQ interventions.
We show in the Appendix that TTIQ becomes more effective as asymptomatic transmission decreases.
Therefore, our results could be underestimating TTIQ efficacy, to a small extent.

Our model is parametrised on distributions of the timing of transmission estimated prior to the emergence of new, more transmissible variants.
If new variants simply have higher transmissibility -- without changes in the timing of transmission -- our fundamental analysis remains the same.
In this case, TTIQ may be insufficient to control the spread of highly-transmissible (higher $R$-value) new variants, as captured in Figs. \ref{fig:tertiaryCases-noTracing} and \ref{fig:tertiaryCases-CT}.
If the increased transmission of the new variants is due to a longer-lasting infectious period, then we expect TTIQ to be more efficient, as the additional transmission events are prevented by isolation and quarantine.
If the new variants are more transmissible during early (presymptomatic) infection, then we expect the relative benefit of contact tracing over testing \& isolating to increase.

In terms of modelling the TTIQ process, we have assumed that identified index cases are isolated and have their contracts traced.
If the index case fails to adhere to the isolation protocol, then we will overestimate the amount of transmission prevented by isolation.
However, uncertainty in whether contacts adhere to quarantine protocols, or whether contact tracers actually identify contacts, is captured in the parameter $g$.
Lower adherence to quarantine or missed contacts due to overwhelmed contact tracers is captured by lowering $g$.

In our approach, we assume a baseline $R$ that is defined in the absence of the modelled TTIQ intervention (i.e no testing, isolation, or contact tracing).
The empirical value for this baseline $R$ is not known, as observed values of the reproductive number in most countries include the impact of the modelled intervention.
In itself, this does not impact the result of our analysis: the impact of isolation remains higher than that of quarantine across different values of $R$ (Fig. \ref{fig:tertiaryCases-CT}).
However, in contexts where a large proportion of symptomatic individuals already isolate, the scope for increasing isolation may be limited.
Under such circumstances, mass testing, if successfully followed by isolation, may be a promising intervention.
This is supported by data on the effectiveness of mass testing interventions, for example in Slovakia \citep{pavelka:Science:2021}.

Here we have shown through systematic analysis how the TTIQ processes can be optimised to bring the effective reproductive number below one.
Crucially, contact tracing \& quarantine adds security to testing \& isolating strategies, where high coverage and short delays are necessary to control an epidemic.
By improving the testing \& isolation coverage and reducing the delay to index case isolation, we can greatly increase the efficacy of the overall TTIQ strategy.

### Contributors {-}
Conceptualization: PA SL SB; Methodology: PA SL SB; Software: PA; Validation: PA SL SB; Formal analysis: PA; Investigation: PA; Resources: -; Data curation: PA; Writingoriginal draft: PA SL SB; Writingreview & editing: PA SL SB; Visualization: PA; Supervision: SB; Project administration: PA SB; Funding acquisition: SB

### Declaration of interests {-}
We declare no competing interests.

### Acknowledgments {-}
This study was funded by the Swiss National Science Foundation (grant no. 310030B_176401).


\clearpage
# Appendix {-}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand*\thesection{S\arabic{section}}
\renewcommand*\theequation{S\arabic{equation}}
\renewcommand*\thefigure{S\arabic{figure}}
\renewcommand*\thetable{S\arabic{table}}


# Generation times, infectivity profiles, and incubation periods {-}
In our branching process model, the time at which an infector transmits SARS-CoV-2 to an infectee is determined from empirically-observed distributions.
Concretely, the time at which an identified index case developed symptoms, $t_{S_1}$, is known, but the time at which they were infected, $t_1$, is generally unknown.
Secondary contacts will be infected by the index case at some time $t_2$ ($t_2 > t_1$), and, if symptomatic, will develop symptoms at time $t_{S_2}$.
These timepoints are illustrated in Fig. \ref{fig:distributions}A.

```{r distributions, fig.height = 4.5, fig.cap = caption}
caption <- "Empirical distributions for infection time and symptom onset.
A) The timeline of infection for an infector--infectee transmission pair.
The infector (index case) is initially infected at time $t_1$, and after a period of incubation develops symptoms at time $t_{S_1}$.
The infectee (secondary contact) is infected by the infector at time $t_2$, which can be before (presymptomatic infections) or after (symptomatic infection) $t_{S_1}$.
The infectee then develops symptoms at time $t_{S_2}$.
The generation time is then defined as $t_2-t_1$ (the time between infections), while the serial interval is defined as $t_{S_2}-t_{S_1}$ (the time between symptom onsets).
B) The generation time distribution [$q(t|\\theta_q) = q(t_2-t_1|\\theta_q)$] follows a Weibull distribution, and is inferred from the serial interval distribution \\citep{ferretti:medRxiv:2020}.
C) The infectivity profile [$p(t|\\theta_p) = p(t_2-t_{S_1}|\\theta_p)$] follows a shifted Student's \\emph{t}-distribution, and is also inferred from the serial interval distribution \\citep{ferretti:medRxiv:2020}.
D) The distribution of incubation times [$h(t) = h(t_{S_1}-t_1)$] follows a meta-distribution constructed from the average of seven reported log-normal distributions, as described in \\citet{ferretti:medRxiv:2020} \\citep{bi:TheLancetInfectiousDiseases:2020,jiang:medRxiv:2020,lauer:AnnInternMed:2020,li:NEJM:2020,linton:J.Clin.Med.:2020,ma:medRxiv:2020,zhang:TheLancetInfectiousDiseases:2020}."

#' Load the distributions
load("data/savedDistributions.RData")

#' Y-limits for consistent plotting
yLim <- c(0,0.35)
labY <- 0.212

#' Plot together
distPlots <- plot_grid(
  #' Generation time
  ggplot(genDistMLE, aes(x = t, y = pdf)) +
    geom_ribbon(aes(ymin = lower, ymax = upper), colour = "transparent", fill = "grey") +
    geom_vline(xintercept = genParams$mean, linetype = "dashed", colour = "darkgrey") +
    annotate("text", label = paste0(" mean = ", format(round(genParams$mean, 1), nsmall = 1), " days"), size = theme_get()$text[["size"]]/3, x = genParams$mean, y = labY, hjust = 0, vjust = 0) +
    geom_line() +
    coord_cartesian(xlim = c(0,15), ylim = yLim, expand = F) +
    labs(x = "generation time", y = "probability density") +
    ggtitle("generation time dist.") +
    plotTheme + theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5)),
  
  #' Infectivity profile
  ggplot(infProfMLE, aes(x = t, y = pdf)) +
    geom_ribbon(aes(ymin = lower, ymax = upper), colour = "transparent", fill = "grey") +
    geom_vline(xintercept = infParams$mean, linetype = "dashed", colour = "darkgrey") +
    annotate("text", label = paste0(" mean = ", format(round(infParams$mean, 1), nsmall = 1), " days"), size = theme_get()$text[["size"]]/3, x = infParams$mean, y = labY, hjust = 0, vjust = 0) +
    geom_line() +
    coord_cartesian(xlim = c(-10,15), ylim = yLim, expand = F) +
    labs(x = "days post symptoms", y = "probability density") +
    ggtitle("infectivity profile") +
    plotTheme + theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5)),
  
  #' Incubation period
  ggplot(incDist, aes(x = t, y = pdf)) +
    geom_vline(xintercept = incParams$mean, linetype = "dashed", colour = "darkgrey") +
    annotate("text", label = paste0(" mean = ", format(round(incParams$mean, 1), nsmall = 1), " days"), size = theme_get()$text[["size"]]/3, x = incParams$mean, y = labY, hjust = 0, vjust = 0) +
    geom_line() +
    coord_cartesian(xlim = c(0,20), ylim = yLim, expand = F) +
    labs(x = "incubation period (days)", y = "probability density") +
    ggtitle("incubation period") +
    plotTheme + theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5)),
  
  nrow = 1, align = "hv", axis = "tb",
  #labels = "AUTO"
  labels = c("B","C","D")
)

plot_grid(
  ggdraw() + draw_image(magick::image_read_pdf("TTIQ-timeIntervals.pdf", density = 600), scale = 0.9),
  distPlots, ncol = 1, labels = c("A",""), rel_heights = c(1,1.4)
)
```

```{r, distribution-properties, include = F}
#' Incubation period properties:
t <- seq(0,100,stepSize)
list(
  # mean = sum(t * getIncubationPeriod(times = t, params = incParams) * stepSize),
  mean = incParams$mean[[1]],
  sd = sqrt(sum(t^2 * getIncubationPeriod(times = t, params = incParams) * stepSize) - incParams$mean[[1]]^2),
  median = approx(x = getIncubationPeriod(times = t, params = incParams, CDF = T), y = t, xout = 0.5)$y
)

#' Generation time properties
t <- seq(0,100,stepSize)
list(
  mean = genParams$mean,
  sd = with(list(a = genParams$shape, b = genParams$scale),
            sqrt(b^2 * (gamma(1 + 2/a) - (gamma(1 + 1/a))^2))),
  median = approx(x = getGenDist(times = t, params = genParams, CDF = T), y = t, xout = 0.5)$y
)
#' Infectivity profile properties
t <- seq(-50,50,stepSize)
list(
  mean = infParams$mean,
  sd = sqrt(sum(t^2 * getInfectivityProfile(times = t, params = infParams) * stepSize) - infParams$mean^2),
  median = approx(x = getInfectivityProfile(times = t, params = infParams, CDF = T), y = t, xout = 0.5)$y
)
```

The relationships between the times $t_1$, $t_{S_1}$, $t_2$, $t_{S_2}$ are determined by:
the generation time distribution, $q(t_2-t_1|\theta_q)$, describing the time interval between the infection of an index case and secondary contact (Fig. \ref{fig:distributions}B);
the infectivity profile, $p(t_2-t_{S_1}|\theta_p)$, describing the time interval between the onset of symptoms in the index case and infection of the secondary contact (Fig. \ref{fig:distributions}C);
and the incubation period distribution, $h(t_{S_1}-t_1)$, describing the time between the infection of an individual and the onset of their symptoms (Fig. \ref{fig:distributions}D).
For these distributions, we use empirical estimates from \citet{ferretti:medRxiv:2020}.
The parameters that define the generation time distribution, infectivity profile, and the incubation period distribution are shown in Table \ref{tab:distribution-parameters}.

\begin{table}[ht]
\begin{tabular}{p{1.8cm} p{1.8cm} p{2.5cm} l}
Distribution & Shape & Properties & Parameters \\ \hline
\multirow{7}{=}{Incubation period $h(t)$} & \multirow{7}{=}{Meta-log-normal} &
\multirow{7}{=}{mean = 5.723, sd = 3.450, median = 4.936} &
meanlog = 1.570, sdlog = 0.650 (Bi) \\ \cline{4-4}
& & & meanlog = 1.621, sdlog = 0.418 (Lauer)  \\ \cline{4-4}
& & & meanlog = 1.434, sdlog = 0.661 (Li) \\ \cline{4-4}
& & & meanlog = 1.611, sdlog = 0.472 (Linton) \\ \cline{4-4}
& & & meanlog = 1.857, sdlog = 0.547 (Ma) \\ \cline{4-4}
& & & meanlog = 1.540, sdlog = 0.470 (Zhang)  \\ \cline{4-4}
& & & meanlog = 1.530, sdlog = 0.464 (Jiang)  \\ \hline
%
Generation time $q(t|\theta_q)$ & Weibull &
mean = 5.494, sd = 1.845, median = 5.479 &
shape = 3.277, scale = 6.127 \\ \hline
%
Infectivity profile $p(t|\theta_p)$ & Shifted Student's \emph{t} &
mean = -0.042, sd = 2.876, median = -0.078 &
shift = -0.078, scale = 1.86, df = 3.35 \\ \hline
\end{tabular}

\caption{Parameters of the distributions used in this work to describe the timing of infection events.
The meta-log-normal incubation period distribution is the average of seven reported log-normal incubation period distributions as described by \citet{ferretti:medRxiv:2020} \citep{bi:TheLancetInfectiousDiseases:2020,jiang:medRxiv:2020,lauer:AnnInternMed:2020,li:NEJM:2020,linton:J.Clin.Med.:2020,ma:medRxiv:2020,zhang:TheLancetInfectiousDiseases:2020}.
The properties listed for the incubation period distribution are the mean, standard deviation (sd), and median of this meta-log-normal distribution.
The shifted Student's \emph{t} distribution for the infectivity profile is defined in \texttt{R} by \texttt{dt((x-shift)/scale, df)/scale} \citep{ferretti:medRxiv:2020}.}
\label{tab:distribution-parameters}
\end{table}

# Asymptomatic vs symptomatic infections {-}
We assume that a fraction $a$ of all infections are persistently asymptomatic, with the remainder being classed as symptomatic (which includes individuals that are pre-symptomatic and post-symptom onset).
Whether a new infectee is persistently-asymptomatic or not is assumed to be independent of whether the infector was persistently-asymptomatic or not.
A meta-analysis has estimated a fraction $a \approx 20\%$ of infections are asymptomatic \citep{buitrago-garcia:PLOSMedicine:2020}.

We now introduce parameters that describe the infectiousness of asymptomatic or symptomatic individuals.
An asymptomatic individual would infect an average of $R_a$ secondary contacts during their whole uninterrupted infectious period (i.e. in the absence of any TTIQ intervention, but in the presence of non-modelled interventions such as social distancing and hygiene protocols).
A symptomatic individual will infect an average of $R_s$ secondary contacts during their whole uninterrupted infectious period (i.e. no TTIQ).
In general we have $R_a \ne R_s$, and we expect that $R_a \le R_s$ based on empirical observations \citep{buitrago-garcia:PLOSMedicine:2020}.

We can define the average reproductive number in the absence of TTIQ as
\begin{equation}
R = a R_a + (1-a) R_s,
\label{eq:R}
\end{equation}
i.e. the average number of secondary infections per infected throughout the infectious period.
The fraction of transmission that is attributable to asymptomatic individuals in the absence of TTIQ is then defined as
\begin{equation}
\alpha = \frac{a R_a}{a R_a + (1-a) R_s} = \frac{a R_a}{R}.
\label{eq:alpha}
\end{equation}
Note that for $R_a = R_s$ (equal transmission from asymptomatics and symptomatics), we have $\alpha = a$.
For $R_a < R_s$, we have $\alpha < a$.
As $\alpha$ must be a positive number, we can bound the fraction of transmission from asymptomatic individuals in the absence of TTIQ by the limits $0 \le \alpha \le a$.

Although we modify the relative infectiousness of asymptomatic versus symptomatic individuals, we assume that the distribution of infection times is equal for both classes.


# Quantifying secondary infections under TTIQ {-}

```{r flowchart-secondary, fig.align = "center", out.width = "0.8\\textwidth", fig.cap = caption}
caption <- "Flowchart for computing the number of secondary infections under testing \\& isolation."

ggdraw() + draw_image(image_read_pdf("TTIQ-flowchart-secondary-1.pdf", density = 600), scale = 1.0)
```

Consider an infected individual who develops symptoms of COVID-19 at time $t_{S_1}$.
The time at which this individual was infected, $t_1 < t_{S_1}$, is generally unknown.
Without any TTIQ intervention this symptomatic individual would contact and infect $R_s$ individuals during the course of the infection.
The number of secondary infections up to a time $T_1$ after developing symptoms would then be
\begin{equation}
R_s \int_{-\infty}^{T_1} {\rm d}t_2 \, p(t_2 - t_{S_1}|\theta_p) = R_s P(T_1 - t_{S_1}|\theta_p),
\end{equation}
where $p(t|\theta_p)$ is the infectivity profile and  $P(t|\theta_p) = \int_{-\infty}^t {\rm d}t' \, p(t'|\theta_p)$ is the cumulative infectivity profile.

Infected individuals who develop symptoms and/or test positive for SARS-CoV-2 should be isolated from the population.
In our model this occurs in a fraction $f$ of symptomatic individuals who are then isolated at a time $T_1=t_{S_1}+\Delta_1$, where $\Delta_1>0$ is the delay between symptom onset and isolation.
The parameter $\Delta_1$ can be interpreted as the delay of taking a test after symptom onset, waiting for the result, and entering isolation, or alternatively as the delay between symptom onset and self-isolation.
The remaining fraction $1-f$ of symptomatic individuals, along with the asymptomatic individuals, are not isolated ($T_1 \to \infty$).
We can compute the expected number of secondary infections, $n_2$, as a function of the asymptomatic fraction $a$, isolation probability $f$, and delay $\Delta_1$, as shown in Fig. \ref{fig:flowchart-secondary}.
We then have
\begin{equation}
\begin{aligned}
n_2(f,\Delta_1 | \theta_p)
%&= a R_a \int_{-\infty}^\infty {\rm d}t_2 \, p(t_2 - t_{S_1}|\theta_p) + \\
%&\quad (1-a)R_s \left[f \int_{-\infty}^{t_{S_1}+\Delta_1} {\rm d}t_2 \, p(t_2 - t_{S_1}|\theta_p) + (1-f) \int_{-\infty}^\infty {\rm d}t_2 \, p(t_2 - t_{S_1}|\theta_p)\right] \\
&= a R_a + (1-a)\left[f P(\Delta_1|\theta_p)R_s + (1-f)R_s\right],
\end{aligned}
\label{eq:secondaryCases}
\end{equation}
where the first term represents the secondary infections caused by asymptomatic individuals (who cannot be isolated), the first term in the bracket represents the secondary infections caused by symptomatic index cases prior to their isolation, and the final term is the secondary infections caused by symptomatic individuals who are not isolated.
Now replacing $a R_a = \alpha R$ and $(1-a)R_s = (1-\alpha)R$ [from Eq. \eqref{eq:alpha}], we can rearrange Eq. \eqref{eq:secondaryCases} to give
\begin{equation}
n_2(f,\Delta_1 | \theta_p) = R \left[(1-\alpha)f P(\Delta_1|\theta_p) + (1-(1-\alpha)f)\right].
\label{eq:secondaryCasesAvg}
\end{equation}


# Quantifying tertiary infections under TTIQ {-}
Each infected secondary contact has the potential to cause further infections, which will be the tertiary contacts of the initial infected.
The number of infections caused by a secondary contact who is infected at $t_2$ and isolated at time $T_2$, will be
\begin{equation}
R_\bullet \int_{t_2}^{T_2} {\rm d}t_3 \, q(t_3 - t_2|\theta_q) = R_\bullet Q(T_2 - t_2|\theta_q),
\end{equation}
where $R_\bullet \in \{R_a,R_s\}$ is the number of infections per secondary contact during the uninterrupted infectious period, $t_3$ is the infection time of the tertiary contacts, $q(t|\theta_q)$ is the generation time distribution, and $Q(t|\theta_q) = \int_0^t {\rm d}t' \, q(t'|\theta_q)$ is the cumulative generation time distribution.
Note that we use the generation time distribution here, as our reference point is the time of infection ($t_2$), whereas in Eq. \eqref{eq:secondaryCasesAvg} the reference point was the time of symptom onset ($t_{S_1}$).

Under TTIQ interventions, the symptomatic index and secondary cases can be isolated following a positive test result after symptom onset.
If an index case is confirmed positive, then contact tracing can be used to identify and quarantine individuals who have recently been exposed to the confirmed case.
Quarantining these individuals prevents the onward infection of tertiary contacts (Fig. \ref{fig:schematic}B).
Importantly, whether an individual is quarantined is independent of symptom status.
We introduce three further parameters to quantify contact tracing and quarantine:
i) $\tau > 0$, the duration of lookback prior to symptom onset of the index case in which contacts are traced;
ii) $0 \le g \le 1$, the probability to identify and quarantine a secondary contact that was infected within the contact tracing window;
and iii) $\Delta_2 > 0$, the delay between isolating the index case and quarantining the identified secondary contacts.

```{r flowchart-tertiary, fig.cap = caption}
caption <- "Flowchart for computing the number of tertiary infections under TTIQ."

ggdraw() + draw_image(image_read_pdf("TTIQ-flowchart-tertiary-1.pdf", density = 600), scale = 1.0)
```

There are many permutations of events that contribute to the number of tertiary infections under TTIQ, as shown in Fig. \ref{fig:flowchart-tertiary}.
The index case may not be detected due to being asymptomatic ($a$), or being symptomatic but not tested ($(1-a)(1-f)$), and hence contact tracing is not possible.
If the index case is symptomatic and detected ($(1-a)f$), then a fraction $g$ of the secondary contacts that were infected within the contact tracing window ($t_{S_1}-\tau \le t_2 \le t_{S_1}+\Delta_1$) are quarantined at time $t_{S_1}+\Delta_1+\Delta_2$ (as shown in Fig. \ref{fig:schematic}B of the main manuscript).
The remaining fraction $1-g$ of secondary contacts, as well as the secondary contacts that were infected outside of the contact tracing window ($t_2 < t_{S_1}-\tau$), are not quarantined.
However, the non-traced contacts may themselves become symptomatic and, after testing, become index cases that are isolated at time $t_{S_2}+\Delta_1$, where $t_{S_2}$ is the symptom onset time of the secondary case.
By considering these different scenarios, we arrive at an expression for the number of tertiary infections per index case under TTIQ,
\begin{equation}
\begin{aligned}
&n_3(f,\Delta_1,\tau,g,\Delta_2 | t_{S_1},t_{S_2},\theta_p,\theta_q) = \\
&\quad
R_s(1-a)fg \int_{t_{S_1}-\tau}^{t_{S_1}+\Delta_1} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) R Q(t_{S_1}+\Delta_1+\Delta_2-t_2|\theta_q) + \\
&\quad
R_s(1-a)f(1-g) \int_{t_{S_1}-\tau}^{t_{S_1}+\Delta_1} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) \psi(f,t_{S_2}+\Delta_1-t_2|\theta_q) + \\ %\left[aR_a + (1-a) R_s\bigl(fQ(t_{S_2}+\Delta_1-t_2|\theta_q) + (1-f)\bigr)\right] + \\
&\quad
R_s(1-a)f \int_{-\infty}^{t_{S_1}-\tau} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) \psi(f,t_{S_2}+\Delta_1-t_2|\theta_q) + \\ %\left[aR_a + (1-a) R_s\bigl(fQ(t_{S_2}+\Delta_1-t_2|\theta_q) + (1-f)\bigr)\right] + \\
&\quad
[aR_a+(1-a)R_s(1-f)] \int_{-\infty}^{\infty} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) \psi(f,t_{S_2}+\Delta_1-t_2|\theta_q), %\left[aR_a + (1-a) R_s\bigl(fQ(t_{S_2}+\Delta_1-t_2|\theta_q) + (1-f)\bigr)\right].
\end{aligned}
\label{eq:tertiaryCases}
\end{equation}
where the shorthand
\begin{equation}
\psi(f,t_{S_2}+\Delta_1-t_2|\theta_q) = \left[aR_a + (1-a) R_s\bigl(fQ(t_{S_2}+\Delta_1-t_2|\theta_q) + (1-f)\bigr)\right]
\end{equation}
is the expected number of onward infections caused by each non-quarantined secondary contact.
Each row in Eq. \eqref{eq:tertiaryCases} corresponds to:
i) tertiary infections caused by secondary contacts prior to their quarantine;
ii) tertiary infections caused by secondary contacts who could have been quarantined but were not;
iii) tertiary infections caused by secondary contacts who were infected before the quarantine window, and hence are not quarantined;
iv) tertiary infections caused by secondary contacts who were infected by non-identified index cases.

We now have to average Eq. \eqref{eq:tertiaryCases} over $t_{S_2}$ to obtain the expected number of tertiary infections per index case under TTIQ.
We first note that $t_{S_2} = t_2 + \gamma$ for incubation period $\gamma \ge 0$.
Hence we can write
\begin{equation}
\Bigl\langle Q(t_{S_2}+\Delta_1-t_2|\theta_q) \Bigr\rangle_{t_{S_2}} =
\int_0^\infty {\rm d}\gamma \, h(\gamma) Q(\gamma+\Delta_1|\theta_q),
\end{equation}
where $h(\gamma)$ is the incubation period distribution.
We define the quantity
\begin{equation}
J(\Delta_1|\theta_q) = \Bigl\langle Q(t_{S_2}+\Delta_1-t_2|\theta_q) \Bigr\rangle_{t_{S_2}}.
\end{equation}
Note that we have assumed the independence between symptom onset and infectivity, which may lead to an overestimation of the fraction of tertiary infections prevented.

Keeping $t_{S_1}$ fixed as the reference time point, averaging Eq. \eqref{eq:tertiaryCases} over $t_{S_2}$ gives the expected number of tertiary infections per infected under TTIQ:
\begin{equation}
\begin{aligned}
&n_3(f,\Delta_1,\tau,g,\Delta_2 | \theta_p,\theta_q) = \\
&\quad
R_s(1-a)fg R \int_{-\tau}^{\Delta_1} {\rm d}t' \, p(t'|\theta_p) Q(\Delta_1+\Delta_2-t'|\theta_q) + \\
&\quad
R_s(1-a)f(1-g) \bigl[P(\Delta_1|\theta_p)-P(-\tau|\theta_p)\bigr] \left[aR_a + (1-a)R_s\bigl(f J(\Delta_1|\theta_q) + (1-f)\bigr)\right] + \\
&\quad
R_s(1-a)f P(-\tau|\theta_p) \left[aR_a + (1-a)R_s\bigl(f J(\Delta_1|\theta_q) + (1-f)\bigr)\right] + \\
&\quad
[aR_a + (1-a)R_s(1-f)] \left[aR_a + (1-a)R_s\bigl(f J(\Delta_1|\theta_q) + (1-f)\bigr)\right],
\end{aligned}
\label{eq:tertiaryCasesAvg}
\end{equation}
where we have substituted $t' = t_2-t_{S_1}$ such that
\begin{equation}
\int_{t_{S_1}-\tau}^{t_{S_1}+\Delta_1} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) Q(t_{S_1}+\Delta_1+\Delta_2-t_2|\theta_q) =
\int_{-\tau}^{\Delta_1} {\rm d}t' \, p(t'|\theta_p) Q(\Delta_1+\Delta_2-t'|\theta_q).
\end{equation}
Now replacing $a R_a = \alpha R$ and $(1-a)R_s = (1-\alpha)R$ [from Eq. \eqref{eq:alpha}], Eq. \eqref{eq:tertiaryCasesAvg} can be further simplified to
\begin{equation}
\begin{aligned}
&n_3(f,\Delta_1,\tau,g,\Delta_2 | \theta_p,\theta_q) = \\
&\quad
R^2(1-\alpha)fg \int_{-\tau}^{\Delta_1} {\rm d}t' \, p(t'|\theta_p) Q(\Delta_1+\Delta_2-t'|\theta_q) + \\
&\quad
R^2 \bigl[ (1-\alpha)f(1-g) P(\Delta_1|\theta_p) + (1-\alpha)f g P(-\tau|\theta_p) + (1-(1-\alpha)f) \bigr] \times \\
&\qquad\qquad\bigl[(1-\alpha)f J(\Delta_1|\theta_q) + (1-(1-\alpha)f)\bigr].
\end{aligned}
\label{eq:tertiaryCasesAvgSimp}
\end{equation}

Finally, in the absence of contact tracing ($g=0$) Eq. \eqref{eq:tertiaryCasesAvgSimp} can be simplified, such that the number of tertiary infections per infected under testing \& isolation only is given by
\begin{equation}
\begin{aligned}
n_3(f,\Delta_1 | \theta_p,\theta_q) &=
R^2 \bigl[(1-\alpha)f P(\Delta_1|\theta_p) + (1-(1-\alpha)f)\bigr] \times \\
&\qquad \bigl[(1-\alpha)f J(\Delta_1|\theta_q) + (1-(1-\alpha)f)\bigr].
\end{aligned}
\label{eq:tertiaryCases-noTracingAvg}
\end{equation}

From Eqs. \eqref{eq:tertiaryCasesAvgSimp} and \eqref{eq:tertiaryCases-noTracingAvg}, we observe that the parameter $f$ is always coupled to $1-\alpha$.
We could therefore define a new parameter $\phi = (1-\alpha)f$ as the fraction of all infecteds that are isolated (as opposed to $f$ which is the fraction of symptomatic infecteds isolated) to simplify our expressions.
However, we choose to keep $\alpha$ and $f$ explicitly in the calculations for clarity.

As a final point, we could repeat the derivation of Eq. \eqref{eq:tertiaryCasesAvgSimp}, but this time only consider the number of tertiary infections that were caused by an asymptomatic secondary contact.
I.e. we can calculate how much transmission is attributable to asymptomatics versus symptomatics in the presence of TTIQ.
This leads to the expression
\begin{equation}
\begin{aligned}
&n_3^{\rm (asymp)}(f,\Delta_1,\tau,g,\Delta_2 | \theta_p,\theta_q) = \\
&\quad
\alpha R^2(1-\alpha)fg \int_{-\tau}^{\Delta_1} {\rm d}t' \, p(t'|\theta_p) Q(\Delta_1+\Delta_2-t'|\theta_q) + \\
&\quad
\alpha R^2 \bigl[ (1-\alpha)f(1-g) P(\Delta_1|\theta_p) + (1-\alpha)f g P(-\tau|\theta_p) + (1-(1-\alpha)f) \bigr].
\end{aligned}
\label{eq:tertiaryCasesAvgSimp-asymp}
\end{equation}


# Reproductive number under TTIQ {-}
For our branching process model, we define the reproductive number as
\begin{equation}
R_{\rm TTIQ} = \dfrac{n_3(f,\Delta_1,\tau,g,\Delta_2 | \theta_p,\theta_q)}{n_2(f,\Delta_1 | \theta_p)},
\end{equation}
where $n_2$ [Eq. \eqref{eq:secondaryCasesAvg}] and $n_3$ [Eq. \eqref{eq:tertiaryCasesAvgSimp}] are the expected number of tertiary and secondary infections per infected, respectively.
In other words, we define the reproductive number as the average number of infecteds in the third generation per infected in the second generation.
It is necessary to work with the third generation (as opposed to just the first and second generations) as this is where the impact of contact tracing and quarantine is first observed.

Likewise, in the presence of testing \& isolation only (i.e. no contact tracing \& quarantine), the reproductive number is given by
\begin{equation}
R_{\rm TI} = \dfrac{n_3(f,\Delta_1 | \theta_p,\theta_q)}{n_2(f,\Delta_1 | \theta_p)},
\end{equation}
where $n_3$ is now given by Eq. \eqref{eq:tertiaryCases-noTracingAvg}.

# Confidence intervals {-}
The primary sources of uncertainty in the outcomes of this model come from the generation time distribution and infectivity profile, which are inferred from empirical serial interval distributions \citep{ferretti:medRxiv:2020}.
Following \citet{ferretti:medRxiv:2020}, we use a likelihood ratio test to extract sample parameter sets for each distribution that lie within the 95\% confidence interval.

Concretely, we first identify the maximum likelihood parameter sets $\hat{\theta}_p$ and $\hat{\theta}_q$ for the infectivity profile and generation time distribution, respectively.
We then randomly sample the parameter space of each distribution, and keep 1,000 parameter sets whose likelihood satisfies $\ln \mathcal{L}(\theta) > \ln \mathcal{L}(\hat{\theta}) - \lambda_n/2$, where $\lambda_n$ is the 95\% quantile of a $\chi^2$ distribution with $n$ degrees of freedom.
The infectivity profile is described a shifted Student's \emph{t}-distribution, which has $n=3$ parameters, while the generation time is described by a Weibull distribution with $n=2$ parameters.

We then use these sampled parameter sets to generate $R_{\rm TTIQ}$, and the extrema across all of these parameter sets determines the 95\% confidence interval for the reproductive number under TTIQ.
We need to use and combine estimates of both $\theta_p$ and $\theta_q$.
We assume parameter independence, and keep all $(\theta_p,\theta_q)$ combinations whose joint likelihood satisfies $\ln \mathcal{L}(\theta_p) + \ln \mathcal{L}(\theta_q) > \ln \mathcal{L}(\hat{\theta}_p) + \ln \mathcal{L}(\hat{\theta}_q) - \lambda_5/2$.

# Impact of asymptomatics {-}
The relative contribution to transmission of asymptomatics versus symptomatics is captured by the parameter $\alpha$, which we define as the fraction of transmission attributable to asymptomatic individuals in the absence of TTIQ [Eq. \eqref{eq:alpha}].
This fraction is difficult to estimate empirically.
However, it has been observed that approximately $20\%$ of infections are asymptomatic, and that asymptomatically-infected individuals have a lower risk of onward transmission \citep{buitrago-garcia:PLOSMedicine:2020}.
Hence we expect $\alpha$ to lie somewhere in the region $0\% \le \alpha \le 20%$, but with substantial uncertainty in this estimate.

By varying $\alpha$ in our model, we can observe how TTIQ effectiveness depends on the amount of asymptomatic transmission.
In Fig. \ref{fig:max-asymptomatic}, we show that idealised TTIQ (and also just testing \& isolation alone) is maximally effective when $\alpha = 0$ (i.e. no transmission from asymptomatic individuals).
The reason for this is that identifying index cases underlies all TTIQ processes, and identification is only possible if individuals are symptomatic.

```{r max-asymptomatic, fig.width = 10, fig.cap = caption}
caption <- "The maximum baseline $R$-value that can be suppressed by TTIQ interventions, as a function of the fraction of transmission that is attributable to asymptomatics $\\alpha$.
As $\\alpha \\to 100\\%$, no infecteds develop symptoms and hence no cases are isolated and no contact tracing occurs.
In this case, TTIQ has no effect and epidemics are only suppressed if the baseline $R$-value is already below one.
To achieve the maximum level of suppression, each symptomatic individual ($f=100\\%$) would have to isolate immediately at symptom onset ($\\Delta_1=0$ days), which represents the upper limit of testing \\& isolation performance.
With additional contact tracing, we assume that $g=100\\%$ of contacts of the symptomatic cases who were infected up to $\\tau=5$ days before symptom onset are quarantined immediately ($\\Delta_2=0$ days).
Shaded regions are 95\\% confidence intervals, representing the uncertainty in the inferred generation time distribution and infectivity profile."

#' Compute R_TTIQ for testing & isolation (g=0) and with additional TTIQ (g=1)
paramList <- list(
  alpha = seq(0,1,0.05),
  f = 1,
  g = c(0,1),
  Delta1 = 0,
  Delta2 = 0,
  tau = 5,
  R = 1
)
load("data/savedDistributions.RData")
rMLE <- getTertiaryCases(paramList = paramList, times = times, stepSize = stepSize,
                         infProf.df = infProfMLE, genDist.df = genDistMLE, integral.df = integralMLE)
#' Prepare and plot
df <- rMLE
df$alpha <- as.numeric(levels(df$alpha))[df$alpha]
plot <- ggplot(df, aes(x = alpha, y = 1/ter.per.sec, colour = g)) +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  #geom_vline(xintercept = 0.2, colour = "darkgrey", size = lineSize) +
  geom_line(size = lineSize) +
  scale_colour_manual(values = c(`0`="red",`1`="black"),
                      labels = c(`0` = "testing &\nisolation only", `1` = "TTIQ"),
                      name = NULL, aesthetics = c("colour","fill")) +
  scale_x_continuous(breaks = seq(0,1,0.2),
                     labels = scales::percent) +
  scale_y_log10(breaks = c(seq(1,9),seq(10,90,10)),
                labels = c(seq(1,6),"",8,"",seq(10,90,10))) +
  coord_cartesian(xlim = c(0,1), ylim = c(0.9,50), expand = F) +
  labs(x = expression("fraction of transmission attributable to asymptomatics without TTIQ" ~ (alpha)), y = "maximum baseline R-value that\ncan be suppressed by TTIQ") +
  plotTheme + theme(legend.position = c(0.8, 0.8), plot.margin = unit(20*c(1,1,1,1), "points"))

#' Do we add CI to the plot?
addCI <- T
if (addCI) {
  out.filename <- "data/asymptomatic-max-CI.RData"
  if (file.exists(out.filename)) {
    #' Load the data, format, and add layer to plot
    load(out.filename)
    rCI <- out.df
    rCI$alpha <- as.numeric(levels(rCI$alpha))[rCI$alpha]
    df <- merge(df,rCI)
    plot$layers <- c(
      geom_ribbon(data = df, aes(ymin = 1/upper, ymax = 1/lower, fill = g), alpha = 0.4, colour = "transparent"),
      plot$layers)
  } else {
    print(paste("File", out.filename, "is missing."))
    print("Pass the following info file to TTIQ-CIscript.R to generate confidence intervals, and then try again.")
    #' Create file "info-for-script_xx.RData" here
    #' First we create the function which accepts id and idGD and returns a
    #' dataframe for the number of cases for the given parameter set
    func <- function(id, idGD) {
      getTertiaryCases(paramList = paramList, times = times,
                       stepSize = stepSize, infProf.df = infProfLLH[[id]],
                       genDist.df = genDistLLH[[idGD]], integral.df = integralLLH[[idGD]])
    }
    #' Which output do we want to construct the CI for?
    output <- "ter.per.sec"
    #' Now add the function(s), parameter list, and the output filename
    info.filename <- "info-for-script_asymptomatic_max.RData"
    save(func, getTertiaryCases, output, paramList, out.filename, file = info.filename)
    print(paste("Info file is", info.filename))
    print(paste("The output file", out.filename, "can be generated by executing:"))
    #' R --vanilla --slave < TTIQ-CIscript.R --args info-for-script_xx.RData run
    print(paste("R --vanilla --slave < TTIQ-CIscript.R --args", info.filename, "run"))
    addCI <- FALSE
  }
}

plot

#' Read values
# dat <- df[df$g == "0" & df$alpha == 0.2, ]
# paste("R_TTIQ = ", paste0(dat$ter.per.sec, ", [", dat$lower, ", ", dat$upper, "]"))
# paste("max preventable = ", paste0(1/dat$ter.per.sec, ", [", 1/dat$upper, ", ", 1/dat$lower, "]"))
# 
# dat <- df[df$g == "1" & df$alpha == 0.2, ]
# paste("R_TTIQ = ", paste0(dat$ter.per.sec, ", [", dat$lower, ", ", dat$upper, "]"))
# paste("max preventable = ", paste0(1/dat$ter.per.sec, ", [", 1/dat$upper, ", ", 1/dat$lower, "]"))
```

Even for imperfect TTIQ interventions with inaccuracies and delays, the fraction of transmission attributable to asymptomatics plays an important role in the effectiveness TTIQ.
Under testing \& isolation alone, the effective reproductive number $R_{\rm TI}$ increases linearly with $\alpha$, while with additional contact tracing \& quarantine the increase is super-linear (Fig. \ref{fig:asymptomatic-impact}A).

Finally, we note that TTIQ leads to an increase in the fraction of transmissions that are attributable to asymptomatics, when compared to this fraction in the absence of TTIQ (Fig. \ref{fig:asymptomatic-impact}B).
This is because the transmission due to symptomatics is lowered by testing \& isolation, but transmission due to asymptomatics is untouched.
Furthermore, additional contact tracing \& quarantine does not affect this fraction as it prevents transmission equally from asymptomatic and symptomatic individuals, hence the lines in Fig. \ref{fig:asymptomatic-impact}B are overlapping.

```{r asymptomatic-impact, fig.width = 8, fig.height = 6, fig.cap = caption}
caption <- "A) The impact of the level of asymptomatic transmission on the reproductive number $R_{\\rm TTIQ}$.
Here we consider imperfect TTIQ interventions, with $f=70\\%$, $\\Delta_1=2$ days, $\\Delta_2=1$ day, $\\tau=2$ days, and a baseline reproductive number of $R=1.5$.
These parameters are equivalent to those used in Fig. \\ref{fig:tertiaryCases-single}C, along with $g=50\\%$.
Here we also consider $g=0\\%$ (testing \\& isolation only) and $g=100\\%$ (all traced contacts are quarantined).
Shaded regions are 95\\% confidence intervals, representing the uncertainty in the inferred generation time distribution and infectivity profile.
B) The fraction of $R_{\\rm TTIQ}$ from panel A that is attributable to asymptomatic infection, as described by Eq. \\eqref{eq:tertiaryCasesAvgSimp-asymp}.
The diagonal grey line represents the fraction of transmission attributable to asymptomatics without TTIQ interventions.
Hence, the TTIQ intervention increases the fraction of transmission that is attributable to asymptomatics.
The lines for testing \\& isolation only, $g=50\\%$, and $g=100\\%$ are overlapping.
"

#' Compute R_TTIQ for testing & isolation (g=0) and with additional TTIQ (g=0.5,1)
paramList <- list(
  alpha = seq(0,1,0.05),
  f = 0.7,
  g = c(0,0.5,1),
  Delta1 = 2,
  Delta2 = 1,
  tau = 2,
  R = 1.5
)
load("data/savedDistributions.RData")
rMLE <- getTertiaryCases(paramList = paramList, times = times, stepSize = stepSize,
                         infProf.df = infProfMLE, genDist.df = genDistMLE, integral.df = integralMLE)
#' Prepare and plot
df <- rMLE
df$alpha <- as.numeric(levels(df$alpha))[df$alpha]
rPlot <- ggplot(df, aes(x = alpha, y = ter.per.sec, colour = g)) +
  geom_hline(yintercept = 1.5, colour = "darkgrey", size = lineSize) +
  #geom_vline(xintercept = 0.2, colour = "darkgrey", size = lineSize) +
  #geom_col(position = position_identity())
  geom_line(size = lineSize) +
  scale_colour_manual(values = c(`0`="red",`0.5`="purple",`1`="black"),
                      labels = c(`0` = "testing &\nisolation only", `0.5` = "TTIQ (g=50%)", `1` = "TTIQ (g=100%)"),
                      name = NULL, aesthetics = c("colour","fill")) +
  scale_x_continuous(breaks = seq(0,1,0.2),
                     labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1.55), expand = F) +
  labs(x = expression("fraction of transmission attributable to asymptomatics without TTIQ" ~ (alpha)), y = expression(atop(R[TTIQ], "(or " * R[TI] * ")"))) +
  plotTheme + theme(legend.position = c(0.8,0.3), plot.margin = unit(20*c(1,1,1,1), "points"))

#' Do we add CI to the plot?
addCI <- T
if (addCI) {
  out.filename <- "data/asymptomatic-impact-CI.RData"
  if (file.exists(out.filename)) {
    #' Load the data, format, and add layer to plot
    load(out.filename)
    rCI <- out.df
    rCI$alpha <- as.numeric(levels(rCI$alpha))[rCI$alpha]
    df <- merge(df,rCI)
    rPlot$layers <- c(
      geom_ribbon(data = df, aes(ymin = lower, ymax = upper, fill = g), alpha = 0.4, colour = "transparent"),
      rPlot$layers)
  } else {
    print(paste("File", out.filename, "is missing."))
    print("Pass the following info file to TTIQ-CIscript.R to generate confidence intervals, and then try again.")
    #' Create file "info-for-script_xx.RData" here
    #' First we create the function which accepts id and idGD and returns a
    #' dataframe for the number of cases for the given parameter set
    func <- function(id, idGD) {
      getTertiaryCases(paramList = paramList, times = times,
                       stepSize = stepSize, infProf.df = infProfLLH[[id]],
                       genDist.df = genDistLLH[[idGD]], integral.df = integralLLH[[idGD]])
    }
    #' Which output do we want to construct the CI for?
    output <- "ter.per.sec"
    #' Now add the function(s), parameter list, and the output filename
    info.filename <- "info-for-script_asymptomatic_impact.RData"
    save(func, getTertiaryCases, output, paramList, out.filename, file = info.filename)
    print(paste("Info file is", info.filename))
    print(paste("The output file", out.filename, "can be generated by executing:"))
    #' R --vanilla --slave < TTIQ-CIscript.R --args info-for-script_xx.RData run
    print(paste("R --vanilla --slave < TTIQ-CIscript.R --args", info.filename, "run"))
    addCI <- FALSE
  }
}

asympPlot <- ggplot(df, aes(x = alpha, y = frac_asymptomatic, colour = g)) +
  geom_abline(intercept = 0, slope = 1, colour = "darkgrey", size = lineSize) +
  geom_line(size = lineSize) +
  scale_colour_manual(values = c(`0`="red",`0.5`="purple",`1`="black"),
                      labels = c(`0` = "testing &\nisolation only", `0.5` = "TTIQ (g=50%)", `1` = "TTIQ (g=100%)"),
                      name = NULL, aesthetics = c("colour","fill")) +
  scale_x_continuous(breaks = seq(0,1,0.2),
                     labels = scales::percent) +
  scale_y_continuous(breaks = seq(0,1,0.2), labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1), expand = F) +
  labs(x = expression("fraction of transmission attributable to asymptomatics without TTIQ" ~ (alpha)), y = "fraction of transmission\nattributable to asymptomatics\n with TTIQ") +
  plotTheme + theme(legend.position = c(0.8,0.3), plot.margin = unit(20*c(1,1,1,1), "points"))

plot_grid(rPlot, asympPlot, ncol = 1, axis = "lr", align = "hv", labels = "AUTO")
```


# Quarantine vs isolation {-}
```{r quarantine-vs-isolation, fig.height = 5, fig.cap = caption}
caption <- "The fraction of prevented transmission that can be attributed to quarantine, rather than isolation.
Let $R_{\\rm TTIQ}(g)$ be the reproductive number in the presence of TTIQ interventions in which a fraction $g$ of contacts of identified index cases are quarantined.
In the absence of TTIQ measures, we expect a reproductive number of $R$.
We then define $Y(g) = R-R_{\\rm TTIQ}(g)$ as reduction of transmission due to TTIQ, and $Y(0) = R-R_{\\rm TTIQ}(0)$ as the reduction of transmission due only to isolation (i.e. no contact tracing \\& quarantine).
We then define the fraction of prevented transmission due to quarantine as
$[Y(g)-Y(0)]/Y(g)$, which we plot as a function of $g$.
We vary the fraction of symptomatic index cases that are isolated $f$ (colour), and we fix $\\Delta_1 = \\Delta_2 = \\tau = 2$ days.
We further fix the fraction of transmission that is attributed to asymptomatic infections to $\\alpha=20\\%$ and $R=1.5$ (although the fraction shown is independent of $R$).
Above the horizontal line, more transmission is prevented by quarantine than by isolation."

paramList <- list(
  alpha = 0.2,
  f = seq(0.2,1,0.2),
  g = seq(0,1,0.05),
  Delta1 = 2,
  Delta2 = 2,
  tau = 2,
  R = 1.5
)
load("data/savedDistributions.RData")
tertiaryCasesMLE <-
  getTertiaryCases(paramList = paramList, times = times, stepSize = stepSize, infProf.df = infProfMLE,
                   genDist.df = genDistMLE, integral.df = integralMLE)

#' Prepare and plot
df <- tertiaryCasesMLE
#' Extract number of tertiary cases without contact tracing
df.zero <- df[which(df$g == "0"), c("R","alpha","f","Delta1","tau","Delta2","tertiaryCases")]
names(df.zero)[names(df.zero) == "tertiaryCases"] <- "tertiaryCases.zero"
df <- merge(df, df.zero, sort = F)
df$ter.per.sec.zero <- df$tertiaryCases.zero / df$secondaryCases

df$prevented.iso <- (as.numeric(levels(df$R))[df$R]) - df$ter.per.sec.zero
df$prevented.q <- (as.numeric(levels(df$R))[df$R]) - df$ter.per.sec
df$frac <- (df$prevented.q - df$prevented.iso) / df$prevented.q

df$g <- as.numeric(levels(df$g))[df$g]


ggplot(df, aes(x = g, y = frac, colour = f, group = f)) +
  geom_hline(yintercept = 0.5, colour = "darkgrey", size = lineSize) +
  geom_line() +
  scale_colour_viridis_d(end = 0.9, option = "inferno", labels = scales::percent(as.numeric(levels(df$f))),
                         name = "fraction of\nsymptomatic index\ncases isolated (f)") +
  scale_x_continuous(breaks = seq(0,1,0.1), labels = function(x) scales::percent(x, accuracy = 1)) +
  scale_y_continuous(breaks = seq(0,1,0.1), labels = function(x) scales::percent(x, accuracy = 1)) +
  labs(x = expression("fraction of contacts quarantined"~(g)), y = "fraction of prevented transmission due to quarantine") +
  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,0.65), expand = F) +
  plotTheme + theme(legend.position = c(0.8,0.3), plot.margin = unit(20*c(1,1,1,1), "points"), panel.grid.major.x = plotTheme$panel.grid.major.y)
```


# Linear discriminant analysis (LDA) {-}
```{r LDA-1D-dist, dependson = "LDA-1D-calc", fig.height = 6, fig.cap = caption}
caption <- "The distributions of (normalised) parameters per categorised group of $R_{\\rm TTIQ}$ as used in the LDA analysis in Fig. \\ref{fig:LDA-1D-plot}.
We uniformly sample 10,000 parameter combinations from $f \\in [0\\%,100\\%]$, $g \\in [0\\%,100\\%]$, $\\Delta_1 \\in [0,5]$ days, $\\Delta_2 \\in [0,5]$ days, and $\\tau \\in [0,5]$ days.
The reproductive number $R_{\\rm TTIQ}$ is calculated for each parameter combination and categorised into bins of width $0.1$ (colour).
The upper row shows how many parameter combinations resulted in each category of $R_{\\rm TTIQ}$.
The next five rows show how the parameters are distributed within each category, while the horizontal bar shows the median parameter value.
We fix $R=1.5$ and $\\alpha=20\\%$."

load("data/LDA-parameter-distributions.RData")

# Format the parameter dataframe
df.melt <- melt(df, id.vars = c("class","output"), measure.vars = c("f","Delta1","g","Delta2","tau"), variable.name = "param")
df.melt$param <- factor(df.melt$param, levels = levels(df.melt$param), labels = expressions[levels(df.melt$param)])
df.melt$discrete <- ff[as.numeric(df.melt$class)]


# Count the number of samples per group
df.melt$i <- 1
labs <- aggregate(i ~ class + param, df.melt, sum)
labs$value <- NA
labs$discrete <- ff[as.numeric(labs$class)]
# Plot
param.histogram <- ggplot(data = labs[labs$param == "tau",], aes(x = class, fill = discrete, y = i)) +
    geom_col(colour = "black") +
    geom_text(aes(x = class, y = i+250, label = i), size = 3) +
    scale_fill_gradientn(colours = c("seagreen","white","white", "white","salmon"),
                         values = scales::rescale(c(output.min,1-.Machine$double.eps,1,1+.Machine$double.eps,output.max)),
                         limits = c(output.min,output.max),
                         guide = guide_colorbar(direction = "horizontal")) +
    scale_y_continuous(name = "no. samples") +
    plotTheme + theme(legend.position = "none", axis.title.x.bottom = element_blank(), axis.text.x.bottom = element_blank())

param.violins <- ggplot(df.melt, aes(x = class, y = value, fill = discrete)) +
    facet_grid(param ~ ., labeller = label_parsed) +
    #geom_point(position = position_jitter(width = 0.2, height = 0)) +
    geom_violin(alpha = 0.8, colour = "black", scale = "width", draw_quantiles = 0.5) +
    scale_fill_gradientn(colours = c("seagreen","white","white", "white","salmon"), name = "tertiary\ncases",
                         values = scales::rescale(c(output.min,1-.Machine$double.eps,1,1+.Machine$double.eps,output.max)),
                         limits = c(output.min,output.max),
                         guide = guide_colorbar(direction = "horizontal")) +
    scale_y_continuous(breaks = seq(0,1,0.25)) +
    labs(x = expression(R[TTIQ]), y = "normalised parameter value") +
    coord_cartesian(ylim = c(0,1)) +
    plotTheme + theme(legend.position = "none", strip.text.y = element_text(angle = 0))

plot_grid(
  param.histogram, param.violins,
  align = "hv", axis = "lr", rel_heights = c(1,4), ncol = 1
)
```


```{r LDA-vary-range-calc, message = F}
ranges <- seq(2,10,1)
paramImpact <- lapply(ranges, function(time.range) {
  #' Parameter ranges
  f.range <- list(min = 0, max = 1)
  g.range <- list(min = 0, max = 1)
  Delta1.range <- list(min = 0, max = time.range)
  Delta2.range <- list(min = 0, max = time.range)
  tau.range <- list(min = 0, max = time.range)
  R.range <- 1.5
  alpha.range <- 0.2
  
  #' Random uniform sampling
  set.seed(42)
  n <- 10000
  f.unif <- runif(n = n, min = f.range$min, max = f.range$max)
  g.unif <- runif(n = n, min = g.range$min, max = g.range$max)
  Delta1.unif <- runif(n = n, min = Delta1.range$min, max = Delta1.range$max)
  Delta2.unif <- runif(n = n, min = Delta2.range$min, max = Delta2.range$max)
  tau.unif <- runif(n = n, min = tau.range$min, max = tau.range$max)
  
  paramList <- lapply(seq_len(n), function(i) {
    list(
      f = f.unif[i],
      g = g.unif[i],
      Delta1 = Delta1.unif[i],
      Delta2 = Delta2.unif[i],
      tau = tau.unif[i],
      R = R.range,
      alpha = alpha.range
    )
  })
  
  #' Compute tertiary cases
  load("data/savedDistributions.RData")
  tertiaryCases <- ParLapply(seq_len(n), function(i) {
    getTertiaryCases(paramList = paramList[[i]], times = times, stepSize = stepSize,
                     infProf.df = infProfMLE, genDist.df = genDistMLE,
                     integral.df = integralMLE)
  }) %>% bind_rows()
  
  df <- tertiaryCases
  #' Enumerate and normalise param values
  df$f <- as.numeric(levels(df$f))[df$f] / (f.range$max - f.range$min)
  df$g <- as.numeric(levels(df$g))[df$g] / (g.range$max - g.range$min)
  df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1] / (Delta1.range$max - Delta1.range$min)
  df$Delta2 <- as.numeric(levels(df$Delta2))[df$Delta2] / (Delta2.range$max - Delta2.range$min)
  df$tau <- as.numeric(levels(df$tau))[df$tau] / (tau.range$max - tau.range$min)
  
  names(df)[names(df) == "ter.per.sec"] <- "output"
  df <- df[,c("output","f","g","Delta1","Delta2","tau")]
  
  #' Discretise cases into bins
  bin <- 0.1
  output.min <- 0.0
  output.max <- 1.5
  df$class <- cut(df$output, seq(output.min, output.max, bin), right = F)
  #' Perform the LDA analysis based on the discrete R class
  lda <- lda(formula = class ~ f + g + Delta1 + Delta2 + tau, data = df)
  
  #' LDA proportion of variance per dimension
  prop.lda <- lda$svd^2 / sum(lda$svd^2)
  
  #' LD1 components for each parameter
  lda.vectors <- data.frame(
    range = factor(time.range, levels = ranges),
    param = factor(rownames(lda$scaling),
                   levels = c("f","g","Delta1","Delta2","tau")),
    LD1 = lda$scaling[,"LD1"],
    row.names = NULL)
  #' Parameter name as expression
  expressions <- c(f = "f", g = "g", tau = "tau", Delta2 = "Delta[2]", Delta1 = "Delta[1]")
  lda.vectors$param.expression <- expressions[as.character(levels(lda.vectors$param))[lda.vectors$param]]
  
  return(lda.vectors)
}) %>% bind_rows()
```

```{r LDA-vary-range-plot, dependson = c(-1), message = F, fig.cap = caption}
caption <- "Impact of varying the range from which we sample time-dependent parameters on the LDA output.
Each bar represents the magnitude of the components of the primary linear discriminant vector (LD1) for each parameter (colour)."

myColours <- c(f = palette_OkabeIto[1], g = palette_OkabeIto[4],
               Delta1 = palette_OkabeIto[2], Delta2 = palette_OkabeIto[5],
               tau = palette_OkabeIto[3])

myLabs <- expression(f,g,Delta[1],Delta[2],tau)
names(myLabs) <- c("f","g","Delta1","Delta2","tau")

ggplot(paramImpact, aes(x = range, y = abs(LD1), fill = param)) +
  geom_col(position = position_dodge(), colour = "white") +
  scale_fill_manual(values = myColours, name = "parameter", labels = myLabs) +
  labs(x = "upper limit of time range (days)", y = "|LD1|") +
  plotTheme
```


\clearpage
# References
