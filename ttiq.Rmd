---
title: "Quantifying the impact of test-trace-isolate-quarantine (TTIQ) strategies on COVID-19 transmission"
author:
  - "Peter Ashcroft\\textsuperscript{1}, Sonja Lehtinen\\textsuperscript{1}, and Sebastian Bonhoeffer\\textsuperscript{1}"
  - "\\textsuperscript{1}*Institute of Integrative Biology, ETH Zurich, Switzerland*"
output:
  pdf_document:
    citation_package: natbib
    fig_caption: true
    latex_engine: pdflatex
bibliography: 2020-ttiq.bib
biblio-style: apalike
---

```{r setup, include = F}
knitr::opts_chunk$set(
  echo = FALSE,
  fig.width = 4,
  fig.height = 4,
  cache = TRUE,
  warning = FALSE,
  eval.after = "fig.cap"
)
library(tidyverse)
library(reshape2)
library(parallel)
library(cowplot)
library(stats)
library(MASS)
library(RColorBrewer)

#' Some plot functions
plotTheme <- theme_bw() + theme(plot.background = element_rect(fill = "white", colour = "white"), panel.grid = element_blank(), strip.background = element_blank())
#' Reduce text size
plotTheme$text$size <- 12

palette_OkabeIto <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

lineSize <- 1.2

dayLabels <- function(x) {
  labs <- paste(x, ifelse(x == 1, "day", "days"))
  names(labs) <- x
  return(labs)
}

#' Parallel loop function
ParLapply <- function(X, FUN, ..., PARALLEL = TRUE, SEED = NULL) {
  if (PARALLEL) {
    if (is.null(SEED)) SEED <- 111
    num.cores <- min(c(length(X), max(detectCores() - 1, 1)))
    cl <- makeCluster(num.cores, type = "FORK")
    clusterSetRNGStream(cl, SEED)
    out <- parLapply(cl, X, FUN, ...)
    stopCluster(cl)
    return(out)
  } else {
    out <- lapply(X, FUN, ...)
    return(out)
  }
}
```

# Abstract
The test-trace-isolate-quarantine (TTIQ) strategy is used to break chains of transmission during a disease outbreak.
Confirmed-positive pathogen carriers are isolated from the community to prevent onward transmission and their recent close contacts are identified and pre-emptively quarantined.
TTIQ, along with mask wearing and social distancing, make up the non-pharmaceutical interventions that are utilised to suppress the ongoing SARS-CoV-2 pandemic.
The efficacy of the TTIQ strategy depends on the probability of isolating a case, the fraction of contacts quarantined, and the delays in these processes.
Here we use empirical distributions of the timing of SARS-CoV-2 transmission to quantify how these parameters individually contribute to the reduction of onwards infection.
We show that finding and isolating index cases, and doing so with minimal delay after symptom onset, have the largest effects on case reduction, and that contact tracing can make up for deficiencies in testing coverage and delays.
These results can be used to assess how TTIQ can be improved and optimised.
We provide an online application to assess the efficacy as a function of these parameters.

# Introduction
Individuals who are confirmed as infected with the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pathogen are isolated from the population to prevent further transmission.
The individuals who have been in recent close contact with an infected individual have an increased risk of being infected themselves.
By identifying the potentially-infected contacts through contact tracing, and eventually quarantining them, transmission chains can be broken.
Thus contact tracing is an essential public health tool for controlling epidemics \citep{WHO:contactTracing}.
The strategy of testing to identify infected cases, isolating them to prevent further transmission, and tracing \& quarantining their recent close contacts is known as test-trace-isolate-quarantine (TTIQ) \citep{salathe:SwissMed.Wkly.:2020}.
This strategy is a fundamental non-pharmaceutical intervention which is used globally to control the ongoing SARS-CoV-2 pandemic \citep{kucharski:TheLancetInfectiousDiseases:2020}.

Testing typically occurs once an individual develops symptoms of coronavirus disease 2019 (COVID-19).
As presymptomatic transmission makes up approximately 40\% of total onward transmission \citep{he:NatMed:2020,ashcroft:SMW:2020,ferretti:medRxiv:2020}, it would be possible for the number of secondary cases to be more than halved if infected individuals are isolated from the community at the time of symptom onset. 
However, as testing follows from symptoms, the testing \& isolating strategy without subsequent contact tracing \& quarantine is unlikely to capture asymptomatic cases which make up 20\% of cases \citep{buitrago-garcia:PLOSMedicine:2020}, and thus isolating 100\% of cases would not be possible.

Contact tracing \& quarantine have the potential to be effective interventions against the spread of COVID-19 because of the high frequency of pre-symptomatic or asymptomatic transmission from recently-infected individuals \citep{moghadas:PNAS:2020}.
Potentially-infected contacts can be identified and quarantined before they would be isolated as a result of developing symptoms and/or receiving a positive test result, such that their onward transmission is reduced.
This is exemplified in the light of the high dispersion of the offspring distribution and frequency of super-spreader events \citep{riou:Eurosurveillance:2020,endo:WellcomeOpenRes:2020a,adam:Nat.Med.:2020}, where large numbers of potentially-infected contacts can be quarantined to prevent widespread community transmission.
Tracing \& quarantine does not depend on symptom development.
Hence, this strategy is capable of reducing onward transmission even from asymptomatically infected individuals.

TTIQ strategies are not perfect: each stage in the process is subject to delays and uncertainties and it would be impossible to prevent all onward transmission through TTIQ alone \citep{ferretti:Science:2020,kucharski:TheLancetInfectiousDiseases:2020,kretzschmar:TheLancetPublicHealth:2020,quilty:medRxiv:2020,ashcroft:medRxiv:2020}.
Furthermore, in the presence of widespread community transmission the contact tracers may be overwhelmed by the volume of cases.
In this scenario it is important to optimise the resources (i.e. the person hours of the contact tracers) to minimise onward transmission.

In a previous study of TTIQ efficacy, \citet{ferretti:Science:2020} used an approach based on the empirically-observed timing of transmission events -- but with substantial approximations around the TTIQ process -- to get to an analytically tractable prediction of the impact of TTIQ on SARS-CoV-2 transmission.
They concluded that widespread digital contact tracing (with minimal delay between index case identification and quarantine of secondary cases) would be necessary to reduce the effective reproduction number, $R_e$, below one to bring an outbreak under control.
\citet{kucharski:TheLancetInfectiousDiseases:2020} used an agent-based model with detailed contact structures to simulate intervention strategies.
While the TTIQ process is more accurately described than in \citet{ferretti:Science:2020}, they did not use empirical data about the timing of transmission, which is crucial for quantifying the impact of isolation and quarantine.
\citet{kretzschmar:TheLancetPublicHealth:2020} opted for a discrete-time branching process model of transmission and TTIQ.
While they explicitly accounted for the timing of infection events and accurately described the TTIQ process, they predominantly focussed on assessing the role of digital contact tracing based on mobile applications.

In this paper we develop an analytical approach which builds on our previous work in which we have quantified the impact of quarantine duration and highlighted the optimal use of test-and-release strategies \citep{ashcroft:medRxiv:2020}.
Briefly, we use the empirically-observed distributions of transmission timing [Fig. \ref{fig:distributions}; \citet{ferretti:medRxiv:2020}] to determine when infections occur (Fig. \ref{fig:TTIQ-schematic}).
We then introduce five parameters to describe the TTIQ process:
i) $f$, the probability that an index case is isolated from the population and is interviewed by contact tracers;
ii) $\Delta_1$, the time delay between symptom onset and isolation of the index case;
iii) $\tau$, the duration prior to symptom onset in which contacts are identifiable;
iv) $g$, the fraction of identifiable contacts that are quarantined;
and v) $\Delta_2$, the delay between isolation of the index case and the start of quarantine for the contacts.
We compute the expected number of tertiary cases per index case under the TTIQ interventions, with the aim being to reduce this number below one to suppress the growth of the epidemic (see Methods for details).
We systematically explore this parameter space, first for the "testing \& isolation" intervention in the absence of contact tracing (Fig. \ref{fig:TTIQ-schematic}A), and then with additional "tracing \& quarantine" (Fig. \ref{fig:TTIQ-schematic}B).

```{r TTIQ-schematic, fig.align = "center", out.width = "\\textwidth", out.height = "\\textheight", fig.cap = caption}
caption <- "A) Under testing \\& isolation, index cases are identified and isolated from the population after a delay $\\Delta_1$ after they develop symptoms (at time $t_{S_1}$).
This curtails their duration of infectiousness and reduces the number of secondary cases.
B) Under tracing \\& quarantine, the contacts of an index case are identified and quarantined after an additional delay $\\Delta_2$.
This reduces the onward transmission from the secondary cases.
Only contacts that occur during a contact tracing window can be identified.
This window extends from $t_{S_1}-\\tau$ (i.e. $\\tau$ days before the index case developed symptoms) to $t_{S_1}+\\Delta_1$ (i.e. when the index case was isolated).
Shown distributions are schematic representations of those shown in Fig. \\ref{fig:distributions}."
knitr::include_graphics("TTIQ-schematic.pdf")
```

# Methods
Our primary goal is to quantify the reduction of transmission by isolating individuals who test positive for SARS-CoV-2 and by quarantining their recent close contacts with an increased risk of infection.
We refer to the initial confirmed case as the index case, and the infected contacts as secondary cases.
We know that the index case developed symptoms at time $t_{S_1}$, but the time at which they were infected, $t_1$, is generally unknown.
Secondary cases will be infected by the index case at some time $t_2$ ($t_2 > t_1$), and develop symptoms at time $t_{S_2}$ (Fig. \ref{fig:distributions}A).

## Generation times, infectivity profiles, and incubation periods
The relationships between the times $t_1$, $t_{S_1}$, $t_2$, $t_{S_2}$ are determined by:
the generation time distribution, $q(t_2-t_1|\theta_q)$, describing the time interval between the infection of an index case and secondary case (Fig. \ref{fig:distributions}B);
the infectivity profile, $p(t_2-t_{S_1}|\theta_p)$, describing the time interval between the onset of symptoms in the index case and infection of the secondary case (Fig. \ref{fig:distributions}C);
and the incubation period distribution, $g(t_{S_1}-t_1)$, describing the time between the infection of an individual and the onset of their symptoms (Fig. \ref{fig:distributions}D).
For these distributions, we use empirical estimates from \citet{ferretti:medRxiv:2020} which are based on a large set of transmission pairs and minimal assumptions about the relationship between infectiousness and symptoms, which would otherwise bias the resulting generation time distribution \citep{lehtinen:medRxiv:2020}.

```{r incubation-distribution-definition-Ferretti, include = F}
# #' Code from Ferretti et al.
# inc<-function(x){
#   (
#     dlnorm(x,meanlog = 1.621, sdlog = 0.418)+ # Lauer
#     dlnorm(x,meanlog = 1.425, sdlog = 0.669)+ # Li
#     dlnorm(x,meanlog = 1.57, sdlog = 0.65)+ # Bi
#     dlnorm(x,meanlog = 1.53, sdlog = 0.464)+ # Jiang???
#     dlnorm(x,meanlog = 1.611, sdlog = 0.472)+ # Linton
#     dlnorm(x,meanlog = 1.54, sdlog = 0.47)+ # Zhang
#     dlnorm(x,meanlog = 1.857, sdlog = 0.547) # Ma
# )/7
# }
# incCum<-function(x){
#   (
#     plnorm(x,meanlog = 1.621, sdlog = 0.418)+
#       plnorm(x,meanlog = 1.425, sdlog = 0.669)+
#       plnorm(x,meanlog = 1.57, sdlog = 0.65)+
#       plnorm(x,meanlog = 1.53, sdlog = 0.464)+
#       plnorm(x,meanlog = 1.611, sdlog = 0.472)+
#       plnorm(x,meanlog = 1.54, sdlog = 0.47)+
#       plnorm(x,meanlog = 1.857, sdlog = 0.547)
#   )/7
# }

# Incubation period distribution is averaged across 7 reported distributions:
incParams <- rbind(
  data.frame(study = "Bi", n = 183, meanlog = 1.570, sdlog = 0.650), # Reported
  data.frame(study = "Lauer", n = 181, meanlog = 1.621, sdlog = 0.418), # Reported
  data.frame(study = "Li", n = 10, meanlog = 1.434, sdlog = 0.661), # From source code of He et al.
  data.frame(study = "Linton", n = 158, meanlog = 1.611, sdlog = 0.472), # with(list(mu = 5.6, sigma = 2.8), c(mean = log(mu^2 / sqrt(mu^2 + sigma^2)), sd = sqrt(log(1 + sigma^2 / mu^2))))
  data.frame(study = "Ma", n = 587, meanlog = 1.857, sdlog = 0.547), # with(list(mu = 7.44, sigma = 4.39), c(mean = log(mu^2 / sqrt(mu^2 + sigma^2)), sd = sqrt(log(1 + sigma^2 / mu^2))))
  data.frame(study = "Zhang", n = 49, meanlog = 1.540, sdlog = 0.470), # Reported
  data.frame(study = "Jiang", n = 2015, meanlog = 1.530, sdlog = 0.464) # Unknown...
)
incParams$study <- factor(incParams$study)
#' Mean and SD of this incubation time
incParams$mean <- mean(exp(incParams$meanlog + (incParams$sdlog^2)/2))
#sqrt(mean((exp(incParams$sdlog^2)-1) * exp(2*incParams$meanlog + incParams$sdlog^2))) # SD

#' PDF and CDF of the incubation period
getIncubationPeriod <- function(times, params, CDF = F) {
  y <- sapply(levels(params$study), function(study) {
    if (CDF) { # Cumulative density function
      return(plnorm(q = times,
                    meanlog = params[params$study == study, "meanlog"],
                    sdlog = params[params$study == study, "sdlog"]))
    } else { # Probability density function
      return(dlnorm(x = times,
                    meanlog = params[params$study == study, "meanlog"],
                    sdlog = params[params$study == study, "sdlog"]))
    }
  })
  
  #' Average over the studies
  if (length(times) == 1) return(mean(y))
  else return(apply(y,1,mean))
}
```

```{r incubation-periods-plot, fig.width = 8, eval = F}
times <- seq(0,20,0.1)

#' Define the incubation distribution from meta-distribution
incDist <- data.frame(
  t = times,
  pdf = getIncubationPeriod(times = times, params = incParams),
  CDF = getIncubationPeriod(times = times, params = incParams, CDF = TRUE),
  study = "combined"
)

#' Calculate incubations distributions for each study individually
incDistSep <- lapply(levels(incParams$study), function(study) {
  params <- incParams[incParams$study == study, ]
  params$study <- factor(study, levels = study)
  
  data.frame(
    t = times,
    pdf = getIncubationPeriod(times = times, params = params),
    CDF = getIncubationPeriod(times = times, params = params, CDF = TRUE),
    study = study
  )
})

df <- rbind(incDist, do.call(rbind,incDistSep))
df$study <- factor(df$study, levels = c("combined", levels(incParams$study)))
colours <- c("black", brewer.pal(length(levels(incParams$study)),"Set1"))

pdfPlot <- ggplot(df, aes(x = t, y = pdf, colour = study)) +
  geom_line(aes(size = ifelse(study == "combined", 1.5, 1.2))) +
  scale_colour_manual(values = colours) +
  guides(size = F) +
  coord_cartesian(xlim = c(0,20), ylim = c(0,0.23), expand = F) +
  labs(x = "incubation period (days)", y = "probability density") +
  ggtitle("incubation period distribution") +
  plotTheme +
  theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5),
        legend.position = c(0.7,0.8))

cdfPlot <- ggplot(df, aes(x = t, y = CDF, colour = study)) +
  geom_line(aes(size = ifelse(study == "combined", 1.5, 1.2))) +
  scale_colour_manual(values = colours) +
  guides(size = F) +
  coord_cartesian(xlim = c(0,20), ylim = c(0,1), expand = F) +
  labs(x = "incubation period (days)", y = "cumulative probability") +
  ggtitle("cumulative incubation period distribution") +
  plotTheme +
  theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5),
        legend.position = "none")

plot_grid(pdfPlot, cdfPlot, align = "hv", axis = "tb", nrow = 1)
```

```{r serial-interval-data, include = F, eval = F}
#' Data from Ferretti (40), Xia (32), Zhang (35), He (66)
info_data <- read.csv("data/Ferretti-TableTransmissionPairs_addendum.tsv", sep = "\t", stringsAsFactors = F)
location <- info_data$Country
info_data[,"Exponential.growth"] <- 0 + (info_data[,"Exponential.growth"] == "YES")
info_data <- cbind(
  apply(info_data[,c("T1L","T1R","s1","T2L","T2R","s2","Tr","Exponential.growth")], 2, as.numeric),
  info_data[,"Source.manuscript", drop = F]
)
info_data <- as.data.frame(info_data)

#' Exposure intervals for non-tested cases in Cheng
dnont <- read.csv("data/Ferretti-Taiwan_transmission_pair_contact_exposure_v0514.csv")
dnont <- dnont[is.na(dnont$test),]
dnont <- data.frame(
  left_exposure = as.numeric(difftime(dnont[,"time2exposure_start"], dnont[,"onset_source"], units = "d")),
  right_exposure = as.numeric(difftime(dnont[,"time2exposure_end"], dnont[,"onset_source"], units = "d"))
)
dnont <- dnont[dnont[,"left_exposure"] <= dnont[,"right_exposure"],]
dnont <- dnont[dnont$left_exposure > -100,]

#' Data of symptomatic cases from Cheng (18)
dt <- read.csv("data/Ferretti-Taiwan_transmission_pair_v0514.csv")
dt <- dt[!is.na(dt$onset),]
#
dtrans <- data.frame(
  left_exposure = as.numeric(difftime(dt[,"time2exposure_start"], dt[,"onset_source"], units = "d")) - 0.5,
  right_exposure = as.numeric(difftime(dt[,"time2exposure_end"], dt[,"onset_source"],units = "d")) + 0.5,
  case.noncase = "case",
  serial_interval = as.numeric(difftime(dt[,"onset"], dt[,"onset_source"], units = "d"))
)
dtrans$left_exposure[dtrans$left_exposure < -100] <- NA

# Joining Ferretti (40), Xia (32), Zhang (35), He (66), and Cheng (18) datasets
info_data_taiwan <- data.frame(
  T1L = NA,
  T1R = NA,
  s1 = 0,
  T2L = dtrans$left_exposure + 0.5,
  T2R = dtrans$right_exposure - 0.5,
  s2 = dtrans$serial_interval,
  Tr = 100,
  Exponential.growth = 0,
  Source.manuscript = "Cheng"
)
serial_interval_data <- rbind(info_data, info_data_taiwan)
save(serial_interval_data, file = "data/Ferretti-serial_interval_data.RData")
```

```{r serial-interval-data-plot, eval = F}
load("data/Ferretti-serial_interval_data.RData")
data <- serial_interval_data
#' Compute serial interval from symptom onsets
data$SI <- data$s2 - data$s1
data$Source.manuscript <- factor(data$Source.manuscript,
                                 levels = c("Zhang", "He", "Ferretti", "Xia", "Cheng"))
minSI <- min(data$SI) - 1.5
maxSI <- max(data$SI) + 1.5
ggplot() +
  geom_histogram(data = data,
                 aes(x = SI, fill = Source.manuscript),
                 breaks = seq(from = minSI, to = maxSI)) +
  scale_fill_brewer(palette = "Set1") +
  coord_cartesian(expand = F) +  # no gaps between bars and axes
  labs(fill = "Dataset:",
       x = "Serial interval (days)",
       y = "Count") +
  plotTheme +
  theme(legend.position = c(0.8, 0.8)) 
```

```{r fit-infectivity-profile, include = F, eval = F}
load("data/Ferretti-serial_interval_data.RData")

#' Maximum duration of infection (days)
M <- 30
mintW <- 2 * M + 2
#' Growth rate of the epidemic (doubling time of 5 days)
r <- log(2)/5

#' Discrete incubation time distribution (integrated probability per day)
Inc <- getIncubationPeriod(seq(0,2*M) + 0.5, params = incParams, CDF = T) -
    getIncubationPeriod(pmax(seq(0,2*M) - 0.5, 0), params = incParams, CDF = T)

#' Student t-distribution for infectivity profile
infectivityProfile <- function(x, pars, CDF = F) {
  if (CDF) {
    return(pt((x - pars["shift"])/exp(pars["log_scale"]), df = exp(pars["log_df"])))
  } else {
    return(dt((x - pars["shift"])/exp(pars["log_scale"]), df = exp(pars["log_df"])) / exp(pars["log_scale"]))
  }
}

#' Limits for the exposure time integrals (this is evaluated per transmission pair)
getLimits <- function(pair_data) {
  v <- list()
  v$T1L <- max(pair_data$T1L, pair_data$s1 - 2*M, na.rm = T)
  v$T1R <- min(pair_data$T1R, pair_data$s1, pair_data$s2, pair_data$T2R, na.rm = T)
  v$T2L <- max(pair_data$T2L, pair_data$s2 - 2*M, v$T1L, na.rm = T)
  v$T2R <- min(pair_data$T2R, pair_data$s2, na.rm = T)
  return(v)
}

#' Perform the double integral (or sum) per transmission pair
#' to get probability of observing serial interval
getProbSI <- function(pair_data, W, limits) {
  sum(
    sapply(seq(limits$T1L, limits$T1R), function(t1) {
      exp(r * pair_data$Exponential.growth * t1) *
        Inc[pair_data$s1 - t1 + 1] * 
        sum(
          sapply(seq(max(t1, limits$T2L), limits$T2R), function(t2) {
            W[t2 - pair_data$s1 + mintW] * Inc[pair_data$s2 - t2 + 1]
          })
        )
    })
  )
}

#' log-likelihood of a single transmission pair (1 row of data)
getLLH <- function(pair_data, W) {
  limits <- getLimits(pair_data)
  log(getProbSI(pair_data, W, limits))
}

#' Evaluate the new profile as a function of the optimisation parameters,
#' and then compute the negative log-likelihood for each transmission pair
getTotalLLH <- function(pars, data, w) {
  x <- seq(0, (4*M + mintW)) - (mintW - 1)
  W <- w(x + 0.5, pars, CDF = T) - w(x - 0.5, pars, CDF = T)
  return(sum(apply(data, 1, function(data_row) getLLH(pair_data = as.list(data_row), W))))
}

#' Set up and run optimiser
data <- subset(
  serial_interval_data,
  select = -c(Source.manuscript),
  subset = (serial_interval_data$Source.manuscript %in% c("Ferretti", "Xia", "He", "Cheng"))
)

optim_out <- optim(par = c(shift = 0, log_scale = 0, log_df = 0),
                    fn = getTotalLLH, data = data, w = infectivityProfile,
                    control = list(fnscale = -1))
optim_out <- optim(par = optim_out$par,
                    fn = getTotalLLH, data = data, w = infectivityProfile,
                    control = list(fnscale = -1))

#' Confidence interval
times <- seq(-10, 10, 0.1)
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
#' Threshold likelihood value for 95%
minLLH <- optim_out$value - qchisq(0.95, df = length(optim_out$par))/2
#' Parameters of the best fitting model
initpars <- optim_out$par
pars <- initpars
df_param <- data.frame(shift = pars["shift"], scale = exp(pars["log_scale"]),
                       df = exp(pars["log_df"]), llh = optim_out$value)
#' Create a random vector on a sphere in the 3D parameter space
set.seed(42)
dir <- qnorm(runif(length(optim_out$par)))
dir <- dir/sqrt(sum(dir^2))
i <- 0
while (i < 1000) {
  #' Perturb parameters along the vector (Monte Carlo)
  pars <- pars + dir*runif(1)*0.2
  llh <- getTotalLLH(pars, data, infectivityProfile)
  if(llh > minLLH) {
    #' Which points make up the confidence interval
    infProf <- infectivityProfile(times, pars)
    lower <- pmin(infProf, lower, na.rm = T)
    upper <- pmax(infProf, upper, na.rm = T)
    df_param <- rbind(df_param, data.frame(shift = pars["shift"], scale = exp(pars["log_scale"]), df = exp(pars["log_df"]), llh = llh))
    i <- i + 1
  } else {
    #' Change the direction of perturbation
    dir <- qnorm(runif(length(optim_out$par)))
    dir <- dir/sqrt(sum(dir^2))
    pars <- initpars
  }
}
#' Save parameters and likelihood value
infParamsLLH <- df_param
save(infParamsLLH, file = "data/infParamsLLH.RData")
```

```{r plot-infectivity-profile, eval = F}
df_plot <- data_frame(
  days = times,
  infectivityProfile = infectivityProfile(times, optim_out$par),
  incubation = getIncubationPeriod(times, params = incParams),
  lower = lower,
  upper = upper
)

# new_df <- lapply(seq_len(nrow(df_param)), function(i) {
#   data.frame(
#     id = factor(i),
#     llh = df_param[[i,"llh"]],
#     days = times,
#     infProf = infectivityProfile(times, c(shift = df_param[[i,"shift"]], log_scale = log(df_param[[i,"scale"]]), log_df = log(df_param[[i,"df"]]))))
# })
# new_df <- do.call(rbind, new_df)

ggplot(data = df_plot, aes(x = days, y = infectivityProfile)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), color = "transparent", fill = "grey") +
  geom_line() +
  geom_vline(xintercept = 0, linetype = "dotted") +
  #geom_line(data = new_df, aes(x = days, y = infProf, colour = llh, group = id)) +
  #scale_colour_viridis_c() +
  labs(x = "TOST (days)", y = "Probability density (per day)") +
  coord_cartesian(xlim = c(-7.5, 10), expand = F) +
  plotTheme
```

```{r infectivity-profile-definition, include = F}
#' Parameters
infParams <- data.frame(shift = -0.0776, scale = 1.857, df = 3.345)
infParams$mean <- infParams$shift/infParams$scale
#' Function
getInfectivityProfile <- function(times, params, CDF = FALSE) {
  if (CDF) return(pt(q = (times - params$shift)/params$scale, df = params$df))
  else return(dt(x = (times - params$shift)/params$scale, df = params$df)/params$scale)
}
```

```{r fit-generation-time, include = F, eval = F}
load("data/Ferretti-serial_interval_data.RData")

#' Maximum duration of infection (days)
M <- 30
#' Growth rate of the epidemic (doubling time of 5 days)
r <- log(2)/5

#' Discrete incubation time distribution (integrated probability per day)
Inc <- getIncubationPeriod(seq(0,2*M) + 0.5, params = incParams, CDF = T) -
    getIncubationPeriod(pmax(seq(0,2*M) - 0.5, 0), params = incParams, CDF = T)

#' Weibull distribution for generation time
generationTime <- function(x, pars, CDF = F) {
  if (CDF) {
    return(pweibull(x, shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"])))
  } else {
    return(dweibull(x, shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"])))
  }
}

#' Limits for the exposure time integrals (this is evaluated per transmission pair)
getLimits <- function(pair_data) {
  v <- list()
  v$T1L <- max(pair_data$T1L, pair_data$s1 - 2*M, na.rm = T)
  v$T1R <- min(pair_data$T1R, pair_data$s1, pair_data$s2, pair_data$T2R, na.rm = T)
  v$T2L <- max(pair_data$T2L, pair_data$s2 - 2*M, v$T1L, na.rm = T)
  v$T2R <- min(pair_data$T2R, pair_data$s2, na.rm = T)
  return(v)
}

#' Perform the double integral (or sum) per transmission pair
#' to get probability of observing serial interval
getProbSI <- function(pair_data, W, limits) {
  sum(
    sapply(seq(limits$T1L, limits$T1R), function(t1) {
      exp(r * pair_data$Exponential.growth * t1) *
        Inc[pair_data$s1 - t1 + 1] * 
        sum(
          sapply(seq(max(t1, limits$T2L), limits$T2R), function(t2) {
            W[t2 - t1 + 1] * Inc[pair_data$s2 - t2 + 1]
          })
        )
    })
  )
}

#' log-likelihood of a single transmission pair (1 row of data)
getLLH <- function(pair_data, W) {
  limits <- getLimits(pair_data)
  log(getProbSI(pair_data, W, limits))
}

#' Evaluate the new profile as a function of the optimisation parameters,
#' and then compute the negative log-likelihood for each transmission pair
getTotalLLH <- function(pars, data, w) {
  x <- seq(0, 4*M)
  W <- w(x + 0.5, pars, CDF = T) - w(pmax(x - 0.5,0), pars, CDF = T)
  return(sum(apply(data, 1, function(data_row) getLLH(pair_data = as.list(data_row), W))))
}

#' Set up and run optimiser
data <- subset(
  serial_interval_data,
  select = -c(Source.manuscript),
  subset = (serial_interval_data$Source.manuscript %in% c("Ferretti", "Xia", "He", "Cheng"))
)

optim_out <- optim(par = c(log_shape = 0, log_scale = 0),
                    fn = getTotalLLH, data = data, w = generationTime,
                    control = list(fnscale = -1))
optim_out <- optim(par = optim_out$par,
                    fn = getTotalLLH, data = data, w = generationTime,
                    control = list(fnscale = -1))

#' Confidence interval
times <- seq(0, 20, 0.1)
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
#' Threshold likelihood value for 95%
minLLH <- optim_out$value - qchisq(0.95, df = length(optim_out$par))/2
#' Parameters of the best fitting model
initpars <- optim_out$par
pars <- initpars
df_param <- data.frame(shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"]),
                       llh = optim_out$value)
#' Create a random vector on a sphere in the 3D parameter space
set.seed(422)
dir <- qnorm(runif(length(optim_out$par)))
dir <- dir/sqrt(sum(dir^2))
i <- 0
while (i < 1000) {
  #' Perturb parameters along the vector (Monte Carlo)
  pars <- pars + dir*runif(1)*0.2
  llh <- getTotalLLH(pars, data, generationTime)
  if(llh > minLLH) {
    #' Which points make up the confidence interval
    genTime <- generationTime(times, pars)
    lower <- pmin(genTime, lower, na.rm = T)
    upper <- pmax(genTime, upper, na.rm = T)
    df_param <- rbind(df_param, data.frame(shape = exp(pars["log_shape"]), scale = exp(pars["log_scale"]), llh = llh))
    i <- i + 1
  } else {
    #' Change the direction of perturbation
    dir <- qnorm(runif(length(optim_out$par)))
    dir <- dir/sqrt(sum(dir^2))
    pars <- initpars
  }
}
#' Save parameters and likelihood value
genParamsLLH <- df_param
save(genParamsLLH, file = "data/genParamsLLH.RData")
```

```{r plot-generation-time, eval = F}
df_plot <- data_frame(
  days = times,
  generationTime = generationTime(times, optim_out$par),
  incubation = getIncubationPeriod(times, params = incParams),
  lower = lower,
  upper = upper
)

# new_df <- lapply(seq_len(nrow(df_param)), function(i) {
#   data.frame(
#     id = factor(i),
#     llh = df_param[[i,"llh"]],
#     days = times,
#     genTime = generationTime(times, c(log_shape = log(df_param[[i,"shape"]]), log_scale = log(df_param[[i,"scale"]]))))
# })
# new_df <- do.call(rbind, new_df)

ggplot(data = df_plot, aes(x = days, y = generationTime)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), color = "transparent", fill = "grey") +
  geom_line() +
  geom_vline(xintercept = 0, linetype = "dotted") +
  #geom_line(data = new_df, aes(x = days, y = genTime, colour = llh, group = id)) +
  #scale_colour_viridis_c() +
  labs(x = "generation time (days)", y = "Probability density (per day)") +
  coord_cartesian(xlim = c(0,15), expand = F) +
  plotTheme
```

```{r generation-time-distribution-definition, include = F}
#' Parameters
#genParams <- data.frame(shape = 3.2862, scale = 6.1244)
genParams <- data.frame(shape = 3.277, scale = 6.127)
genParams$mean <- genParams$scale * gamma(1 + 1/genParams$shape)
#' Function
getGenDist <- function(times, params, CDF = FALSE) {
  if (CDF) return(pweibull(q = times, shape = params$shape, scale = params$scale))
  else dweibull(x = times, shape = params$shape, scale = params$scale)
}
```

```{r save-distributions, include = F}
stepSize <- 1e-2
times <- seq(-25, 25, stepSize)

i0 <- which(times == 0)
iMax <- length(times)

#' Define the incubation distribution, which is independent of the generation time parameters
incDist <- data.frame(
  t = times,
  pdf = getIncubationPeriod(times = times, params = incParams),
  CDF = getIncubationPeriod(times = times, params = incParams, CDF = TRUE)
)

#' Infectivity profile
infProfMLE <- data.frame(
  t = times,
  pdf = getInfectivityProfile(times = times, params = infParams),
  CDF = getInfectivityProfile(times = times, params = infParams, CDF = T)
)
#' Now compute the confidence interval
load("data/infParamsLLH.RData")
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
for (id in seq_len(nrow(infParamsLLH))) {
  ip <- getInfectivityProfile(times = times, params = infParamsLLH[id,])
  lower <- pmin(ip, lower, na.rm = T)
  upper <- pmax(ip, upper, na.rm = T)
}
infProfMLE$lower <- lower
infProfMLE$upper <- upper

#' Generation time distribution
genDistMLE <- data.frame(
  t = times,
  pdf = getGenDist(times = times, params = genParams),
  CDF = getGenDist(times = times, params = genParams, CDF = T)
)
#' Now compute the confidence interval
load("data/genParamsLLH.RData")
upper <- rep(NA, length(times))
lower <- rep(NA, length(times))
for (id in seq_len(nrow(genParamsLLH))) {
  gt <- getGenDist(times = times, params = genParamsLLH[id,])
  lower <- pmin(gt, lower, na.rm = T)
  upper <- pmax(gt, upper, na.rm = T)
}
genDistMLE$lower <- lower
genDistMLE$upper <- upper

#' Integral of g(t')Q(t'+t) from 0 to \infty with 0 \le t < \infty
integralMLE <- data.frame(
  t = times,
  J = sapply(seq_along(times), function(i) {
    g <- incDist[seq(i0, iMax), "pdf"]
    Qt <- genDistMLE[seq(i, iMax - i0 + i), "CDF"]
    Qt[is.na(Qt)] <- 1
    sum(g * Qt) * stepSize
  }, USE.NAMES = F)
)

#' Save results
save(times, i0, iMax, stepSize, incDist, infProfMLE, genDistMLE, integralMLE, file = "data/savedDistributions.RData")
```

```{r save-distribution-samples, include = F, eval = T}
#' Load the MLE distributions
load("data/savedDistributions.RData")

#' Infectivity profile
load("data/infParamsLLH.RData")
infProfLLH <- ParLapply(seq_len(nrow(infParamsLLH)), function(id) {
  data.frame(
    t = times,
    pdf = getInfectivityProfile(times = times, params = infParamsLLH[id,]),
    CDF = getInfectivityProfile(times = times, params = infParamsLLH[id,], CDF = T)
  )
})

#' Generation time distribution
load("data/genParamsLLH.RData")
genDistLLH <- ParLapply(seq_len(nrow(infParamsLLH)), function(id) {
  data.frame(
    t = times,
    pdf = getGenDist(times = times, params = genParamsLLH[id,]),
    CDF = getGenDist(times = times, params = genParamsLLH[id,], CDF = T)
  )
})
#' Integral of g(t')Q(t'+t) from 0 to \infty with 0 \le t < \infty
integralLLH <- ParLapply(seq_len(nrow(infParamsLLH)), function(id) {
  data.frame(
    t = times,
    J = sapply(seq_along(times), function(i) {
      g <- incDist[seq(i0, iMax), "pdf"]
      Qt <- genDistLLH[[id]][seq(i, iMax - i0 + i), "CDF"]
      Qt[is.na(Qt)] <- 1
      sum(g * Qt) * stepSize
    }, USE.NAMES = F)
  )
})
save(infProfLLH, genDistLLH, integralLLH, file = "data/savedDistributionsLLH.RData")

#' Create a zip file with the data files:
#' - data/savedDistributions.RData
#' - data/savedDistributionsLLH.RData
#' - data/infParamsLLH.RData
#' - data/genParamsLLH.RData
# tar(tarfile = "archivedDistributions.tar.gz", files = c("data/savedDistributions.RData", "data/savedDistributionsLLH.RData",
# "data/infParamsLLH.RData", "data/genParamsLLH.RData"), compression = "gzip", tar = "tar")
```

```{r distributions, fig.width = 7, fig.height = 4.5, fig.cap = caption, fig.pos = "ht"}
caption <- "A) The timeline of infection for an infector--infectee transmission pair.
The infector (index case) is initially infected at time $t_1$, and after a period of incubation develops symptoms at time $t_{S_1}$.
The infectee (secondary case) is infected by the infector at time $t_2$, which can be before (presymptomatic) or after (symptomatic) $t_{S_1}$.
The infectee then develops symptoms at time $t_{S_2}$.
The generation time is then defined as $t_2-t_1$ (the time between infections), while the serial interval is defined as $t_{S_2}-t_{S_1}$ (the time between symptom onsets).
B) The generation time distribution [$q(t|\\theta_q) = q(t_2-t_1|\\theta_q)$] follows a Weibull distribution \\citep{ferretti:medRxiv:2020}.
C) The infectivity profile [$p(t|\\theta_p) = p(t_2-t_{S_1}|\\theta_p)$] follows a shifted Student's \\emph{t}-distribution \\citep{ferretti:medRxiv:2020}.
D) The distribution of incubation times [$g(t) = g(t_{S_1}-t_1)$] follows a meta-distribution constructed from the mean of seven reported log-normal distributions as reported in \\citet{ferretti:medRxiv:2020} \\citep{bi:TheLancetInfectiousDiseases:2020,jiang:medRxiv:2020,lauer:AnnInternMed:2020,li:NEJM:2020,linton:J.Clin.Med.:2020,ma:medRxiv:2020,zhang:TheLancetInfectiousDiseases:2020}.
\\label{fig:distributions}"

#' Load the distributions
load("data/savedDistributions.RData")

#' Y-limits for consistent plotting
yLim <- c(0,0.35)
labY <- 0.212

#' Plot together
distPlots <- plot_grid(
  #' Generation time
  ggplot(genDistMLE, aes(x = t, y = pdf)) +
    geom_ribbon(aes(ymin = lower, ymax = upper), colour = "transparent", fill = "grey") +
    geom_vline(xintercept = genParams$mean, linetype = "dashed", colour = "darkgrey") +
    annotate("text", label = paste0(" mean = ", round(genParams$mean, 1), " days"), size = theme_get()$text[["size"]]/4, x = genParams$mean, y = labY, hjust = 0, vjust = 0) +
    geom_line() +
    coord_cartesian(xlim = c(0,15), ylim = yLim, expand = F) +
    labs(x = "generation time", y = "probability density") +
    ggtitle("generation time dist.") +
    plotTheme + theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5)),
  #' Infectivity profile
  ggplot(infProfMLE, aes(x = t, y = pdf)) +
    geom_ribbon(aes(ymin = lower, ymax = upper), colour = "transparent", fill = "grey") +
    geom_vline(xintercept = infParams$mean, linetype = "dashed", colour = "darkgrey") +
    annotate("text", label = paste0(" mean = ", round(infParams$mean, 1), " days"), size = theme_get()$text[["size"]]/4, x = infParams$mean, y = labY, hjust = 0, vjust = 0) +
    geom_line() +
    coord_cartesian(xlim = c(-10,15), ylim = yLim, expand = F) +
    labs(x = "days post symptoms", y = "probability density") +
    ggtitle("infectivity profile") +
    plotTheme + theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5)),
  #' Incubation period
  ggplot(incDist, aes(x = t, y = pdf)) +
    geom_vline(xintercept = incParams$mean, linetype = "dashed", colour = "darkgrey") +
    annotate("text", label = paste0(" mean = ", round(incParams$mean, 1), " days"), size = theme_get()$text[["size"]]/4, x = incParams$mean, y = labY, hjust = 0, vjust = 0) +
    geom_line() +
    coord_cartesian(xlim = c(0,20), ylim = yLim, expand = F) +
    labs(x = "incubation period (days)", y = "probability density") +
    ggtitle("incubation period") +
    plotTheme + theme(plot.title = element_text(size = plotTheme$text$size, hjust = 0.5)),
  
  nrow = 1, align = "hv", axis = "tb",
  #labels = "AUTO"
  labels = c("B","C","D")
)

plot_grid(
  ggdraw() + draw_image(magick::image_read_pdf("TTIQ-timeIntervals.pdf", density = 600), scale = 0.9),
  distPlots, ncol = 1, labels = c("A",""), rel_heights = c(1,1.4)
)
```

## Quantifying the number of secondary cases
Consider an index case who develops symptoms of COVID-19 at time $t_{S_1}$.
The time of infection, $t_1 < t_{S_1}$, is generally unknown.
Without any TTIQ intervention this individual would contact and infect $k_1$ individuals during the course of the infection, where this number of contacts is distributed as $p_{k_1}$ across individuals in the population.
Note that this number of contacts depends on the current level of other non-pharmaceutical interventions, such as mask wearing and social distancing.
The number of secondary infections up to a time $T$ after developing symptoms would then be
\begin{equation}
k_1 \int_{-\infty}^{T} {\rm d}t_2 \, p(t_2 - t_{S_1}|\theta_p) = k_1 P(T - t_{S_1}|\theta_p),
\end{equation}
where $p(t|\theta_p)$ is the infectivity profile and  $P(t|\theta_p) = \int_{-\infty}^t {\rm d}t' \, p(t'|\theta_p)$ is the cumulative infectivity profile.

Index cases who develop symptoms and/or test positive for SARS-CoV-2 should be isolated from the population.
This occurs in a fraction $f$ of index cases who are isolated at a time $T=t_{S_1}+\Delta_1$, where $\Delta_1 > 0$ is the delay between symptom onset and isolation.
The parameter $\Delta_1$ can be interpreted as the delay of taking a test after symptom onset, waiting for the result, and entering isolation, or alternatively as the delay between symptom onset and self-isolation.
The remaining $1-f$ index cases are not isolated ($T \to \infty$).
We can compute the number of secondary infections, $n_2$, as a function of testing coverage $f$ and delay $\Delta_1$, as shown in Fig. \ref{fig:flowchart-secondary}.
For a given symptom onset time $t_{S_1}$ and degree $k_1$ of the index case, we have
\begin{equation}
n_2(f,\Delta_1 | t_{S_1},k_1,\theta_p) = k_1 \left[f P(\Delta_1|\theta_p) + (1-f)\right].
\label{eq:secondaryCases}
\end{equation}
Averaging over $k_1$, which is distributed as $p_{k_1}$, and keeping $t_{S_1}$ fixed as the reference time point, we arrive at
\begin{equation}
n_2(f,\Delta_1 | t_{S_1},\theta_p) = R_e \left[f P(\Delta_1|\theta_p) + (1-f)\right],
\label{eq:secondaryCasesAvg}
\end{equation}
where $R_e = \langle k_1 \rangle$ is the mean of $p_{k_1}$, i.e. the average number of secondary infections in the absence of testing \& isolation ($f=0$).

```{r compute-secondary-cases, include = F}
#' These are fast to compute, so I won't parallelise anything here
getSeconaryCases <- function(paramList) {
  #' Load pre-computed distributions
  load("data/savedDistributions.RData")
  #' We want to vectorise everything in terms of Delta1
  indexDelta1 <- sapply(paramList$Delta1, function(Delta1) which.min(abs(times - Delta1)), USE.NAMES = F)
  #' Loop over Re values
  out.df <- lapply(paramList$Re, function(Re) {
    k <- Re
    #' Loop over f values
    lapply(paramList$f, function(f) {
      #' Compute cases
      secondaryCases <- k * (f * infProfMLE[indexDelta1, "CDF"] + (1 - f))
      data.frame(
        f = factor(f, levels = paramList$f),
        Delta1 = factor(paramList$Delta1, levels = paramList$Delta1),
        Re = factor(Re, levels = paramList$Re),
        cases = secondaryCases
      )
    }) %>% bind_rows()
  }) %>% bind_rows()
  return(out.df)
}

getSeconaryCasesCI <- function(paramList) {
  #' Load pre-computed distributions
  load("data/savedDistributions.RData")
  load("data/savedDistributionsLLH.RData")
  #' We want to vectorise everything in terms of Delta1
  indexDelta1 <- sapply(paramList$Delta1, function(Delta1) which.min(abs(times - Delta1)), USE.NAMES = F)
  #' Loop over Re values
  out.df <- lapply(paramList$Re, function(Re) {
    k <- Re
    #' Loop over f values
    lapply(paramList$f, function(f) {
      #' Compute cases
      secondaryCases <- sapply(seq_along(infProfLLH), function(id) {
        k * (f * infProfLLH[[id]][indexDelta1, "CDF"] + (1 - f))
      })
      #' Extract lower and upper bounds on number of secondary cases
      lower <- apply(secondaryCases, 1, min)
      upper <- apply(secondaryCases, 1, max)
      data.frame(
        f = factor(f, levels = paramList$f),
        Delta1 = factor(paramList$Delta1, levels = paramList$Delta1),
        Re = factor(Re, levels = paramList$Re),
        lower = lower,
        upper = upper
      )
    }) %>% bind_rows()
  }) %>% bind_rows()
  return(out.df)
}
```

```{r compute-summary, include = F}
#' Compute the threshold between controlled and uncontrolled epidemics (R0 <> 1)
#' This only applies to testing and isolating (i.e. functions of f and Delta1)
getSummary <- function(data, value) {
  #' Check data
  requiredNames <- c("f","Re","Delta1",value)
  if (!all(requiredNames %in% names(data))) {
    stop(paste0("Missing data columns: data must contain [", paste0(requiredNames, collapse = ", "), "]"))
  }
  
  #' Compute summary for each f, Re combination
  out.df <- by(data, INDICES = list(data$f, data$Re), function(df) {
    df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1]
    Tc <- -Inf
    if (all(df[[value]] < 1)) Tc <- Inf
    else if (any(df[[value]] < 1) & any(df[[value]] > 1)) Tc <- approx(x = df[[value]], y = df$Delta1, xout = 1)$y
    data.frame(
      f = factor(unique(df$f), levels = levels(data$f)),
      Re = factor(unique(df$Re), levels = levels(data$Re)),
      Delta1 = Tc
    )
  })
  #' Combine and return
  do.call(rbind, out.df)
}
```

## Quantifying the number of tertiary cases
Each secondary case has some potential to cause further infections, which will be the tertiary cases of the index case.
The number of tertiary infections caused by a secondary case who is infected at $t_2$ and isolated at time $T$, will be
\begin{equation}
k_2 \int_{t_2}^{T} {\rm d}t_3 \, q(t_3 - t_2|\theta_q) = k_2 Q(T - t_2|\theta_q),
\end{equation}
where $k_2$ is the number of contacts of the secondary case, $t_3$ is the infection time of the tertiary cases, $q(t|\theta_q)$ is the generation time distribution, and $Q(t|\theta_q) = \int_0^t {\rm d}t' \, q(t'|\theta_q)$ is the cumulative generation time distribution.
Note that we use the generation time distribution here, as our reference point is the time of infection ($t_2$), whereas in Eq. \eqref{eq:secondaryCasesAvg} the reference point was the time of symptom onset ($t_{S_1}$).

Under TTIQ interventions, the index and secondary cases can be isolated following a positive test result and/or self-isolation after symptom onset.
If an index case is confirmed positive, then contact tracing can be used to identify and quarantine individuals who have recently been exposed to the confirmed case.
Quarantining these individuals prevents the onward infection of tertiary cases (Fig. \ref{fig:TTIQ-schematic}B).
We introduce three further parameters to quantify contact tracing:
i) $\tau > 0$, the duration of lookback prior to symptom onset of the index case;
ii) $0 \le g \le 1$, the probability to identify and quarantine a secondary contact that was infected within the contact tracing window;
and iii) $\Delta_2 > 0$, the delay between isolating the index case and quarantining the identified secondary contacts.

There are many permutations of events that contribute to the number of tertiary cases under TTIQ, as shown in Fig. \ref{fig:flowchart-tertiary}.
The index case may not be detected ($1-f$), and hence contact tracing is not possible.
If the index case is detected ($f$), then a fraction $g$ of the secondary cases that were infected within the contact tracing window ($t_{S_1}-\tau \le t_2 \le t_{S_1}+\Delta_1$) are quarantined at time $t_{S_1}+\Delta_1+\Delta_2$ (Fig. \ref{fig:TTIQ-schematic}B).
The remaining fraction $1-g$, as well as the secondary cases that were infected outside of the contact tracing window ($t_2 < t_{S_1}-\tau$), are not quarantined.
However, the non-traced contacts may themselves be tested and become index cases that are isolated at time $t_{S_2}+\Delta_1$, where $t_{S_2}$ is the symptom onset time of the secondary case.
By considering these different scenarios, we arrive at an expression for the number of tertiary cases per index case under TTIQ,
\begin{equation}
\begin{aligned}
&n_3(f,\Delta_1,\tau,g,\Delta_2|t_{S_1},t_{S_2},k_1,k_2,\theta_p,\theta_q) = \\
&\quad
f g k_1 k_2 \int_{t_{S_1}-\tau}^{t_{S_1}+\Delta_1} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) Q(t_{S_1}+\Delta_1+\Delta_2-t_2|\theta_q) + \\
&\quad
f(1-g) k_1 k_2 \int_{t_{S_1}-\tau}^{t_{S_1}+\Delta_1} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) \left[f Q(t_{S_2}+\Delta_1-t_2|\theta_q) + (1-f)\right] + \\
&\quad
f k_1 k_2 \int_{-\infty}^{t_{S_1}-\tau} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) \left[f Q(t_{S_2}+\Delta_1-t_2|\theta_q) + (1-f)\right] + \\
&\quad
(1-f) k_1 k_2 \int_{-\infty}^{\infty} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) \left[f Q(t_{S_2}+\Delta_1-t_2|\theta_q) + (1-f)\right].
\end{aligned}
\label{eq:tertiaryCases}
\end{equation}
We now have to average Eq. \eqref{eq:tertiaryCases} over $t_{S_2}$, $k_1$, and $k_2$ to obtain the expected number of tertiary cases per index case under TTIQ.
We first note that $t_{S_2} = t_2 + \gamma$ for incubation period $\gamma \ge 0$. Hence we can write
\begin{equation}
\Bigl\langle Q(t_{S_2}+\Delta_1-t_2|\theta_q) \Bigr\rangle_{t_{S_2}} =
\int_0^\infty {\rm d}\gamma \, g(\gamma) Q(\gamma+\Delta_1|\theta_q) = J(\Delta_1|\theta_q),
\end{equation}
where $g(\gamma)$ is the incubation period distribution.
Note that we have assumed the independence between symptom onset and infectivity, which may lead to an overestimation of the fraction of tertiary cases prevented.
Keeping $t_{S_1}$ fixed as the reference time point, averaging Eq. \eqref{eq:tertiaryCases} over $t_{S_2}$, $k_1$, and $k_2$ gives the expected number of tertiary cases per index case under TTIQ:
\begin{equation}
\begin{aligned}
&n_3(f,\Delta_1,\tau,g,\Delta_2|t_{S_1},\theta_p,\theta_q) = \\
&\quad
f g \langle k_1 \rangle \langle k_2 \rangle \int_{-\tau}^{\Delta_1} {\rm d}t' \, p(t'|\theta_p) Q(\Delta_1+\Delta_2-t'|\theta_q) + \\
&\quad
f(1-g) \langle k_1 \rangle \langle k_2 \rangle \bigl[P(\Delta_1|\theta_p)-P(-\tau|\theta_p)\bigr] \bigl[f J(\Delta_1|\theta_q) + (1-f)\bigr] + \\
&\quad
f \langle k_1 \rangle \langle k_2 \rangle P(-\tau|\theta_p) \bigl[f J(\Delta_1|\theta_q) + (1-f)\bigr] + \\
&\quad
(1-f) \langle k_1 \rangle \langle k_2 \rangle \bigl[f J(\Delta_1|\theta_q) + (1-f)\bigr],
\end{aligned}
\label{eq:tertiaryCasesAvg}
\end{equation}
where we have substituted $t' = t_2-t_{S_1}$ such that
\begin{equation}
\int_{t_{S_1}-\tau}^{t_{S_1}+\Delta_1} {\rm d}t_2 \, p(t_2-t_{S_1}|\theta_p) Q(t_{S_1}+\Delta_1+\Delta_2-t_2|\theta_q) =
\int_{-\tau}^{\Delta_1} {\rm d}t' \, p(t'|\theta_p) Q(\Delta_1+\Delta_2-t'|\theta_q).
\end{equation}
Eq. \eqref{eq:tertiaryCasesAvg} can be further simplified to
\begin{equation}
\begin{aligned}
&n_3(f,\Delta_1,\tau,g,\Delta_2|t_{S_1},\theta_p,\theta_q) = \\
&\quad
f g R_e^2 \int_{-\tau}^{\Delta_1} {\rm d}t' \, p(t'|\theta_p) Q(\Delta_1+\Delta_2-t'|\theta_q) + \\
&\quad
R_e^2 \bigl[ f(1-g) P(\Delta_1|\theta_p) + f g P(-\tau|\theta_p) + (1-f) \bigr] \bigl[f J(\Delta_1|\theta_q) + (1-f)\bigr].
\end{aligned}
\label{eq:tertiaryCasesAvgSimp}
\end{equation}

```{r compute-tertiary-cases, include = F}
getTertiaryCases <- function(paramList, times, stepSize, infProf.df, genDist.df, integral.df) {
  #' Compute functions and integrals for each (Delta_1, tau, Delta_2) combination
  functions <- lapply(paramList$Delta1, function(Delta1) {
    index_Delta1 <- which.min(abs(times - Delta1))
    P_Delta1 <- infProf.df[[index_Delta1, "CDF"]]
    J_Delta1 <- integral.df[[index_Delta1, "J"]]
    
    lapply(paramList$tau, function(tau) {
      index_minusTau <- which.min(abs(times + tau))
      P_minusTau <- infProf.df[[index_minusTau, "CDF"]]
      P_diff <- P_Delta1 - P_minusTau
      
      lapply(paramList$Delta2, function(Delta2) {
        integral <- 0
        if(any(paramList$g > 0)) {
          p <- infProf.df[seq(index_minusTau,index_Delta1),"pdf"]
          index_lower <- which.min(abs(times - (Delta1 + Delta2 + tau)))
          index_upper <- which.min(abs(times - Delta2))
          Q <- genDist.df[seq(index_lower,index_upper),"CDF"]
          integral <- sum(p * Q) * stepSize
        }
        data.frame(
          Delta1 = factor(Delta1, levels = paramList$Delta1),
          tau = factor(tau, levels = paramList$tau),
          Delta2 = factor(Delta2, levels = paramList$Delta2),
          P_Delta1 = P_Delta1,
          P_minusTau = P_minusTau,
          P_diff = P_diff,
          J_Delta1 = J_Delta1,
          integral = integral
        )
      }) %>%bind_rows()
    }) %>%bind_rows()
  }) %>%bind_rows()
  
  #' Finally loop over the values of (f,g,Re) and sum up all the cases per parameter set
  paramDF <- expand.grid(Re = paramList$Re, f = paramList$f, g = paramList$g, KEEP.OUT.ATTRS = F)
  out.df <- lapply(seq_len(nrow(paramDF)), function(i) {
    Re <- paramDF[[i,"Re"]]
    f <- paramDF[[i,"f"]]
    g <- paramDF[[i,"g"]]
    
    k1 <- Re
    k2 <- Re
    F_f_Delta1 <- f * functions$J_Delta1 + (1 - f)
    
    tertiaryCases_detected <- f * g * k1 * k2 * functions$integral
    tertiaryCases_undetected <- f * (1 - g) * k1 * k2 * functions$P_diff * F_f_Delta1
    tertiaryCases_outside <- f * k1 * k2 * functions$P_minusTau * F_f_Delta1
    tertiaryCases_indexUndetected <- (1 - f) * k1 * k2 * F_f_Delta1
    tertiaryCases <- tertiaryCases_detected + tertiaryCases_undetected +
        tertiaryCases_outside + tertiaryCases_indexUndetected
    data.frame(
      Re = factor(Re, levels = paramList$Re),
      f = factor(f, levels = paramList$f),
      g = factor(g, levels = paramList$g),
      Delta1 = functions$Delta1,
      tau = functions$tau,
      Delta2 = functions$Delta2,
      tertiaryCases_detected = tertiaryCases_detected,
      tertiaryCases_undetected = tertiaryCases_undetected,
      tertiaryCases_outside = tertiaryCases_outside,
      tertiaryCases_indexUndetected = tertiaryCases_indexUndetected,
      cases = tertiaryCases
    )
  }) %>% bind_rows()
  return(out.df)
}
```

Finally, in the absence of contact tracing ($g=0$), the number of tertiary cases under testing & isolation only is given by
\begin{equation}
n_3(f,\Delta_1|t_{S_1},\theta_p,\theta_q) = R_e^2 \bigl[f P(\Delta_1|\theta_p) + (1-f)\bigr] \bigl[f J(\Delta_1|\theta_q) + (1-f)\bigr].
\label{eq:tertiaryCases-noTracingAvg}
\end{equation}

```{r compute-tertiary-cases-no-tracing, include = F}
#' To enable parallelisation, we pass everything as arguments
getTertiaryCasesNoTracing <- function(paramList, times, infProf.df, integral.df) {
  #' We want to vectorise everything in terms of Delta1
  indexDelta1 <- sapply(paramList$Delta1, function(Delta1) which.min(abs(times - Delta1)), USE.NAMES = F)
  #' Loop over Re values
  out.df <- lapply(paramList$Re, function(Re) {
    k1 <- Re
    k2 <- Re
    #' Loop over f values
    lapply(paramList$f, function(f) {
      #' Compute cases
      tertiaryCases <- k1 * k2 * (f * infProf.df[indexDelta1, "CDF"] + (1 - f)) * (f * integral.df[indexDelta1, "J"] + (1 - f))
      data.frame(
        f = factor(f, levels = paramList$f),
        Delta1 = factor(paramList$Delta1, levels = paramList$Delta1),
        Re = factor(Re, levels = paramList$Re),
        cases = tertiaryCases
      )
    }) %>% bind_rows()
  }) %>% bind_rows()
  return(out.df)
}
```

## Confidence intervals
The primary sources of uncertainty in the outcomes of this model come from the generation time distribution and infectivity profile, which are inferred from empirical serial interval distributions \citep{ferretti:medRxiv:2020}.
Following \citet{ferretti:medRxiv:2020}, we use a likelihood ratio test to extract sample parameter sets for each distribution that lie within the 95\% confidence interval.

Concretely, we first identify the maximum likelihood parameter sets $\hat{\theta}_p$ and $\hat{\theta}_q$ for the infectivity profile and generation time distribution, respectively.
We then randomly sample the parameter space of each distribution, and keep 1,000 parameter sets whose likelihood satisfies $\ln \mathcal{L}(\theta) > \ln \mathcal{L}(\hat{\theta}) - \lambda_n/2$, where $\lambda_n$ is the 95\% quantile of a $\chi^2$ distribution with $n$ degrees of freedom.
The infectivity profile is described a shifted Student's \emph{t}-distribution, which has $n=3$ parameters, while the generation time is described by a Weibull distribution with $n=2$ parameters.

We then use these sampled parameter sets to generate the number of secondary and tertiary cases, and the extrema of cases across all of these parameter sets determines the 95\% confidence interval for the number of cases.
For the estimate of the number of secondary cases under testing \& isolation [Eq. \eqref{eq:secondaryCasesAvg}], we only have to consider the uncertainty of the parameters of the infectivity profile $\theta_p$.
Under the full TTIQ strategy, we need to use estimates of both $\theta_p$ and $\theta_q$.
We assume parameter independence, and keep all $(\theta_p,\theta_q)$ combinations whose joint likelihood satisfies $\ln \mathcal{L}(\theta_p) + \ln \mathcal{L}(\theta_q) > \ln \mathcal{L}(\hat{\theta}_p) + \ln \mathcal{L}(\hat{\theta}_q) - \lambda_5/2$.

## Interactive app
To complement the results in this manuscript, and to allow readers to investigate different TTIQ parameter settings, we have developed an online interactive application.
This can be found on the \emph{CH Covid-19 Dashboard} at \url{https://ibz-shiny.ethz.ch/covidDashboard/}.

# Results
## Reducing cases by testing & isolating
The efficacy of testing \& isolating is determined by two parameters: the probability $f$ to find and isolate an infected individual; and the time delay $\Delta_1$ between symptom onset and isolation of the index case.
The expected number of secondary or tertiary cases [Eqs. \eqref{eq:secondaryCasesAvg} & \eqref{eq:tertiaryCases-noTracingAvg}] is also dependent on the current intensity of the epidemic, $R_e$, which is the expected number of secondary cases per infected in the absence of testing \& isolating ($f=0$).
This effective reproduction number depends on the current suppression measures against SARS-CoV-2 transmission (social distancing, mask wearing, home office, etc.), as well as seasonality and levels of immunity/vaccination.

Epidemics can be controlled by testing \& isolating if this intervention reduces the expected number of secondary or tertiary cases per index case to below one.
We here focus on the number of tertiary cases, but results for the number of secondary cases are qualitatively equivalent (Fig. \ref{fig:secondaryCases}).

The region of $(f,\Delta_1)$ parameter space in which the number of tertiary cases is less than one, i.e. the region in which the epidemic is controlled by testing \& isolating, is shrinking for higher $R_e$ epidemics (Fig. \ref{fig:tertiaryCases-noTracing}A).
Higher testing \& isolation coverage ($f$) or shortened delays between symptom onset and isolation ($\Delta_1$) are required to control SARS-CoV-2 outbreaks as $R_e$ increases.
Increasing the fraction of infecteds that are isolated buys more time to isolate them, but with diminishing returns.

```{r tertiaryCases-noTracing-line, include = F}
#' Tertiary cases as a function of f and Delta1
paramList <- list(
  f = seq(0,1,0.05),
  Delta1 = seq(0,4,1),
  Re = seq(1.1, 1.5, 0.1)
)
load("data/savedDistributions.RData")
tertiaryCasesMLE <- getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                              infProf.df = infProfMLE, integral.df = integralMLE)

Re.plotVals <- c("1.1","1.3","1.5")
Delta1.plotVals <- c("0","2","4")

#' Prepare and plot
df <- tertiaryCasesMLE
df$f <- as.numeric(levels(df$f))[df$f]
tertiaryCasesPlot <- ggplot(df[df$Re %in% Re.plotVals & df$Delta1 %in% Delta1.plotVals,], aes(x = f, y = cases, colour = Delta1, group = Delta1)) +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  geom_line(size = lineSize) +
  facet_grid(~Re) +
  scale_color_manual(values = colorRampPalette(brewer.pal(9, "Oranges"))(12)[c(5,8,10)],
                     name = expression("delay" ~ (Delta[1])),
                     aesthetics = c("colour","fill"),
                     labels = dayLabels(levels(df$Delta1))) +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0.45,2.55), expand = F) +
  labs(x = expression("fraction of index cases found and isolated" ~ (f)), y = expression("tertiary cases" ~ (n[3]))) +
  plotTheme + theme(legend.position = "right", panel.spacing = unit(1.5, "lines"), strip.background = element_blank(), strip.text = element_blank())

#' Do we add CI to the plot?
addCI <- T
if (addCI) {
  out.filename <- "data/tertiaryCases-noTracing-lines-CI.RData"
  if (file.exists(out.filename)) {
    #' Load the data, format, and add layer to plot
    load(out.filename)
    tertiaryCasesCI <- out.df
    tertiaryCasesCI$f <- as.numeric(levels(tertiaryCasesCI$f))[tertiaryCasesCI$f]
    df <- merge(df,tertiaryCasesCI)
    tertiaryCasesPlot$layers <- c(
      geom_ribbon(data = df[df$Re %in% Re.plotVals & df$Delta1 %in% Delta1.plotVals,], aes(ymin = lower, ymax = upper, fill = Delta1), alpha = 0.4, colour = "transparent"),
      tertiaryCasesPlot$layers)
  } else {
    print(paste("File", out.filename, "is missing."))
    print("Pass the following info file to contactTracing-CIscript.R to generate confidence intervals, and then try again.")
    #' Create file "info-for-script_xx.RData" here
    #' First we create the function which accepts id and idGD and returns a
    #' dataframe for the number of cases for the given parameter set
    func <- function(id, idGD) {
      getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                infProf.df = infProfLLH[[id]], integral.df = integralLLH[[idGD]])
    }
    #' Now add the function(s), parameter list, and the output filename
    info.filename <- "info-for-script_1.RData"
    save(func, getTertiaryCasesNoTracing, paramList, out.filename, file = info.filename)
    print(paste("Info file is", info.filename))
    print(paste("The output file", out.filename, "can be generated by executing:"))
    #' R --vanilla --slave < contactTracing-CIscript.R --args info-for-script_xx.RData run
    print(paste("R --vanilla --slave < contactTracing-CIscript.R --args", info.filename, "run"))
    addCI <- FALSE
  }
}

# #' Read values
# dat <- df[df$Re == "1.1" & df$Delta1 == "0",]
# sapply(c("cases","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$Re == "1.1" & df$Delta1 == "2",]
# sapply(c("cases","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$Re == "1.1" & df$Delta1 == "4",]
# sapply(c("cases","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$Re == "1.5" & df$Delta1 == "0",]
# sapply(c("cases","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
# dat <- df[df$Re == "1.5" & df$Delta1 == "2",]
# sapply(c("cases","lower","upper"), function(var) round(approx(x = dat[[var]], y = dat$f, xout = 1)$y, digits = 2))
```

```{r tertiaryCases-noTracing-density, include = F}
#' Critical region where epidemic is controlled
paramList <- list(
  f = seq(0,1,0.05),
  Delta1 = seq(0,6,0.2),
  Re = seq(1.1, 1.5, 0.1)
)
load("data/savedDistributions.RData")
tertiaryCasesMLE <- getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                              infProf.df = infProfMLE, integral.df = integralMLE)

Re.plotVals <- c("1.1","1.3","1.5")

#' Prepare and plot
df <- tertiaryCasesMLE
df$f <- as.numeric(levels(df$f))[df$f]
df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1]
df$Re <- as.character(df$Re)

cases.min <- 0.25
cases.max <- 2.25

tertiarySummaryPlot <- ggplot(df[df$Re %in% Re.plotVals,], aes(x = f, y = Delta1, group = Re)) +
  geom_raster(aes(fill = cases), interpolate = T) +
  geom_contour(aes(z = cases), colour = "black", size = lineSize, breaks = 1) +
  scale_fill_gradientn(colours = c("seagreen","white","white","white","salmon"),
                       name = "tertiary\ncases",
                       values = scales::rescale(c(cases.min,1-.Machine$double.eps,1,1+.Machine$double.eps,cases.max)),
                       limits = c(cases.min,cases.max)) +
  facet_grid(~Re, labeller = label_bquote(cols = R[e]~"="~.(Re))) +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0, 4), expand = F) +
  labs(x = expression("fraction of index cases found and isolated" ~ (f)),
       y = expression(atop("delay to isolation" ~ (Delta[1]),"(days after symptoms)"))) +
  plotTheme +
  theme(legend.position = "right", panel.spacing = unit(1.5, "lines"))

#' Do we add CI to the plot?
addCI <- T
if (addCI) {
  out.filename <- "data/tertiaryCases-noTracing-density-CI.RData"
  if (file.exists(out.filename)) {
    #' Load the data, format, and add layer to plot
    load(out.filename)
    tertiaryCasesCI <- out.df
    tertiaryCasesCI$f <- as.numeric(levels(tertiaryCasesCI$f))[tertiaryCasesCI$f]
    df <- merge(df,tertiaryCasesCI)
    tertiarySummaryPlot <- tertiarySummaryPlot +
      geom_contour(data = df[df$Re %in% Re.plotVals,], aes(z = lower), colour = "black", linetype = "dashed", breaks = 1) +
      geom_contour(data = df[df$Re %in% Re.plotVals,], aes(z = upper), colour = "black", linetype = "dashed", breaks = 1)
  } else {
    print(paste("File", out.filename, "is missing."))
    print("Pass the following info file to contactTracing-CIscript.R to generate confidence intervals, and then try again.")
    #' Create file "info-for-script_xx.RData" here
    #' First we create the function which accepts id and idGD and returns a
    #' dataframe for the number of cases for the given parameter set
    func <- function(id, idGD) {
      getTertiaryCasesNoTracing(paramList = paramList, times = times,
                                infProf.df = infProfLLH[[id]], integral.df = integralLLH[[idGD]])
    }
    #' Now add the function(s), parameter list, and the output filename
    info.filename <- "info-for-script_3.RData"
    save(func, getTertiaryCasesNoTracing, paramList, out.filename, file = info.filename)
    print(paste("Info file is", info.filename))
    print(paste("The output file", out.filename, "can be generated by executing:"))
    #' R --vanilla --slave < contactTracing-CIscript.R --args info-for-script_xx.RData run
    print(paste("R --vanilla --slave < contactTracing-CIscript.R --args", info.filename, "run"))
    addCI <- FALSE
  }
}
```

```{r tertiaryCases-noTracing, dependson = c(-1,-2,-3), fig.height = 6, fig.width = 10, fig.cap = caption, fig.pos = "ht"}
caption <- "A) The impact of testing \\& isolation on the number of tertiary cases per index case, $n_3$, as a function of the testing coverage $f$ (x-axis) and delay to isolation after symptom onset $\\Delta_1$ (y-axis) for different $R_e$ values (columns) [Eq. \\eqref{eq:tertiaryCases-noTracingAvg}].
The black line shows $n_3 = 1$.
Above this line (red zone) we have $n_3>1$ and the epidemic is growing.
Below this line we have $n_3<1$ and the epidemic is suppressed.
Dashed lines are the 95\\% confidence interval for this threshold.
B) Lines correspond to slices of panel A at fixed delay $\\Delta_1 =$ 0, 2, or 4 days (colour).
Shaded regions are 95\\% confidence intervals for the number of tertiary cases per index case.
Horizontal grey line is the threshold for epidemic control ($n_3=1$). 
\\label{fig:tertiaryCases-noTracing}"

#' Combine plots
plots <- list(tertiaryCasesPlot, tertiarySummaryPlot)
legends <- list(get_legend(plots[[1]]), get_legend(plots[[2]]))
plots <- lapply(plots, function(plot) plot + theme(legend.position = "none"))

plot_grid(
  plot_grid(plotlist = list(plots[[2]],plots[[1]]), ncol = 1, axis = "lr", align = "hv", rel_heights = c(1,1), labels = "AUTO"),
  plot_grid(plotlist = list(legends[[2]],legends[[1]]), ncol = 1, rel_heights = c(1,1)),
  rel_widths = c(1,0.1), ncol = 2
)
```

A SARS-CoV-2 outbreak with $R_e = 1.1$ can be controlled by isolating as few as 18\% [95\% confidence interval (CI): 15\%,22\%] of infected cases at the time of symptom onset ($\Delta_1=0$ days) (Fig. \ref{fig:tertiaryCases-noTracing}B).
If the infected index or secondary cases wait $\Delta_1=2$ days after symptom onset before isolating (i.e. they wait for a test result), then 39\% [CI: 28\%,60\%] of infecteds would have to be isolated for the epidemic to be controlled.
Isolating after $\Delta_1=4$ days would be insufficient to control the epidemic even if all cases were isolated [CI: 65\%,n.a.].
For faster-spreading SARS-CoV-2 outbreaks ($R_e = 1.5$), we would require 65\% [CI: 55\%,81\%] of infecteds to be isolated immediately after they develop symptoms ($\Delta_1=0$ days) to control the epidemic.
With a delay $\Delta_1 \ge 2$ days, testing & isolating would be insufficient to control the epidemic even if 100\% of infecteds are isolated.
We note that the frequency of asymptomatic cases (20\%; \citet{buitrago-garcia:PLOSMedicine:2020}) means that we would not be able to isolate 100\% of infecteds if we wait for symptoms to develop.

## Reducing cases by additional contact tracing & quarantine
The efficacy of tracing \& quarantine is determined by three further parameters: the duration of the contact tracing window prior to symptom onset in the index case $\tau$; the probability to identify and quarantine a secondary case that was infected by an index case within the contact tracing window $g$; and the delay between isolating the index case and quarantining the secondary cases $\Delta_2$.
The expected number of tertiary cases [Eq. \eqref{eq:tertiaryCasesAvg}] is also dependent on the intensity of the epidemic in the absence of TTIQ, $R_e$, as well as the probability ($f$) and delay ($\Delta_1$) of finding and isolating an index case.

The impact that contact tracing has on epidemic control can be seen by varying the parameter $g$.
For $g=0$, no contacts are traced \& quarantined, and hence we return to the testing \& isolation strategy (Fig. \ref{fig:tertiaryCases-noTracing}).
By increasing $g$, we expand the parameter space in which $n_3<1$ (Fig. \ref{fig:tertiaryCases-CT}), i.e. contract tracing allows an epidemic to be controlled for lower fractions of index cases found ($f$) and/or longer delays to isolating the index case after they develop symptoms ($\Delta_1$).

```{r tertiaryCases-CT-benefit, fig.height = 3, fig.width = 10, fig.cap = caption, fig.pos = "ht"}
caption <- "The impact of tracing \\& quarantine on the number of tertiary cases per index case, $n_3$, as a function of the testing coverage $f$ (x-axis) and delay to isolation after symptom onset $\\Delta_1$ (y-axis), for different contact tracing success probabilities $g$ (colour) across different $R_e$ values (columns) [Eq. \\eqref{eq:tertiaryCasesAvg}].
We fix $\\Delta_2 = 2$ days and $\\tau = 2$ days.
The contours divide the regions where $n_3>1$ (the epidemic is growing) and $n_3<1$ (the epidemic is suppressed).
The contours for $g=0$ are equivalent to the contours in Fig. \\ref{fig:tertiaryCases-noTracing}.
We do not show confidence intervals for clarity of presentation.
\\label{fig:tertiaryCases-CT}"

paramList <- list(
  f = seq(0,1,0.05),
  g = seq(0,1,0.2),
  Delta1 = seq(0,4,0.2),
  Delta2 = 2,
  tau = 2,
  Re = c(1.1,1.3,1.5)
)
load("data/savedDistributions.RData")
tertiaryCasesMLE <-
  getTertiaryCases(paramList = paramList, times = times, stepSize = stepSize, infProf.df = infProfMLE,
                   genDist.df = genDistMLE, integral.df = integralMLE)

#' Prepare and plot
df <- tertiaryCasesMLE
df$f <- as.numeric(levels(df$f))[df$f]
df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1]
df$Re <- as.character(df$Re)

ggplot(df, aes(x = f, y = Delta1, group = g)) +
  geom_contour(aes(colour = g, z = cases), size = lineSize, breaks = 1) +
  facet_grid(~Re, labeller = label_bquote(cols = R[e]~"="~.(Re))) +
  scale_colour_viridis_d(end = 0.9, labels = scales::percent(as.numeric(levels(df$g))),
                         name = "fraction of\ncontacts\nquarantined (g)") +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0, 4), expand = F) +
  labs(x = expression("fraction of index cases found and isolated" ~ (f)),
       y = expression(atop("delay to isolation" ~ (Delta[1]),"(days after symptoms)"))) +
  plotTheme +
  theme(legend.position = "right", panel.spacing = unit(1.5, "lines")) +
  annotate("text", label = expression(n[3]~"> 1"), size = theme_get()$text[["size"]]/4,
           x = 0.01, y = 3.8,
           hjust = 0, vjust = 0) +
  annotate("text", label = expression(n[3]~"< 1"), size = theme_get()$text[["size"]]/4,
           x = 0.88, y = 0.05,
           hjust = 0, vjust = 0)
```

To visualise the impact of each parameter on the number of tertiary cases, we consider focal parameter sets for the five TTIQ parameters, $(f,g,\Delta_1,\Delta_2,\tau)$.
We then calculate the expected number of tertiary cases when we perturb each single parameter, keeping the remaining four parameters fixed (Fig. \ref{fig:tertiaryCases-single}).

```{r tertiaryCases, fig.height = 8, fig.width = 10, fig.cap = caption, fig.pos = "ht"}
caption <- "The number of tertiary cases per index case in the presence of TTIQ interventions.
We set $R_e = 1.5$ throughout, which is the intensity of the epidemic in the absence of TTIQ.
We consider four focal TTIQ parameter combinations, with $f \\in \\{0.3,0.7\\}$, $\\Delta_1 \\in \\{0,2\\}$ days, $g = 0.5$, $\\Delta_2 = 1$ day, and $\\tau = 2$ days.
The number of tertiary cases for the focal parameter sets are shown as thin black lines.
With $f=0$ (no TTIQ) we expect $R_e^2$ tertiary cases (upper grey line).
We then vary each TTIQ parameter individually, keeping the remaining four parameters fixed at the focal values.
The upper panel shows the probability parameters $f$ and $g$, while the lower panel shows the parameters which carry units of time (days).
The critical threshold for controlling an epidemic is one tertiary case per index case (lower grey line).
\\label{fig:tertiaryCases-single}"

#' Function to perturb a single parameter and compute tertiary cases
perturbParam <- function(paramList, param, values, addCI = F, index = NULL) {
  load("data/savedDistributions.RData")
  #' Perturb values
  paramList[[param]] <- values
  tertiaryCases <- getTertiaryCases(paramList = paramList, times = times,
                                    stepSize = stepSize, infProf.df = infProfMLE,
                                    genDist.df = genDistMLE, integral.df = integralMLE)
  df <- data.frame(
    param = factor(param, levels = c("f", "g", "Delta1", "Delta2", "tau")),
    value = values,
    cases = tertiaryCases$cases
  )
  
  #' Now add confidence intervals
  if (addCI) {
    out.filename <- paste0("data/tertiaryCases-single-CI_", param, "_", index, ".RData")
    if (file.exists(out.filename)) {
      #' Load files and add lower and upper to df
      load(out.filename)
      df$lower <- out.df$lower
      df$upper <- out.df$upper
    } else {
      #' Need to create file
      print(paste("File", out.filename, "is missing."))
      print("Pass the following info file to contactTracing-CIscript.R to generate confidence intervals, and then try again.")
      #' Create file "info-for-script_xx.RData" here
      #' First we create the function which accepts id and idGD and returns a
      #' dataframe for the number of cases for the given parameter set
      func <- function(id, idGD) {
        getTertiaryCases(paramList = paramList, times = times,
                         stepSize = stepSize, infProf.df = infProfLLH[[id]],
                         genDist.df = genDistLLH[[idGD]], integral.df = integralLLH[[idGD]])
      }
      #' Now add the function(s), parameter list, and the output filename
      info.filename <- paste0("info-for-script_", param, "_", index,".RData")
      save(func, getTertiaryCases, paramList, out.filename, file = info.filename)
      print(paste("Info file is", info.filename))
      print(paste("The output file", out.filename, "can be generated by executing:"))
      #' R --vanilla --slave < contactTracing-CIscript.R --args info-for-script_xx.RData run
      print(paste("R --vanilla --slave < contactTracing-CIscript.R --args", info.filename, "run"))
      addCI <- FALSE
    }
  }
  if (!addCI) {
    df$lower <- 1
    df$upper <- 1
  }
  return(df)
}

#' Function to compute all data
getData <- function(focalParams, addCI = F, index = NULL) {
  #' Number of cases under focal parameters
  load("data/savedDistributions.RData")
  focal <- getTertiaryCases(paramList = focalParams, times = times,
                            stepSize = stepSize, infProf.df = infProfMLE,
                            genDist.df = genDistMLE, integral.df = integralMLE)
  #' Perturb parameters and compute cases
  df <- rbind(
    perturbParam(paramList = focalParams, param = "f", values = seq(0,1,0.05), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "g", values = seq(0,1,0.05), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "Delta1", values = seq(0,5,0.25), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "Delta2", values = seq(0,5,0.25), addCI = addCI, index = index),
    perturbParam(paramList = focalParams, param = "tau", values = seq(0,5,0.25), addCI = addCI, index = index)
  )
  df$index <- factor(index)
  focal$index <- factor(index)
  return(list(focal = focal, perturbed = df))
}

#' Now compute data
focalParams.list <- list(
  list(f = 0.3, g = 0.5, Delta1 = 2, Delta2 = 1, tau = 2, Re = 1.5),
  list(f = 0.3, g = 0.5, Delta1 = 0, Delta2 = 1, tau = 2, Re = 1.5),
  list(f = 0.7, g = 0.5, Delta1 = 2, Delta2 = 1, tau = 2, Re = 1.5),
  list(f = 0.7, g = 0.5, Delta1 = 0, Delta2 = 1, tau = 2, Re = 1.5)
)
allData <- lapply(seq_along(focalParams.list), function(index) {
  getData(focalParams = focalParams.list[[index]], addCI = T, index = index)
})
focalCases <- lapply(allData, function(data) data$focal) %>% bind_rows()
focalCases$Re <- as.numeric(levels(focalCases$Re))[focalCases$Re]
perturbedCases <- lapply(allData, function(data) data$perturbed) %>% bind_rows()

#' Plot
yLim <- c(0.4,2.5)
plot_labeller <- function(variable, value) {
  labs <- lapply(value, function(index) {
    index <- as.numeric(levels(value))[index]
    lett <- LETTERS[index]
    f <- focalParams.list[[index]]$f
    Delta1 <- focalParams.list[[index]]$Delta1
    bquote(bold(.(lett))~~"f ="~.(f)*","~Delta[1]~"="~.(Delta1)~"days")
  })
  return(labs)
}
myColours <- c(f = palette_OkabeIto[1], g = palette_OkabeIto[4],
               Delta1 = palette_OkabeIto[2], Delta2 = palette_OkabeIto[5],
               tau = palette_OkabeIto[3])

probPlot <- ggplot(perturbedCases[perturbedCases$param %in% c("f","g"),],
                   aes(x = value, y = cases, colour = param)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = param),
              alpha = 0.4, colour = "transparent") +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = Re^2),
             colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = cases),
             colour = "black") +
  geom_line(size = lineSize) + 
  facet_grid(~index, labeller = plot_labeller) +
  scale_colour_manual(values = myColours,
                      aesthetics = c("colour","fill"), name = NULL) +
  coord_cartesian(xlim = c(0,1), ylim = yLim, expand = F) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "probability", y = expression("tertiary cases" ~ (n[3]))) +
  plotTheme + theme(plot.margin = unit(c(1.1,0.2,0,0), "lines"),
                    legend.position = "right",#c(0.95,0.7),
                    legend.text = element_text(size = 14),
                    legend.key.size = unit(1.5,"line"),
                    strip.text = element_text(hjust = 0, size = 14),
                    panel.spacing = unit(1.5,"lines"))

myLabs <- expression(Delta[1],Delta[2],tau)
names(myLabs) <- c("Delta1","Delta2","tau")
timePlot <- ggplot(perturbedCases[perturbedCases$param %in% c("Delta1","Delta2","tau"),],
                   aes(x = value, y = cases, colour = param)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = param),
              alpha = 0.4, colour = "transparent") +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = Re^2),
             colour = "darkgrey", size = lineSize) +
  geom_hline(data = focalCases, aes(yintercept = cases),
             colour = "black") +
  geom_line(size = lineSize) +
  facet_grid(~index) +
  scale_colour_manual(values = myColours,
                      aesthetics = c("colour","fill"), name = NULL, labels = myLabs) +
  coord_cartesian(xlim = c(0,5), ylim = yLim, expand = F) +
  labs(x = "time (days)", y = expression("tertiary cases" ~ (n[3]))) +
  plotTheme + theme(plot.margin = unit(c(1,0.2,0,0), "lines"),
                    legend.position = "right",#c(0.82,0.7),
                    legend.text = element_text(size = 14, hjust = 0.5),
                    legend.key.size = unit(1.5,"line"),
                    strip.background = element_blank(),
                    strip.text = element_blank(),
                    panel.spacing = unit(1.5,"lines"))

#' Extract legend and then plot together
probPlot.legend <- get_legend(probPlot)
timePlot.legend <- get_legend(timePlot)
probPlot <- probPlot + theme(legend.position = "none")
timePlot <- timePlot + theme(legend.position = "none")

plot_grid(
  plot_grid(probPlot, timePlot, nrow = 2),
  plot_grid(probPlot.legend, timePlot.legend, nrow = 2),
  nrow = 1, rel_widths = c(1,0.08)
)
```

Modifying the fraction of index cases that are identified and isolated ($f$) has the largest effect of all parameter changes.
By identifying more index cases (increasing $f$), we not only prevent the onward transmission to new secondary cases through isolation, but we also allow infected contacts to be traced and quarantined.

Increasing the fraction of secondary cases that are quarantined ($g$) has a smaller return than increasing $f$.
If only 30\% of index cases are identified, then increasing $g$ results in a small reduction of the number of tertiary cases and for $R_e=1.5$ the epidemic cannot be controlled even if all secondary cases ($g=1$) of known index cases are quarantined (Figs. \ref{fig:tertiaryCases-single}A & B).
However, if a large fraction of index cases are identified ($f=0.7$), then increasing $g$ can control an epidemic that would be out of control in the absence of contact tracing (Figs. \ref{fig:tertiaryCases-single}C & D).

After increasing $f$, the next most effective control strategy is to reduce the delay between symptom onset and isolation of the index case ($\Delta_1$).
Reducing the time taken to quarantine secondary cases has a lesser effect on the total number of tertiary cases.
Finally, looking back further while contact tracing (increasing $\tau$) allows more secondary cases to be traced & quarantined.
However, this does not translate into a substantial reduction in the number of tertiary cases as the extra cases which are traced have already been infectious for a long time, and will thus have less remaining infectivity potential.
Hence increasing $\tau$ comes with diminishing returns.

To check the robustness of these effects across all parameter combinations (not just varying a single parameter), we performed uniform parameter sampling and used linear discriminant analysis (LDA) to capture the impact that each parameter has on the number of tertiary cases (Fig. \ref{fig:LDA-1D}).
We find that $f$ is the dominant parameter to determine the number of tertiary cases, followed by $\Delta_1$, $g$, $\Delta_2$, and finally $\tau$ has the smallest impact (Fig. \ref{fig:LDA-1D}B).

```{r LDA-plot-1D, warning=F, message=F, fig.height = 5, fig.width = 8, fig.cap = caption, fig.pos = "ht"}
caption <- "A) Linear discriminant analysis (LDA) of the impact of TTIQ strategies on the number of tertiary cases.
We fix $R_e=1.5$ and then uniformly sample parameter combinations from $f \\in [0,1]$, $g \\in [0,1]$, $\\Delta_1 \\in [0,5]$ days, $\\Delta_2 \\in [0,5]$ days, and $\\tau \\in [0,5]$ days.
The number of tertiary cases is calculated [Eq. \\eqref{eq:tertiaryCasesAvg}] for each parameter combination, and the output ($n_3$) is categorised into bins of width $0.2$ (colour).
We then use LDA to construct a linear combination (LD1) of the five (normalised) TTIQ parameters which maximally separates the output categories.
We then predict the LD1 values for each paramter combination, and construct a histogram of these values for each category.
B) The components of the LD1 vector.
By multiplying the (normalised) TTIQ parameters by the corresponding vector component, we arrive at the LD1 prediction which corresponds with the number of tertiary cases under that TTIQ strategy.
Longer arrows (larger magnitude components) correspond to a parameter having a larger effect on the output.
\\label{fig:LDA-1D}"

#' Uniform param sampling
paramList <- list(
  f = seq(0, 1, length.out = 6),
  g = seq(0, 1, length.out = 6),
  Delta1 = seq(0, 5, length.out = 6),
  Delta2 = seq(0, 5, length.out = 6),
  tau = seq(0, 5, length.out = 6),
  Re = 1.5
)
#' Compute tertiary cases
load("data/savedDistributions.RData")
tertiaryCases <- getTertiaryCases(paramList = paramList, times = times, stepSize = stepSize,
                                  infProf.df = infProfMLE, genDist.df = genDistMLE, integral.df = integralMLE)
df <- tertiaryCases
#' Enumerate and normalise param values
df$f <- as.numeric(levels(df$f))[df$f] / (max(paramList$f) - min(paramList$f))
df$g <- as.numeric(levels(df$g))[df$g] / (max(paramList$g) - min(paramList$g))
df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1] / (max(paramList$Delta1) - min(paramList$Delta1))
df$Delta2 <- as.numeric(levels(df$Delta2))[df$Delta2] / (max(paramList$Delta2) - min(paramList$Delta2))
df$tau <- as.numeric(levels(df$tau))[df$tau] / (max(paramList$tau) - min(paramList$tau))
df <- df[,c("cases","f","g","Delta1","Delta2","tau")]

#' Discretise cases into bins
bin <- 0.2
n3.min <- 0.0
n3.max <- 2.4
df$class <- cut(df$cases, seq(n3.min, n3.max, bin), right = F)
#' Perform the LDA analysis based on the discrete Re class
lda <- lda(formula = class ~ f + g + Delta1 + Delta2 + tau, data = df)
#' Project the data onto the LDA space
plda <- predict(object = lda, newdata = df)
plot.df <- data.frame(class = df[,"class"], cases = df[,"cases"], lda = plda$x) # Output will be the category, lda.LD1 will be the new x-axis


ff <- seq(n3.min,n3.max-bin, bin) + bin/2
plot.df$discreteCases <- ff[as.numeric(plot.df$class)]

#' Plot the data
plot <- ggplot(plot.df, aes(x = lda.LD1, fill = discreteCases, group = fct_rev(class))) +
  stat_density(aes(y = ..count../nrow(plot.df)), position = "identity", alpha = 0.8, colour = "black") +
  scale_fill_gradientn(colours = c("seagreen","white","white", "white","salmon"), name = "tertiary\ncases",
                       values = scales::rescale(c(n3.min,1-.Machine$double.eps,1,1+.Machine$double.eps,n3.max)),
                       limits = c(n3.min,n3.max),
                       guide = guide_colorbar(direction = "horizontal")) +
  plotTheme + theme(legend.position = c(0.2,0.85)) +
  coord_cartesian(xlim = c(-1,1)*7, ylim = c(0,0.085), expand = F) +
  scale_x_continuous(breaks = seq(-8,8,2))
#' LDA proportion of variance per dimension
prop.lda <- lda$svd^2 / sum(lda$svd^2)
#' Add proportion to label
plot <- plot + labs(x = paste("LD1 (", scales::percent(prop.lda[1], accuracy = 0.1), ")", sep = ""), y = "frequency")

#' Add the parameter arrows and labels
lda.vectors <- as.data.frame(lda$scaling)
lda.vectors <- cbind(lda.vectors, data.frame(params = rownames(lda$scaling), stringsAsFactors = FALSE))
#' Reorder parameters
lda.vectors$params <- factor(lda.vectors$params, levels = c("tau","Delta2","g","Delta1","f"))
#' Sort vectors by size
#lda.vectors$params <- factor(lda.vectors$params, levels = lda.vectors$params[order(lda.vectors$LD1)])
#' Parameter name as expression
expressions <- c(f = "f", g = "g", tau = "tau", Delta2 = "Delta[2]", Delta1 = "Delta[1]")
lda.vectors$param.expressions <- expressions[as.character(levels(lda.vectors$params))[lda.vectors$params]]
#' Plot the parameter arrows and labels
param.plot <- ggplot(lda.vectors, aes(x = 0, xend = LD1, y = params, yend = params)) +
  geom_vline(xintercept = 0) +
  geom_segment(lineend = "butt", size = lineSize, arrow = arrow(length = unit(0.2, "cm"))) +
  geom_text(aes(x = LD1 + sign(LD1)*0.6, label = format(round(LD1, digits = 2), nsmall = 2))) +
  geom_text(aes(label = param.expressions, x = sign(-LD1)*0.4), parse = T, size = 5) +
  coord_cartesian(xlim = c(-1,1)*7, ylim = c(0.5,5.5), expand = F) +
  labs(caption = "parameter impact on the number of tertiary cases") +
  theme_void() +
  theme(plot.caption = element_text(hjust = 0.5, size = plotTheme$text$size),
        plot.margin = unit(c(1,0,1,0), units = "lines"))

#' Combine
plot_grid(plot, param.plot, nrow = 2, rel_heights = c(1,0.5), align = "v", axis = "lr", labels = "AUTO")
```

# Discussion
By combining empirically well-supported estimates of the infection timing of SARS-CoV-2 with a simple model of transmission, we have calculated the impact of test-trace-isolate-quarantine (TTIQ) interventions against the spread of COVID-19.
Overall, we find that TTIQ has the potential to control epidemics with an $R_e$ of up to 1.5.
This would be practically infeasible under testing \& isolation alone, which would require 65\% of positive cases to isolate immediately after the time of symptom onset to be effective.
By increasing the fraction of contacts that are identified and quarantined, we can successfully suppress an epidemic even if fewer index cases are isolated or if isolation is delayed by up to 2 days.
Based on this analysis,
we find that the greatest impact comes from increased identification of index cases and reduction of delay between symptom onset and isolation.
These parameters have a compound effect on overall transmission as they contribute to the direct reduction of onward infection from an index case, and they allow more contacts to be traced earlier.

Increasing the duration of the contact tracing window by looking back further in time has limited return under our model of forward contact tracing (identifying who is infected by the index case).
However, if we were interested in identifying the source of infection (backwards contact tracing), then increasing the duration of the contact tracing window could lead to the identification of transmission clusters.

When comparing to the findings of \citet{ferretti:Science:2020}, we find that contact tracing has less impact on epidemic suppression, and that the speed of contact tracing is of secondary importance to the speed of isolating index cases.
This difference can be attributed to \citet{ferretti:Science:2020}'s approach to model contact tracing and isolation as independent events (i.e. tracing an index cases' contacts says nothing about whether the index case has been isolated), which leads to an overestimation of contact tracing's impact \citep{fraser:PNAS:2004}.

In \citet{kretzschmar:TheLancetPublicHealth:2020} -- this time with contact tracing dependent on testing \& isolation -- they concluded that reducing the delay to isolation after symptom onset has the greatest impact on TTIQ effectiveness.
This conclusion was made without systematic analysis of all parameters, and we now find that changing testing \& isolation coverage has a greater effect on the number of tertiary cases.

Our approach and results are crucially dependent on the distribution of infection times (generation time and infectivity profile) and although we have used well-supported estimates, there's inherent limitations to deriving these distributions based on transmission pairs.
These transmission pairs are representative of symptomatic cases, but the infectiousness profiles for fully asymptomatic cases are unknown \citep{ferretti:medRxiv:2020}.
We have assumed that asymptomatic cases have the same infectiousness profiles as symptomatic cases, but if asymptomatic cases are infectious for a shorter duration, or have a lower probability of transmission during a contact \citep{buitrago-garcia:PLOSMedicine:2020}, then we would overestimate the transmission prevented by quarantining these cases.
We do account for uncertainty in the infection time distributions, and this uncertainty is carried through into our analysis and is captured by the confidence intervals shown in the figures and reported in the text.

In terms of modelling the TTIQ process, we have assumed that identified index cases are isolated and have their contracts traced.
If the index case fails to adhere to the isolation protocol, then we will overestimate the amount of transmission prevented by isolation.
However, uncertainty in whether contacts adhere to quarantine protocols, or whether contact tracers actually identify contacts, is contained in the parameter $g$.
Lower adherence to quarantine or missed cases due to overwhelmed contact tracers is captured by lowering $g$.

Here we have shown through systematic analysis that TTIQ processes can be optimised to bring the effective reproductive number below one.
Crucially, contact tracing \& quarantine adds security to testing \& isolating strategies, where high coverage and short delays are necessary to control an epidemic.
By improving the testing \& isolation coverage and reducing the delay to index case isolation, we can greatly increase the efficacy of the overall TTIQ strategy.


\pagebreak

# Supplemental figures {-}
\setcounter{figure}{0}
\renewcommand*\thefigure{S\arabic{figure}}

```{r flowchart-secondary, fig.align = "center", out.width = "\\textwidth", fig.cap = caption}
caption <- "Flowchart for computing the number of secondary cases under testing \\& isolation."
knitr::include_graphics("TTIQ-flowchart-secondary.pdf")
```

```{r flowchart-tertiary, fig.align = "center", out.width = "\\textwidth", fig.cap = caption}
caption <- "Flowchart for computing the number of tertiary cases under TTIQ."
knitr::include_graphics("TTIQ-flowchart-tertiary.pdf")
```

```{r secondaryCases-line, include = F}
#' Secondary cases as a function of f and Delta1
paramList <- list(
  f = seq(0,1,0.05),
  Delta1 = seq(0,4,1),
  Re = seq(1.1, 1.5, 0.1)
)
secondaryCasesMLE <- getSeconaryCases(paramList)
secondaryCasesCI <- getSeconaryCasesCI(paramList)

Re.plotVals <- c("1.1","1.3","1.5")
Delta1.plotVals <- c("0","2","4")

#' Plot as secondary cases vs f (colour = Delta1)
df <- merge(secondaryCasesMLE, secondaryCasesCI)
df$f <- as.numeric(levels(df$f))[df$f]
secondaryCasesPlot <- ggplot(df[df$Re %in% Re.plotVals & df$Delta1 %in% Delta1.plotVals,], aes(x = f, y = cases, colour = Delta1, group = Delta1)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = Delta1), alpha = 0.4, colour = "transparent") +
  geom_hline(yintercept = 1, colour = "darkgrey", size = lineSize) +
  geom_line(size = lineSize) +
  facet_grid(~Re) +
  scale_color_manual(values = colorRampPalette(brewer.pal(9, "Oranges"))(12)[c(5,8,10)],
                     name = expression("delay" ~ Delta[1]),
                     aesthetics = c("colour","fill"),
                     labels = dayLabels(levels(df$Delta1))) +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0,1), ylim = c(0.45,1.55), expand = F) +
  labs(x = "f: fraction of index cases found and isolated", y = expression("secondary cases" ~ n[2])) +
  plotTheme + theme(legend.position = "right", panel.spacing = unit(1.5, "lines"), strip.background = element_blank(), strip.text = element_blank())
```

```{r secondaryCases-density, include = F}
#' Critical region where epidemic is controlled
paramList <- list(
  f = seq(0,1,0.05),
  Delta1 = seq(0,6,0.2),
  Re = seq(1.1, 1.5, 0.1)
)
secondaryCasesMLE <- getSeconaryCases(paramList)
secondaryCasesCI <- getSeconaryCasesCI(paramList)

#' Prepare and plot
df <- merge(secondaryCasesMLE, secondaryCasesCI)
df$f <- as.numeric(levels(df$f))[df$f]
df$Delta1 <- as.numeric(levels(df$Delta1))[df$Delta1]
df$Re <- as.character(df$Re)

cases.min <- 0.5
cases.max <- 1.5

secondarySummaryPlot <- ggplot(df[df$Re %in% Re.plotVals,], aes(x = f, y = Delta1, group = Re)) +
  geom_raster(aes(fill = cases), interpolate = T) +
  geom_contour(aes(z = cases), colour = "black", size = lineSize, breaks = 1) +
  geom_contour(aes(z = lower), colour = "black", linetype = "dashed", breaks = 1) +
  geom_contour(aes(z = upper), colour = "black", linetype = "dashed", breaks = 1) +
  scale_fill_gradientn(colours = c("seagreen","white","white", "white","salmon"), name = "secondary\ncases",
                         values = scales::rescale(c(cases.min,1-.Machine$double.eps,1,1+.Machine$double.eps,cases.max)), limits = c(cases.min,cases.max)) +
  facet_grid(~Re, labeller = label_bquote(cols = R[e]~"="~.(Re))) +
  coord_cartesian(xlim = c(0,1), ylim = c(0, 4), expand = F) +
  labs(x = "f: fraction of index cases found and isolated",
       y = expression(atop(Delta[1] ~ ": time of index case isolation","(days after symptoms)")), fill = expression(R[e])) +
  plotTheme +
  theme(legend.position = "right", panel.spacing = unit(1.5, "lines"))
```

```{r secondaryCases, dependson = c(-1,-2,-3), fig.height = 6, fig.width = 10, fig.cap = caption, fig.pos = "ht"}
caption <- "A) The impact of testing \\& isolation on the number of secondary cases per index case, $n_2$, as a function of the testing coverage $f$ (x-axis) and delay to isolation after symptom onset $\\Delta_1$ (y-axis) for different $R_e$ values (columns) [Eq. \\eqref{eq:secondaryCasesAvg}].
The black line shows $n_2 = 1$.
Above this line (red zone) we have $n_2>1$ and the epidemic is growing.
Below this line we have $n_2<1$ and the epidemic is suppressed.
Dashed lines are the 95\\% confidence interval for this threshold.
B) Lines correspond to slices of panel A at fixed delay $\\Delta_1 =$ 0, 2, or 4 days (colour).
Shaded regions are 95\\% confidence intervals for the number of secondary cases per index case.
Horizontal grey line is the threshold for epidemic control ($n_2=1$).
\\label{fig:secondaryCases}"

#' Combine plots
plots <- list(secondaryCasesPlot, secondarySummaryPlot)
legends <- list(get_legend(plots[[1]]), get_legend(plots[[2]]))
plots <- lapply(plots, function(plot) plot + theme(legend.position = "none"))

plot_grid(
  plot_grid(plotlist = list(plots[[2]],plots[[1]]), ncol = 1, axis = "lr", align = "hv", rel_heights = c(1,1), labels = "AUTO"),
  plot_grid(plotlist = list(legends[[2]],legends[[1]]), ncol = 1, rel_heights = c(1,1)),
  rel_widths = c(1,0.1), ncol = 2
)
```

# References
